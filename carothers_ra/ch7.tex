\chapter{Completeness}

\section{Totally Bounded Sets}

\begin{exercise}{1}
    If $A \subset B \subset M$ and \(B\) is totally bounded, then \(A\) is totally bounded.
\end{exercise}

\begin{solution}
    
    Pick any $\epsilon > 0$.
    By Lemma 7.1, because $B$ is totally bounded, there exist finitely many sets $B_1, \ldots, B_n \subset B$ such that $\text{diam}(B_i) < \epsilon$ for all $i$ and $B \subset \bigcup_{i=1}^{n} B_i$.
    Let then $A_i = B_i \cap A$, each of which is clearly a subset of $A$.
    It holds of course that $\text{diam}(A_i) \leq \text{diam}(B_i) < \epsilon$.
    Furthermore, because $A \subset B \subset \bigcup_{i=1}^{n} B_i$, any $x \in A$ must belong in at least one $B_i$, and hence also in the corresponding $A_i$, which means that $A \subset \bigcup_{i=1}^{n} A_i$.
    By all of the above and Lemma 7.1, we conclude that $A$ is totally bounded.
\end{solution}

\begin{exercise}{2}
    Show that a subset $A \subset \mathbb{R}$ is totally bounded if and only if it is bounded.
    In particular, if $I$ is a closed, bounded, interval in $\mathbb{R}$ and $\epsilon > 0$, show that $I$ can be covered by finitely many closed subintervals $J_1, \ldots, J_n$, each of length at most $\epsilon$.
\end{exercise}

\begin{solution}

    We recall example 7.2 (a): a totally bounded set $A$ is necessarily bounded.
    To show this, pick $\epsilon = 1$ and obtain $x_1, \ldots, x_n$ such that $A \subset \bigcup_{i=1}^{n} B_1(x_i)$.
    Set $M = \max_{i, j} d(x_i, x_j)$ and pick any two $x, y \in A$, for which it must hold that $x \in B_i, y \in B_j$ for some $i, j$.
    By the triangle inequality:

    \[d(x, y) \leq d(x, x_i) + d(x_i, x_j) + d(x_j, y) < 1 + M + 1 = M + 2\]

    This of course shows that $A$ is bounded.
    
    Conversely, if $A \subset \mathbb{R}$ is bounded, then $\text{diam} A < R$, which in $\mathbb{R}$ is equivalent to $A \subset [-R, R]$.
    For any $\epsilon > 0$, set $N = \lceil \frac{2R}{\epsilon} \rceil$ and subdivide $[-R, R]$ into $N$ closed subintervals of length $\epsilon, [-R, a_1], [a_1, a_2], \ldots [a_{N-1}, R]$.
    Then observe that any element of $A$ must belong in one of these intervals, and thus is at most $\epsilon$-far from some $a_i$.
    Since there are finitely many $a_i$, we conclude that $A$ is totally bounded.
\end{solution}

\begin{exercise}{4}
    Show that $A$ is totally bounded if and only if $A$ can be covered by finitely many \textit{closed} sets of diameter at most $\epsilon$ for every $\epsilon > 0$.
\end{exercise}

\begin{solution}
    
    $\implies$: Suppose first that $A$ is totally bounded and pick any $\epsilon > 0$.
    Then there exist finitely many $x_1, \ldots, x_n$ such that $A \subset \bigcup_{i=1}^{n} B_{\epsilon/4}(x_i)$.
    As in the book, we may assume that $A \cap B_{\epsilon/4}(x_i) \neq \emptyset$ for all $i$, and thus for each $x_i$ find $a_i \in A, a_i \in B_{\epsilon/4}(x_i)$.
    Now, pick any $y \in B_{\epsilon/4}(x_i)$, and by the triangle inequality:

    \[d(y, a_i) \leq d(y, x_i) + d(x_i, a_i) \leq \epsilon/4 + \epsilon/4 \leq \epsilon/2\]

    This shows that all elements of $B_{\epsilon/4}(x_i)$ belong in the \textit{closed} ball of radius $\epsilon/2$ around $a_i$.
    More specifically then, all elements of $A$ that were covered by $B_{\epsilon/4}(x_i)$ are also covered by this closed ball.
    We conclude that the union of these $n$ closed balls ---each of which of course has diameter $\epsilon$--- covers $A$.

    $\impliedby$: Conversely, suppose that for any $\epsilon > 0$, $A$ can be covered by finitely many closed sets of diameter at most $\epsilon$.
    For any $\epsilon > 0$, use this hypothesis on $\epsilon/2$ to obtain finitely many closed sets $S_1, \ldots, S_n$ such that $A \subset \bigcup_{i=1}^{n} S_i$ and $\text{diam}(S_i) \leq \epsilon/2$ for all $i$.
    Once again, we may safely assume that $A \cap S_i \neq \emptyset$ for all $i$, and thus for every $i$ find an $a_i \in A, a_i \in S_i$.
    For any $y \in S_i$, it holds that $d(y, a_i) \leq \text{diam}(S_i) \leq \epsilon/2 < \epsilon$, and so $y \in B_{\epsilon}(a_i)$.
    This means that all elements of $A$ that were covered by $S_i$ are also covered by $B_{\epsilon}(a_i)$, meaning that $A \subset \bigcup_{i=1}^{n} B_{\epsilon}(a_i)$, which is of course the definition of total boundedness for $A$.
\end{solution}

\begin{exercise}{5}
    Prove that $A$ is totally bounded if and only if $\overline{A}$ is totally bounded.
\end{exercise}

\begin{solution}
    
   $\implies$: Suppose $A$ is totally bounded and pick any $\epsilon > 0$.
   Find $x_1, \ldots, x_n$ such that $A \subset \bigcup_{i=1}^{n} B_{\epsilon/2} (x_i)$, which is always possible due to $A$ being totally bounded.
   For any $z \in \overline{A}$, we have the following.
   If $z \in A$, we have that $z$ is inside at least one of these $n$ balls of radius $\epsilon/2$, and thus also inside the corresponding ball of radius $\epsilon$.
   If $z \notin A$, by the definition of the closure it must be that $y_i \rightarrow z$ for some $(y_i) \subset A$.
   Then there exists $N > 0$ such that for all $i > N, d(y_i, z) < \epsilon/2$.
   Pick any of these $y_i$, and find a corresponding $x_j$ such that $y_i \in B_{\epsilon/2}(x_j)$.
   By the triangle inequality, $d(y_i, x_j) \leq d(y_i, z) + d(z, x_j) < \epsilon/2 + \epsilon/2 = \epsilon$, which means that $y_i \in B_{\epsilon}(x_j)$.
   We've therefore shown total boundedness for $\overline{A}$.

   $\impliedby$: In the other direction, recall that $A \subset \overline{A}$, so by Exercise 1 if $\overline{A}$ is totally bounded, $A$ is as well.
\end{solution}

\begin{exercise}{9}
    Give an example of a closed bounded subset of $l_{\infty}$ that is not totally bounded.
\end{exercise}

\begin{solution}
    
    Consider the set corresponding to the sequence examined in Example 7.2 (d):
    \[S = \{(1, 0, \ldots), (0, 1, 0, \ldots), (0, 0, 1, \ldots), \ldots\} \subset l_{\infty}\]

    It's easy to see that the set is bounded, since all of its elements have a $l_\infty$ norm of 1.
    At the same time, it is closed, since the only convergent sequences it contains are eventually constant ones.
    However, the observation given in that example shows that it is not totally bounded: any two of its elements are at a distance of 1, and hence no finite number of balls of radius less than 1 can cover it.

\end{solution}

\begin{exercise}{10}
    Prove that a totally bounded metric space $M$ is separable. [Hint: For each $n$, let $D_n$ be a finite $(1/n)$-net for $M$.
    Show that $D = \bigcup_{n=1}^{\infty} D_n$ is a countable dense set.]
\end{exercise}

\begin{solution}
    
    Suppose $M$ is totally bounded, and form the construction indicated in the hint.
    Namely, for each $n$, collect the \textit{finite}, due to total boundedness, $x_1, x_2, \ldots, x_{k_n}$ needed to cover $M$ with balls of radius $1/n$.
    Call this set of $k_n$ elements $D_n$, and let $D = \bigcup_{n=1}^{\infty} D_n$, which is countable as a countable union of countable sets.
    Now, select any $y \in M$ and for each $i$, select an element $x_i \in D_i$ such that $d(y, x_i) < 1/i$, which is guaranteed to exist.
    Notice that this sequence gets arbitrarily close to $y$, and hence $D$ is also dense, showing that $M$ is separable.
\end{solution}

\begin{exercise}{11}
    Prove that $H^{\infty}$ is totally bounded (see Exercises 3.10 and 4.48).
\end{exercise}

\begin{solution}
    
    Recall the methodology we followed in Exercise 4.48.
    In particular, recall the observation that given any $\epsilon > 0$, we can find $N$ such that for any $x \in H^{\infty}$ and any sequence $r = (r_1, \ldots, r_N, 0, \ldots)$ it holds that under the metric of $H^{\infty}$:

    \[d(x, r) < \sum_{n=1}^{N} \lvert x_n - r_n \rvert + \frac{\epsilon}{2}\]

    If we can show that for any $\epsilon > 0$ we can select \textit{finitely} many sequences of the form $r = (r_1, \ldots, r_N, 0, 0, \ldots)$ such that for any $x \in H^{\infty}$, it is the case that for at least one of them it holds that $\sum_{n=1}^{N} \lvert x_n - r_{i,n} \rvert < \epsilon/2$, we will have shown that $d(x, r_i) < \epsilon$, and thus that $H^{\infty}$ is totally bounded.
    Let then $K = \lceil \frac{4N}{\epsilon} \rceil$, and consider subdividing the interval $[-1, 1]$ in $K$ equal subintervals, each of length $\frac{2}{K}$.
    Let $r_1, r_2, \ldots, r_K$ be the endpoints of these subintervals.
    Then, for any given sequence $(x_n)$, and for any of its terms $x_i$, it must hold that for some $r_j$:

    \[\lvert x_i - r_j \rvert < \frac{2}{K} < \frac{2}{\frac{4N}{\epsilon}} < \frac{\epsilon}{2N}\]

    We form $r$ by selecting such an $r_j$ for each $i = 1, 2, \ldots, N$, and then we have that:

    \[d(x, r) < \sum_{n=1}^{N} \frac{\epsilon}{2N} + \frac{\epsilon}{2} = \epsilon\]

    To summarize, for any $\epsilon > 0$, pick first $N$ as described previously, then select $r_1, \ldots, r_K$ as above (where $K$ is a function of $N, \epsilon$), and then set $R = \{(\rho_1, \rho_2, \ldots, \rho_N, 0, 0, \ldots), \rho_i \in \{r_1, r_2, \ldots, r_K\}\}$, which is a finite set such that for any $x \in H^{\infty}, d(x, r) < \epsilon$ for some $r \in R$.
    Therefore, $H^{\infty}$ is totally bounded.
\end{solution}

\section{Complete Metric Spaces}

\begin{exercise}{12}
    Let $(A, d)$ be a subset of an arbitrary metric space \((M, d)\).
    If \((A, d)\) is complete, show that \(A\) is closed in \(M\).
\end{exercise}

\begin{solution}
    
    Suppose $(x_n) \subset A$ is a sequence that converges to $x$.
    By Exercise 36 of Ch. 3, we know that $(x_n)$ is Cauchy as a convergent sequence.
    Because $A$ is complete, this means that $(x_n)$ must converge to a point inside $A$, which of course shows that $x \in A$, meaning that $A$ is indeed closed in $M$.
\end{solution}

\begin{exercise}{16}
    Prove that $\mathbb{R}^n$ is complete under any of the norms $\lvert \lvert \cdot \rvert \rvert_1, \lvert \lvert \cdot \rvert \rvert_2, \lvert \lvert \cdot \rvert \rvert_{\infty}$.
    [This is interesting because completeness is not usually preserved by the mere equivalence of \textit{metrics}.
    Here we use the fact that all of the metrics involved are generated by \textit{norms}.
    Specifically, we need the norms in question to be equivalent as functions: $\lvert \lvert \cdot \rvert \rvert_{\infty} \leq \lvert \lvert \cdot \rvert \rvert_2 \leq \lvert \lvert \cdot \rvert \rvert_1 \leq n \lvert \lvert \cdot \rvert \rvert_{\infty}$.
    As we will see later, any two norms on $\mathbb{R}^n$ are comparable in this way.]
\end{exercise}

\begin{solution}

    Suppose $(x_i)$ is a Cauchy sequence in $\mathbb{R}^n$ under $\lvert \lvert \cdot \rvert \rvert_1$.
    Then for every $\epsilon > 0$, there exists $N > 0$ such that for every $i, j \geq N$ it holds that:

    \[\lvert \lvert x_i - x_j \rvert \rvert_1 < \epsilon \implies \sum_{k=1}^{n}\lvert x_{i,k} - x_{j, k} \rvert < \epsilon\]

    This of course means that each of the coordinate sequences is Cauchy in $\mathbb{R}$, and so it converges to some limit, which we call $l_k$ for the $k$-th coordinate.
    Then we already know that $(x_i)$ converges to $(l_1, l_2, \ldots, l_n)$, which is clearly in $\mathbb{R}^n$ and so $\mathbb{R}^n$ is complete under $\lvert \lvert \cdot \rvert \rvert_1$.
    Note also that the equivalence of the metrics induced by these norms shows that $x_i \rightarrow_{\infty} (l_1, l_2, \ldots, l_n), x_i \rightarrow_{2} (l_1, l_2, \ldots, l_n)$ as well.
    What we furthermore have is that, as stated in the hint, $\lvert \lvert x_i - x_j \rvert \rvert_{\infty} \leq \lvert \lvert x_i - x_j \rvert \rvert_1 < \epsilon$, so any sequence that is Cauchy under $\lvert \lvert \cdot \rvert \rvert_1$ is also Cauchy under $\lvert \lvert \cdot \rvert \rvert_{\infty}$.
    Similarly, because $\lvert \lvert x_i - x_j \rvert \rvert_1 \leq n \lvert \lvert x_i - x_j \rvert \rvert_{\infty}$, any Cauchy sequence under $\lvert \lvert \cdot \rvert \rvert_{\infty}$ is also Cauchy under $\lvert \lvert \cdot \rvert \rvert_1$., and so the two sets of Cauchy sequences are the identical, and the argument presented above for $\lvert \lvert \cdot \rvert \rvert_1$ suffices to show that $\mathbb{R}^n$ is complete under $\lvert \lvert \cdot \rvert \rvert_{\infty}$ as well.
    By a similar argument we can see that $\mathbb{R}^n$ is complete under $\lvert \lvert \cdot \rvert \rvert_2$ as well.
\end{solution}

\begin{exercise}{17}
    Given metric spaces $M, N$, show that $M \times N$ is complete if and only if both $M, N$ are complete.
\end{exercise}

\begin{solution}
    
    We will use $d_1((x_1, y_1), (x_2, y_2)) = d_M(x_1, x_2) + d_N(y_1, y_2)$ as the product metric on $M \times N$ (from Exercise 46 of Ch. 3 we know that we have three choices that define equivalent metrics).
    Suppose first that $M, N$ are complete.
    Pick any Cauchy sequence $((x_i, y_i)) \in M \times N$.
    Then for any $\epsilon > 0$ there exists $K > 0$ such that for $k, l \geq K$ it holds that:

    \[d_1((x_k, y_k), (x_l, y_l)) < \epsilon \implies d_M(x_k, x_l) + d_N(y_k, y_l) < \epsilon\]
    This of course shows that $d_M(x_k, x_l) < \epsilon, d_N(y_k, y_l) < \epsilon$ for $k, l \geq K$, i.e., that $(x_i), (y_i)$ are Cauchy in $M, N$ respectively.
    By the completeness of $M, N$, we have that $x_i \rightarrow x \in M, y_i \rightarrow y \in N$, and then Exercise 46 Ch. 3 guarantees that $(x_i, y_i) \rightarrow (x, y)$, which is of course an element of $M \times N$, and so we've shown completeness for $M \times N$.

    Conversely, assume $M \times N$ is complete, and pick any Cauchy sequence $(x_i) \in M$.
    For a non-trivial problem, we assume that $N \neq \emptyset$, and so there exists at least one $y \in N$.
    Form the sequence $(x_i, y) \subset M \times N$, and pick any $\epsilon > 0$.
    We then have that there exists $K > 0$ such that for $k, l \geq K$: 

    \[ d_M(x_k, x_l) < \epsilon \implies d_M(x_k, x_l) + 0 < \epsilon \implies d_M(x_k, x_l) + d_N(y, y) < \epsilon \implies d_1((x_k, y), (x_l, y)) < \epsilon\]

    This shows that $(x_i, y)$ is Cauchy in $M \times N$, and by the completeness of $M \times N$ we obtain that $(x_i, y) \rightarrow (x, y')$ for some $x \in M, y' \in N$.
    Because convergence in the product space implies convergence in each of $M, N$, we have that $x_i \rightarrow x, y \rightarrow y'$ ($y' = y$ due to the fact that this sequence is constant).
    We've therefore shown that any Cauchy sequence in $M$ converges to an element of $M$, which means that $M$ is complete.
    Exchanging the roles of $M, N$ shows that the same holds for $N$.

\end{solution}

\begin{exercise}{18}
    Fill in the details of the proofs that $l_1, l_{\infty}$ are complete.
\end{exercise}

\begin{solution}
    
    The two proofs are done in much the same way as the proof that $l_2$ is complete.
    We begin with $l_1$.
    Consider a Cauchy sequence (of sequences) $(f_n) \subset l_1$, where we write $f_n = (f_n(k))_{k=1}^{\infty}$.
    Fix any $k \in \mathbb{N}$ and consider the sequence $(f_n(k))$.
    Select any $\epsilon > 0$ and observe that because $(f_n)$ is Cauchy, there exists $N > 0$ such that for any $n, m \geq N$ we have that $\lvert \lvert f_n - f_m \rvert \rvert _{1} < \epsilon$.
    Thus, it also holds that:

    \[\lvert f_n(k) - f_m(k) \rvert \leq \lvert \lvert f_n - f_m \rvert \rvert _{1} < \epsilon,\]

    which shows that the sequence $(f_1(k), f_2(k), \ldots)$ is Cauchy in $\mathbb{R}$ and thus converges.
    Call its limit $f(k)$ and set $f = (f(1), f(2), \ldots)$.
    We now show that $f \in l_1$.
    Fix any $N \in \mathbb{N}$, and consider the ``prefix'' $(f(1), f(2), \ldots, f(N))$.
    We have that:

    \[\sum_{n=1}^{N} \lvert f(n) \rvert = \sum_{n=1}^{N} \lvert \lim_{m \rightarrow \infty} f_m(n) \rvert = \lim_{m \rightarrow \infty} \sum_{n=1}^{N} \lvert f_m(n) \rvert\]

    Notice now that the RHS here is bounded, since $f_m \in l_1$ for any $m$.
    Therefore, for any $N, \sum_{n=1}^{N} \lvert f(n) \rvert \leq B$, and since limits preserve non-strict inequalities, $\lvert \lvert f \rvert \rvert _{1} \leq B \implies f \in l_1$.
    Finally, we need to show that $f_n \rightarrow f$ in $l_1$.
    
    Pick any $\epsilon > 0$ and select $m_0$ such that for $m, l \geq m_0, \lvert \lvert f_m - f_l \rvert \rvert _{1} < \epsilon$.
    Again, fix $N$ and observe that if $m \geq m_0$:

    \[\sum_{n=1}^{N} \lvert f_m(n) - f(n) \rvert = \sum_{n=1}^{N} \lvert \lim_{k \rightarrow \infty} f_k(n) - f_m(n) \rvert = \lim_{k \rightarrow \infty} \sum_{n=1}^{N} \lvert f_k(n) - f_m(n) \rvert\]

    Notice also that for any $m, k \geq m_0$:
    
    \[ \sum_{n=1}^{N} \lvert f_k(n) - f_m(n) \rvert \leq \lvert \lvert f_k - f_m \rvert \rvert_1 < \epsilon \]
    
    Taking the limit wrt. $k$ we obtain that:

    \[\lim_{k \rightarrow \infty} \sum_{n=1}^{N} \lvert f_k(n) - f_m(n) \rvert \leq \epsilon \implies \sum_{n=1}^{N} \lvert f_m(n) - f(n) \rvert \leq \epsilon\]


    Finally, since this holds for any $N$, we have that $\lvert \lvert f_m -f \rvert \rvert \leq \epsilon$ for $m \geq m_0$ and so $f_n \rightarrow f$, and we've thus shown that $l_1$ is complete (any Cauchy sequence in it converges to a point in it).

    Now we do the same for $l_{\infty}$.
    Let $(f_n) \subset l_{\infty}$ be Cauchy.
    First, fix $k \in \mathbb{N}$ and examine $(f_n(k))$ (sequence of reals). 
    We have that, for any $\epsilon > 0$ there exists $N > 0$ such that for $n, m \geq N$ it holds that $\lvert \lvert f_n - f_m \rvert \rvert_{\infty} < \epsilon$.
    From this we obtain:

    \[\lvert f_n(k) - f_m(k) \rvert \leq \sup_{l} \lvert f_n(l) - f_m(l) \rvert = \lvert \lvert f_n - f_m \rvert \rvert < \epsilon,\]

    showing that $(f_n(k))$ is Cauchy in $\mathbb{R}$ and thus converges, say to $f(k)$.
    We again define $f$ as $f = (f(1), f(2), \ldots)$, and we need to show that $f \in l_{\infty}$.
    For any fixed $N$:

    \[\max_{n \leq N} \lvert f(n) \rvert = \max_{n \leq N} \lvert \lim_{m \rightarrow \infty} f_m(n) \rvert = \lim_{m \rightarrow \infty} \max_{n \leq N} \lvert f_m(n) \rvert\]

    Because all $f_m \in l_{\infty}$, the RHS here is bounded, and since this holds for any $N$, we have also that $f \in l_{\infty}$.
    Now we need to show that $f_n \rightarrow f$.
    Pick any $\epsilon > 0$, and select $m_0$ such that for $m, l \geq m_0, \lvert \lvert f_m - f_l \rvert \rvert _{\infty} < \epsilon$.
    To begin, fix $N$ and observe that if $m \geq m_0$:

    \[\max_{n \leq N} \lvert f_m(n) - f(n) \rvert = \max_{n \leq N} \lvert \lim_{k \rightarrow \infty} (f_k(n) - f_m(n) ) \rvert = \lim_{k \rightarrow \infty} \max_{n \leq N} \lvert f_k(n) -f_m(n) \rvert \]

    Now, for any $k \geq m_0$ we have that:

    \[\max_{n \leq N} \lvert f_k(n) - f_m(n) \rvert \leq \sup_{n} \lvert f_k(n) -f_m(n) \rvert = \lvert \lvert f_k -f_m \rvert \rvert _{\infty} < \epsilon,\]

    and thus taking the limit wrt. $k$ yields that:

    \[\lim_{k \rightarrow \infty} \max_{n \leq N} \lvert f_k(n) - f_m(n) \rvert \leq \epsilon\]

    Therefore $\max_{n \leq N} \lvert f_m(n) - f(n) \rvert \leq \epsilon$, and since this holds for any $N$, we conclude that $\lvert f_m - f \rvert \leq \epsilon$, showing that $f_n \rightarrow f \in l_{\infty}$ and hence that $l_{\infty}$ is complete.
    

\end{solution}

\begin{exercise}{20}
    If $(x_n), (y_n)$ are Cauchy in $(M, d)$ show that $(d(x_n, y_n))_{n=1}^{\infty}$ is Cauchy in $\mathbb{R}$.
\end{exercise}

\begin{solution}
    
    Pick any $\epsilon > 0$.
    We have first that:
    
    \[\lvert d(x_n, y_n) - d(x_m, y_m) \rvert = \lvert d(x_n, y_n) - d(x_m, y_n) + d(x_m, y_n) - d(x_m, y_m) \rvert \leq \]
    \[\lvert d(x_n, y_n) - d(x_m, y_n) \rvert + \lvert d(x_m, y_n) - d(x_m, y_m) \rvert \leq d(x_n, x_m) + d(y_n, y_m),\]

    where we've used the triangle inequality in $\mathbb{R}$ and Exercise 2 of Ch. 3 (reverse triangle inequality in a metric space).
    Due to $(x_n), (y_n)$ being Cauchy, we can find $N, M > 0$ such that whenever $n, m \geq N, d(x_n, x_m) < \epsilon/2$ and whenever $n, m \geq M, d(y_n, y_m) < \epsilon/2$.
    Set $K = \max\{N, M\}$ and then we conclude that for $n, m \geq K, d(x_n, x_m) < \epsilon/2, d(y_n, y_m) < \epsilon/2$, which by the above inequality shows that $\lvert d(x_n, y_n) - d(x_m, y_m) \rvert < \epsilon$, i.e.\, that $(d(x_n, y_n))_{n=1}^{\infty}$ is Cauchy in $\mathbb{R}$.
\end{solution}

\begin{exercise}{21}
    If $(M, d)$ is complete, prove that two Cauchy sequences $(x_n), (y_n)$ have the same limit if and only if $d(x_n, y_n) \rightarrow 0$.
\end{exercise}

\begin{solution}
    
    Suppose first that $d(x_n, y_n) \rightarrow 0$.
    Because $(M, d)$ is complete, we have that $(x_n)$ converges to some $x$ as a Cauchy sequence.    
    Pick any $\epsilon > 0$, and obtain $N$ such that for $n \geq N, d(x_n, y_n) < \epsilon/2$.
    By the reverse triangle inequality:

    \[\lvert d(y_n, x) - d(x_n, x) \rvert \leq d(y_n, x_n) < \epsilon/2 \implies d(x_n, x) - \epsilon/2 < d(y_n, x) < \epsilon/2 + d(x_n, x)\]
    
    There exists $M > 0$ such that for $n \geq M, d(x_n, x) < \epsilon/2$.
    For $n \geq \max\{N, M\}$, we then have that:

    \[d(y_n, x) < \epsilon/2 + d(x_n, x) < \epsilon/2 + \epsilon/2 \implies d(y_n, x) < \epsilon,\]

    which shows that $y_n \rightarrow x$.
    Conversely, if $x_n \rightarrow x, y_n \rightarrow x$, we have that:

    \[d(x_n, y_n) \leq d(x_n, x) + d(x, y_n)\]

    Since the RHS can be made arbitrarily small, we obtain that $d(x_n, y_n) \rightarrow 0$.
\end{solution}

\begin{exercise}{22}
    Let $D$ be a dense subset of a metric space $M$, and suppose that every Cauchy sequence from $D$ converges to some point of $M$.
    Prove that $M$ is complete.
\end{exercise}

\begin{solution}
    
    Pick any Cauchy sequence $(x_n)$ in $M$.
    Now, construct a sequence $(y_n) \subset D$ in the following manner.
    For any $n$, pick $y_n \in D$ such that $d(x_n, y_n) < \frac{1}{2^n}$.
    This is always possible due to $D$ being dense in $M$.
    We show now that $(y_n)$ is Cauchy.
    Pick any $\epsilon > 0$ and find $N > 0$ such that $\frac{1}{2^N} < \frac{\epsilon}{3}$.
    By the construction of $(y_n)$, for any $n, m \geq N$, it holds that $d(x_n, y_n) < \epsilon/3, d(x_m, y_m) < \epsilon / 3$.
    Furthermore, because $(x_n)$ is Cauchy we can find $M$ such that for $n, m \geq M, d(x_n, x_m) < \epsilon/3$.

    Setting $K = \max\{N, M\}$, we observe that for $n, m \geq K$:

    \[d(y_n, y_m) \leq d(x_n, y_n) + d(x_n, y_m) \leq d(x_n, y_n) + d(x_n, x_m) + d(x_m, y_m) < \epsilon/3 + \epsilon/3 + \epsilon/3 = \epsilon\]

    This shows that $(y_n)$ is Cauchy in $D$, and thus, by the hypothesis, converges to a point in $M$.
    Now, since $d(x_n, y_n) \rightarrow 0$, we can apply Exercise 21 (Ch. 7) to conclude that $(x_n), (y_n)$ have the same limit, and hence that $(x_n)$ converges to a point in $M$, showing that $M$ is complete.
\end{solution}

\begin{exercise}{23}
    Prove that $M$ is complete if and only if every sequence $(x_n)$ in $M$ satisfying $d(x_n, x_{n+1}) < 2^{-n}$, for all $n$, converges to a point in $M$.
\end{exercise}

\begin{solution}
    
    First, we observe that every sequence that satisfies this constraint is Cauchy.
    Indeed, if $(x_n)$ is one such sequence, pick any $\epsilon > 0$ and set $N = \lceil \log(\frac{1}{\epsilon}) \rceil$, in which case $2^N > \frac{1}{\epsilon} \implies \epsilon > 2^{-N}$.
    For any $n, m > N$ (suppose $m > n$) it holds that:

    \[d(x_n, x_m) \leq d(x_n, x_{n+1}) + d(x_{n+1}, x_m) \leq d(x_n, x_{n+1}) + d(x_{n + 1}, x_{n + 2}) + d(x_{n + 2}, x_m) \leq \ldots \leq\]
    \[\sum_{i = n}^{m - 1} d(x_i, x_{i+1}) = \sum_{j = 0}^{m - n - 1} d(x_{n + j}, x_{n + j + 1})\]

    Now notice that for each such term it holds that $d(x_{n + j}, x_{n + j + 1}) < 2^{-n - j}$.
    Substituting:

    \[d(x_n, x_m) < \sum_{j = 0}^{m - n - 1} \frac{1}{2^{n + j}} = \frac{1}{2^{n}} \sum_{j = 0}^{m - n - 1} \frac{1}{2^j} < \frac{2}{2^n} = \frac{1}{2^{n - 1}} \leq \frac{1}{2^N} < \epsilon,\]

    completing the proof that the sequence is indeed Cauchy.
    The immediate consequence is that if $M$ is complete, any such sequence in $M$ converges to a point in $M$ (by the definition of completeness).

    Conversely, suppose that any such sequence in $M$ converges to a point in $M$, and let $(y_n) \subset M$ be Cauchy.

    Construct the following subsequence $(y_{n_k})$.
    For any $k = 1, 2, \ldots$, set $\epsilon_k = 2^{-k}$, and obtain $N_k$ such that for $i, j \geq N_k, d(x_i, x_j) < \epsilon_k = 2^{-k}$.
    Set $y_{n_k}$ to be equal to the first $x_i, i \geq N_k$ that has not yet been previously selected (this is always possible since $x_n$ has infinite terms).
    Now notice that by its definition, $d(y_{n_k}, y_{n_{k+1}}) < \epsilon_k = 2^{-k} \leq 2^{-n_k}$ (since $n_k \leq k$ always holds due to the fact that we select each term to be different from all previous ones).
    By our hypothesis, $(y_{n_k})$ converges to a point in $M$, and then Exercise 15 Ch. 1 guarantees that $(y_n)$ converges to the same point as a Cauchy sequence with a convergent subsequence.
\end{solution}

\begin{exercise}{24}
    Prove that the Hilbert cube $H^{\infty}$ is complete.
\end{exercise}

\begin{solution}
    
    Let $(x_n) \subset H^{\infty}$ be a Cauchy sequence, and consider, for any $k$, the sequence formed by taking the $k$-term of each of these, i.e. $(x_{11}, x_{21}, x_{31}, \ldots)$, and call it $y_k$.
    For any $\epsilon > 0$, set $\epsilon' = \epsilon/2^k$.
    Since $(x_n)$ is Cauchy, we can find $N$ such that for $n, m \geq N, d(x_n, x_m) < \epsilon$, which means:

    \[d(x_n, x_m) < \epsilon \implies \sum_{i=1}^{\infty} 2^{-i} \lvert x_{ni} - x_{mi} \rvert < \epsilon' \implies 2^{-k} \lvert x_{nk} - x_{mk} \rvert < \epsilon/2^{k} \implies \lvert x_{nk} - x_{mk} \rvert < \epsilon\]
    \[\implies \lvert y_{k_n} - y_{k_m} \rvert < \epsilon\]

    We have thus shown that $y_k$ is Cauchy in $\mathbb{R}$, and thus converges, say to $l_k$.
    This holds for any $k$, so let $l = (l_1, l_2, \ldots)$.
    Because it holds that $\lvert x_{nk} \rvert \leq 1$, it is also the case that $\lvert l_k \rvert \leq 1$ for any $k$, and so $l \in H^{\infty}$.

    We now only need to show that $x_n \rightarrow l$.
    Take any $\epsilon > 0$, and fix any $N > 0$.
    We examine the quantity $\sum_{k=1}^{N} 2^{-k} \lvert x_{nk} - l_k \rvert$.
    We have that:

    \[\sum_{k=1}^{N} 2^{-k} \lvert x_{nk} - l_k \rvert = \sum_{k=1}^{N} 2^{-k}\lvert x_{nk} - \lim_{m \rightarrow \infty} x_{mk} \rvert = \lim_{m \rightarrow \infty} \sum_{k=1}^{N} 2^{-k} \lvert x_{nk} - x_{mk} \rvert\]

    We now pick $M > 0$ such that $d(x_n, x_m) < \epsilon$ (possible since $(x_n)$ is Cauchy), and examine the quantity inside the limit for $n, m \geq M$:

    \[\sum_{k=1}^{N} 2^{-k} \lvert x_{nk} - x_{mk} \rvert \leq \sum_{k=1}^{\infty} 2^{-k} \lvert x_{nk} - x_{mk} \rvert = d(x_n, x_m) < \epsilon\]

    Since this holds for any $m \geq M$, we can take limits to obtain that $\lim_{m \rightarrow \infty} \sum_{k=1}^{N} 2^{-k} \lvert x_{nk} - x_{mk} \rvert \leq \epsilon$.
    Finally, since this holds for any$N$, we observe that we can thus also take the limit wrt. to $N$ in this inequality to obtain that:

    \[\sum_{k=1}^{\infty} 2^{-k} \lvert x_{nk} - l_k \rvert \leq \epsilon \implies d(x_n, l) \leq \epsilon,\]

    as long as $n \geq M$, where $M$ is defined as above.
    Since this holds for any $\epsilon > 0$, we conclude that $x_n \rightarrow l \in H^{\infty}$, hence that $H^{\infty}$ is complete.
\end{solution}

\begin{exercise}{25}
    True or false?
    If $f: \mathbb{R} \rightarrow \mathbb{R}$ is continuous and if $(x_n)$ is Cauchy, then $(f(x_n))$ is Cauchy.
    Examples?
    How about if we insist that $f$ be strictly increasing?
    Show that the answer is ``true'' if $f$ is Lipschitz.
\end{exercise}

\begin{solution}
    
    We know that every Cauchy sequence in $\mathbb{R}$ converges, which means that $x_n \rightarrow x$ for some $x$.
    Then the continuity of $f$ implies that $f(x_n) \rightarrow f(x)$.
    Now, select any $\epsilon > 0$ and pick $N$ such that for any $n \geq N, \lvert f(x_n) - f(x) \rvert < \epsilon/2$.
    For any $n, m \geq N$ we have that:

    \[\lvert f(x_n) - f(x_m) \rvert \leq \lvert f(x_n) - f(x) + f(x) - f(x_m) \rvert \leq \lvert f(x_n) - f(x) \rvert + \lvert f(x_m) - f(x) \rvert < \epsilon/2 + \epsilon/2 = \epsilon,\]

    which shows that $f(x_n)$ is Cauchy.
    The remaining questions in the exercise are obvious.
\end{solution}

\begin{exercise}{26}
    Just as with the nested interval theorem, it is essential that the sets $F_n$ used in the nested set theorem be both closed and bounded.
    Why?
    Is the condition $\text{diam}(F_n) \rightarrow 0$ really necessary?
    Explain.
\end{exercise}

\begin{solution}
    
    One simple reason is the fact that the counterexamples presented for the case of intervals are of course counterexamples for the case of sets too.
    Differently, we can extend the reasoning of these counterexamples in any metric space $M$.
    For example, if $x \in M$ and we set $F_n = \{y \in M: d(y, x) \leq \frac{1}{n}\} \setminus \{x\}$ (bounded but not closed), we observe that even though $F_1 \supset F_2 \supset \ldots$, the infinite intersection must be the empty set: if it were to contain any point $z$, then $d(z, x) \leq \frac{1}{n}$ for all $n$, which would mean $z = x$.
    Similarly, if we set $F_n = \{y \in M: d(y, x) \geq n\}$ in a metric space where these sets are not bounded, we again have $F_1 \supset F_2 \supset \ldots$, but again the infinite intersection must be the empty set, otherwise there would have to exist a $z$ such that $d(z, x) \geq n$ for all $n$, which is of course impossible (metrics are finite-valued).

    Regarding the condition $\text{diam}(F_n) \rightarrow 0$, suppose $x, y \in M$ with $d(x, y) < 1$ and let $F_n = \{z \in M: d(z, x) \leq 1 + \frac{1}{n}\}$.
    Here the sets are nested but the diameter clearly does not go to zero: each $F_n$ will always contain both $x, y$, since $d(x, y) < 1 \leq 1 + \frac{1}{n}$ for every $n$.
    Therefore the diameter will always be at least $d(x, y)$, and the consequence is that we can no longer conclude that the infinite intersection contains only one point.
    Other than that, the nested set theorem still holds in the sense that the infinite intersection is not empty.
\end{solution}

\begin{exercise}{27}
    Note that the version of the Bolzano-Weierstrass theorem given in Theorem 7.11 replaces boundedness with total boundedness.
    Is this really necessary?
    Explain.
\end{exercise}

\begin{solution}
    
    We know that the metric space $l_1$ is complete (Exercise 18 Ch. 7).
    Consider the set $\{e_n\}$, where $e_n = (0, 0, \ldots, 1, 0, \ldots)$, i.e., $e_n$ has 1 in its $n$-th coordinate and 0 elsewhere (example 7.2 (d)).
    As discussed in the example, this is a bounded but not totally bounded set in $l_1$.
    Suppose that it contains some convergent sequence $x_i \rightarrow x \in l_1$, as would have to be the case if Theorem 7.3 (i) $\implies$ (iii) held for \textit{bounded} sets.
    Note that this sequence cannot be eventually constant, otherwise we would not be able to obtain a limit point as defined in Exercise 4.33.
    Then, for $\epsilon = 1$ there must exist $N > 0$ such that for $i \geq N$:

    \[\lvert \lvert x_i - x \rvert \rvert _1 < \epsilon \implies \sum_{j=1}^{\infty} \lvert x_{ij} - x_j \rvert < \epsilon\]

    Because all $x_n$ are drawn from the set of $e_n$, if we consider the infinitely many terms $x_i, i \geq N$, an infinite number of them must be drawn from the set $\{e_m, e_{m + 1}, \ldots\}$ for \textit{any} $m$.
    These must then have $x_{ij} = 0$ for $i < m$, and hence from the inequality we have also $x_j = 0$ for $j < m$.
    Since this holds for any $m, x = (0, 0, 0, \ldots)$.
    But then by definition it must be that $\lvert \lvert x_i - x \rvert \rvert = 1 = \epsilon$, since these differ in precisely one coordinate.
    This makes it impossible for the inequality above to hold, and thus the sequence fails to converge, showing that boundedness of a set alone is not enough in the nested set theorem.
\end{solution}

\begin{exercise}{30}
    If $(M, d)$ is complete, prove that every open subset $G \subset M$ is homeomorphic to a complete metric space.
    [Hint: Let $F = M \setminus G$ and consider the metric $\rho(x, y) = d(x, y) + \lvert d(x, F)^{-1} - d(y, F)^{-1} \rvert$ on $G$.]
\end{exercise}

\begin{solution}
    
    Based on the hint, we will show that the function $i: (G, d) \rightarrow (G, \rho), i(x) = x$ is an homeomorphism.
    We know (Theorem 5.5) that this is equivalent to showing that $x_n \rightarrow^{d} x \iff x_n \rightarrow^{\rho} x$.
    Suppose therefore that $x_n \rightarrow^{d} x$.
    Recall that by preposition 5.4 we know that $x \rightarrow d(x, F)$ is a continuous function.
    This means that $d(x_n, F) \rightarrow d(x, F)$.
    We observe also that $d(x, F) > 0, d(x_n, F) > 0$ must always hold: indeed, $x_n, x \in G$, and because $G \cap F = \emptyset, \overline{F} = F$ ($F$ is closed), it can never be that $d(x_n, F) = 0, d(x, F) = 0$ since this would mean that these are in $\overline{F}$.
    Therefore the function $x \mapsto d(x, F)^{-1}$ defined for $x \in G$ is also well-defined and continuous.
    Pick now any $\epsilon > 0$.
    There exists $N_1 > 0$ such that for $n \geq N_1, \lvert d(x_n, F)^{-1} - d(x, F)^{-1} \rvert < \epsilon/2$, and $N_2 > 0$ such that for $n \geq N_2, d(x_n, x) < \epsilon/2$.
    Therefore for $n \geq \max\{N_1, N_2\}$:

    \[\rho(x_n, x) = d(x_n, x) + \lvert d(x_n, F)^{-1} - d(x, F)^{-1} \rvert < \epsilon/2 + \epsilon/2 = \epsilon,\]

    which shows that $x_n \rightarrow^{\rho} x$.
    Conversely, suppose $x_n \rightarrow^{\rho} x$, which means that for any $\epsilon > 0$, there exists $N > 0$ s.t. for $n \geq N, \rho(x_n, x) < \epsilon \implies d(x_n, x) < \epsilon$, which shows that $x_n \rightarrow^{d} x$.
    This completes the proof that $i$ is a homeomorphism between $(G, d), (G, \rho)$.
    We also need to show that $(G, \rho)$ is complete.
    Suppose therefore $(x_n) \subset G$ is Cauchy under $\rho$.
    Then for any $\epsilon > 0$ there exists $N > 0$ such that for $n, m \geq N$:

    \[\rho(x_n, x_m) < \epsilon \implies d(x_n, x_m) + \lvert d(x_n, F)^{-1} - d(x_m, F)^{-1} \rvert < \epsilon \implies d(x_n, x_m) < \epsilon,\]

    which shows that $(x_n)$ is Cauchy in $M$ under $d$.
    This means that $x_n \rightarrow^{d} x$, since $M$ is complete.
    If we can show that $x \in G$, then we can use the homeomorphism $i$ to immediately obtain that $x_n \rightarrow^{\rho} x$, which would complete the proof that $G$ is complete under $\rho$.
    Suppose $x \notin G$, which means $x \in F$ and hence $d(x, F) = 0$, which implies that $d(x_n, F) \rightarrow 0$.
    Fix $\epsilon > 0$ and obtain $N > 0$ such that for $n, m \geq N$ it holds that $\rho(x_n, x_m) < \epsilon$.
    In particular, fix then any such $n \geq N$, and select $\epsilon' = \frac{1}{d(x_n, F)^{-1} + \epsilon}$.
    There exists $M > 0$ such that for $m \geq M$:
    
    \[d(x_m, F) < \epsilon' \implies d(x_m, F) < \frac{1}{d(x_n, F)^{-1} + \epsilon} \implies d(x_n, F)^{-1} + \epsilon < \frac{1}{d(x_m, F)} \implies \]
    \[\epsilon < d(x_m, F)^{-1} - d(x_n, F)^{-1}\]

    Since the LHS is positive, we have $\epsilon < \lvert d(x_m, F)^{-1} - d(x_n, F)^{-1} \rvert \implies \rho(x_n, x_m) > \epsilon$.
    But then if $m \geq \max\{N, M\}$, this directly contradicts the fact that $(x_n)$ is Cauchy under $\rho$.
    Therefore $x \notin F \implies x \in G$, which completes the proof.

\end{solution}

\begin{exercise}{31}
    If $\sum_{n=1}^{\infty} x_n$ is a convergent series in a normed vector space $X$, show that  $\lvert \lvert \sum_{n=1}^{\infty} x_n \rvert \rvert \leq \sum_{n=1}^{\infty} \lvert \lvert x_n \rvert \rvert$.
\end{exercise}

\begin{solution}

    If the RHS of the inequality doesn't converge, then the statement holds trivially.
    If it does converge, then consider the following for any $N$:

    \[\lvert \lvert \sum_{n=1}^{N} x_n \rvert \rvert \leq \sum_{n=1}^{N} \lvert \lvert x_n \rvert \rvert\]
    
    Because norms are continuous, we know that the LHS converges as $N \rightarrow \infty$, and in fact it converges to $\lvert \lvert \sum_{n=1}^{\infty} x_n \rvert \rvert$.
    We thus have that both sides converge, and since limits preserve non-strict inequalities we obtain that $\lvert \lvert\sum_{n=1}^{\infty} x_n \rvert \rvert \leq \sum_{n=1}^{\infty} \lvert \lvert x_n \rvert \rvert$.
\end{solution}

\begin{exercise}{36}
    The function $f(x) = x^2$ has two obvious fixed points: $p_0 = 0, p_1 = 1$.
    Show that there is a $0 < \delta < 1$ such that $\lvert f(x) - p_0 \rvert < \lvert x - p_0 \rvert$ whenever $\lvert x - p_0 \rvert < \delta, x \neq p_0$.
    Conclude that $f^{(n)}(x) \rightarrow p_0$ whenever $\lvert x - p_0 \rvert < \delta, x \neq p_0$.
    This means that $p_0$ is an \textit{attracting} fixed point for $f$; every orbit that starts out near 0 converges to 0.
    In contrast, find a $\delta > 0$ such that if $\lvert x - p_1 \rvert < \delta, x \neq p_1$, then $\lvert f(x) - p_1 \rvert > \lvert x - p_1 \rvert$.
    This means that $p_1$ is a \textit{repelling} fixed point for $f$; orbits that start out near 1 are pushed away from 1.
    In fact, given any $x \neq 1$, we have $f^{(n)}(x) \nrightarrow 1$.
\end{exercise}

\begin{solution}
    
    For the first case, we are interested in the inequality $\lvert f(x) - 0 \rvert < \lvert x - 0 \rvert \iff \lvert x^2 \rvert < \lvert x \rvert$.
    For $x \neq 0$, this simplifies to $\lvert x \rvert < 1$.
    Therefore, select, for example, $\delta = 1/2$.
    For any $x$ such that $\lvert x - 0 \rvert < 1/2$, it holds that $\lvert f(x) - 0 \rvert < \lvert x - 0 \rvert$.
    Since $f(x)$ will also be in $(-1/2, 1/2)$, this will hold upon re-application of $f$ too.
    This shows that $f^{(n)}(x) \rightarrow 0$.
    Now consider the second case, where we are interested in $\lvert f(x) - 1 \rvert > \lvert x - 1 \rvert \iff \lvert x^2 - 1 \rvert > \lvert x - 1 \rvert$.
    For $x \neq 1$, this simplifies to $\lvert x + 1 \rvert > 1$.
    Therefore, pick, for example, $\delta = 1/2$, in which case whenever $\lvert x - 1 \rvert < 1/2$ we have more specifically that $x > 0$, and so $\lvert x + 1 \rvert > 1$.
    Therefore for all such $x$ with $x \neq 1$, we get $\lvert f(x) - 1 \rvert > \lvert x - 1 \rvert$.
    Observe that this suffices to see why it can never be the case that $f^{(n)}(x) \nrightarrow 1$ for $x \neq 1$: if at any point a term of this sequence ends up in $(1 - 1/2, 1 + 1/2)$ (which it must, in order to converge to 1), a re-application of $f$ will push the next term away again, and thus we can never obtain convergence.
\end{solution}

\begin{exercise}{37}
    Suppose that $f: (a, b) \rightarrow (a, b)$ has a fixed point $p$ in $(a, b)$ and that $f$ is differentiable at $p$.
    If $\lvert f'(p) \rvert < 1$, prove that $p$ is an attracting fixed point for $f$.
    If $\lvert f'(p) \rvert > 1$, prove that $f$ is a repelling fixed point for $f$.
\end{exercise}

\begin{solution}
    
    Suppose first that $\lvert f'(p) \rvert < 1$.
    We recall the definition of the derivative: $f'(p) = \lim_{h \rightarrow 0} \frac{f(p + h) - (p)}{h}$, meaning that for every $\epsilon > 0$ there exists $\delta > 0$ such that whenever $\lvert h \rvert < \delta$ it holds that:

    \[\Bigl\lvert \frac{f(p + h) - f(p)}{h} - f'(p) \Bigr\rvert < \epsilon \implies \Biggl \lvert \Bigl \lvert \frac{f(p + h) - f(p)}{h} \Bigr \rvert - \lvert f'(p) \rvert \Biggr \rvert < \epsilon \implies \Bigl \lvert \frac{f(p + h) - f(p)}{h} \Bigr \rvert < \epsilon + \lvert f'(p) \rvert\]

    Since $\lvert f'(p) \rvert < 1$, let $\epsilon = 1 - \lvert f'(p) \rvert$, obtain the corresponding $\delta$ and rewrite the above as:

    \[\lvert f(p + h) - f(p) \rvert < \lvert h \rvert(1 - \lvert f'(p) \rvert + \lvert f'(p) \rvert) = \lvert h \rvert\]

    This holds for $\lvert h \rvert < \delta$, or, by substituting $x = p + h$, for $\lvert x - p \rvert < \delta$, which means $\lvert f(x) - f(p) \rvert < \lvert x - p \rvert$.
    Recall that $f(p) = p$ since $p$ is a fixed point for $f$, and observe that we have arrived at the same conclusion as in the previous exercise: if we choose $x_0 \in (p - \delta, p + \delta)$, it will always be the case that $\lvert f(x_0) - p \rvert < \lvert x_0 - p \rvert$, and this distance will monotonously shrink as we re-apply $f$, leading us to conclude that $f^{(n)}(x_0) \rightarrow p$, which means $p$ is indeed an attracting fixed point for $f$.
\end{solution}