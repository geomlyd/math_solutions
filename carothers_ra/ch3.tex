\chapter{Metrics and Norms}

\section{Metric Spaces}


\begin{exercise}{2}
    If $d$ is a metric on $M$, show that $\lvert d(x, z) - d(y, z) \rvert \leq d(x, y) $ for any $x, y, z \in M$.
\end{exercise}

\begin{solution}
    
    First, we apply the triangle inequality as follows:
    $$d(x, z) \leq d(x, y) + d(y ,z) \implies d(x, z) - d(y, z) \leq d(x, y)$$
    Another application of it yields:
    $$d(y, z) \leq d(y, x) + d(x, z) \implies -d(y, x) \leq d(x, z) - d(y, z) \implies -d(x, y) \leq d(x, z) - d(y,z),$$
    where we used the symmetry property of metric $d$.
    Putting the two inequalities together results in:
    $$\lvert d(x, z) - d(y, z) \rvert \leq d(x, y)$$
\end{solution}

\begin{exercise}{3}
    As it happens, some of our requirements for a metric are redundant.
    To see why this is so, let $M$ be a set and suppose $d: M \times M \rightarrow \mathbb{R}$ satisfies $d(x, y) = 0$ if and only if $x = y$, and $d(x, y) \leq d(x, z) + d(y, z)$ for all $x, y, z \in M$.
    Prove that $d$ is a metric; that is, show that $d(x, y) \geq 0$ and $d(x, y) = d(y, x)$ hold for all $x, y$.
\end{exercise}

\begin{solution}
    
    Pick any two $x, y \in M$.
    We know then that for any $z \in M$ it holds that $d(x, z) \leq d(x, y) + d(z, y)$.
    More specifically, this holds for $z = x$, in which case we obtain:
    $$d(x, x) \leq d(x, y) + d(x, y) \implies 0 \leq 2d(x, y),$$
    which means that for any two $x, y \in M, d(x, y) \geq 0$.

    For the symmetry propety, once again pick any two $x, y \in M$.
    Then $d(x, y) \leq d(x, z) + d(y, z)$ for any $z$.
    Pick then $z = x$, in which case we obtain $d(x, y) \leq d(x, x) + d(y, x) \implies d(x, y) \leq d(y, x)$.
    By exchanging the roles of $x, y$, we have that $d(y, x) \leq d(y, z) + d(x, z)$ for any $z$.
    Pick $z = y$ to observe that $d(y, x) \leq d(y, y) + d(x, y) \implies d(y, x) \leq d(x, y)$.
    But then $d(x, y) \leq d(y, x) \leq d(x, y)$, thus the only possibility is that $d(x, y) = d(y, x)$.
    Therefore $d$ is indeed a metric.
\end{solution}

\newpage

\begin{exercise}{6}
    If $d$ is any metric on $M$, show that $\rho(x, y) = \sqrt{d(x, y)}, \sigma(x, y) = d(x, y)/(1 + d(x, y))$ and $\tau(x, y) = \min\{d(x, y), 1\}$ are also metrics on $M$.
    [Hint: $\sigma(x, y) = F(d(x, y))$, where $F$ is as in exercise 5.]
\end{exercise}

\begin{solution}
    
    We will solve this as a simple application of 7.
    If $F_1: [0, \infty) \rightarrow [0, \infty), F_1(x) = \sqrt{x}$, then $\rho(x, y) = F_1(d(x, y))$.
    Then $F_1(x)/x = 1/\sqrt{x}$, which is clearly decreasing for $x> 0$, and therefore exercise 7 guarantees that $\rho$ is a metric.

    Similarly, if $F_2: [0, \infty) \rightarrow [0, \infty), F_2(x) = \frac{x}{1 + x}$, then $F_2'(x) = \frac{x + 1 - x}{(1 + x)^2} = \frac{1}{(x + 1)^2}$, which is clearly decreasing as for $x \geq 0$, and therefore $\sigma$ is also a metric.

    Lastly, if $F_3: [0, \infty) \rightarrow [0, \infty), F_3(x) = \min\{x, 1\}$, then $\tau(x, y) = F_3(d(x, y))$ and:
    $$F_3(x)/x = \begin{cases}
        1, & x \leq 1 \\
        \frac{1}{x}, & x > 1
    \end{cases},$$
    which is of course a decreasing function for $ x > 0$, and therefore $\tau$ is a metric.
\end{solution}
    
\begin{exercise}{7}
    Here is a generalization of exercises 5 and 6.
    Let $f: [0, \infty) \rightarrow [0, \infty)$ be increasing and satisfy $f(0) = 0$, and $f(x) > 0$ for all $x > 0$.
    If $f$ also satisfies $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$, then $f \circ d$ is a metric whenever $d$ is a metric.
    Show that each of the following conditions is sufficient to ensure that $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$:

    (a) $f$ has a second derivative satisfying $f'' \leq 0$

    (b) $f$ has a decreasing first derivative

    (c) $f(x)/x$ is decreasing for $x > 0$
    
    [Hint: First show that (a) $\implies$ (b) $\implies$ (c).]
\end{exercise}

\begin{solution}
    
    As indicated in the hint, we first show that (a) $\implies$ (b) $\implies$ (c).
    If, after that, we can show that (c) implies the triangle inequality for $f$, we know also that any of (a), (b) imply it as well (since they imply (c)).

    (a) $\implies$ (b): Since $f'' \leq 0$, $f$ more specifically has a first derivative on every point of $[0, \infty)$.
    From calculus 1, it's known also that this implies that $f'$ is decreasing (one would prove this via the Mean Value Theorem).

    (b) $\implies$ (c): (b) implies that $f$ is differentiable, thus $g(x) = f(x)/x, x > 0$ is also differentiable, with $g'(x) = \frac{f'(x)x - f(x)}{x^2}$.
    If we can show that the numerator here is non-positive for $x > 0$, we'll have shown that $g$ is decreasing.
    Pick any $x > 0$, and apply the Mean Value Theorem on $f$ in the interval $[0, x]$.
    This means that there exists $y \in (0, x)$ such that:
    $$f'(y) = \frac{f(x) - f(0)}{x - 0} = \frac{f(x)}{x},$$
    where we used the fact that $f(0) = 0$.
    Therefore, we have that:
    $$f'(x)x - f(x) = f'(x)x - xf'(y) = x(f'(x) - f'(y)) \leq 0,$$
    since $f'$ is decreasing and $y < x$.
    This completes the proof that (b) $\implies$ (c).

    We now need to show that the triangle inequality for $f$ follows from (c).
    Pick any two $x, y \geq 0$.
    If any of them, or both, are zero, the triangle inequality follows from the fact that $f(0) = 0$.
    If $x, y \neq 0$, then we know that $x \leq x + y, y \leq x + y$, and thus that $\frac{f(x)}{x} \geq \frac{f(x + y)}{x + y}, \frac{f(y)}{y} \geq \frac{f(x + y)}{x + y}$ (by the fact that $f(x)/x$ is decreasing). 
    We can rewrite these as:
    $$(x + y)f(x) \geq xf(x+ y), (x+ y)f(y) \geq yf(x + y),$$
    and now we can add them to obtain that:
    $$(x + y)(f(x) + f(y)) \geq f(x + y)(x + y) \implies f(x) + f(y) \geq f(x + y),$$
    which is of course what we wanted to show.
\end{solution}

\begin{exercise}{10}
    The \textit{Hilbert cube}, $H^{\infty}$, is the collection of all real sequences $x = (x_n)$ with $\lvert x_n \rvert \leq 1, n = 1, 2, \ldots$.

    (i) Show that $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - y_n \rvert$ defines a metric on $H^{\infty}$.

    (ii) Given $x, y \in H^{\infty}$ and $k \in \mathbb{N}$, let $M_k = \max\{ \lvert x_1 - y_1 \rvert, \ldots, \lvert x_k - y_k \rvert\}$.
    Show that $2^{-k}M_k \leq d(x, y) \leq M_k + 2^{-k + 1}$.
\end{exercise}

\begin{solution}
    
    (i) We will make use of exercise 3, which allows us to conclude that $d$ is a metric if the following three things hold:
    \begin{itemize}
        \item First, that $d(x, y)$ is a non-negative real numer for any $x, y$, which, due to the infinite sum, is non-trivial here. 
        Consider any two sequences $x, y$. We know that $\lvert x_n \rvert \leq 1, \lvert y_n \rvert \leq 1$, for all $n$, therefore the maximum value that $\lvert x_n - y_n \rvert$ is 2.
        This means that $2^{-n} \lvert x_n - y_n \rvert \leq 2^{-n+1}$.
        The partial sums that correspond to $d$ are therefore non-decreasing and upper bounded by a convergent geometric series, therefore the series that defines $d$ also converges.
        \item Second, that $d(x, y) = 0$ iff $x = y$.
        If two sequences $x, y$ are equal, then $x_n = y_n$ for all $n$, thus $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - x_n \rvert = 0$.
        Conversely, if $d(x, y) = 0$, then we have a non-decreasing sequence of non-negative partial sums that converges to zero.
        The only way this can happen is if each term of the sequence is zero, i.e.\ if $\lvert x_n - y_n \rvert = 0$ for all $n$, that is, if $x_n = y_n$ for all $n$.
        But this means precisely that $x = y$.
        \item Third, that the triangle inequality holds.
        Pick any three sequences $x, y, z$, and $k \in \mathbb{N}^+$.
        Then:
        $$\sum_{n = 1}^{k} 2^{-n} \lvert x_n - y_n \rvert = \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n + z_n - y_n \rvert \leq \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n \rvert + \sum_{n=1}^{k} 2^{-n} \lvert z_n - y_n \rvert,$$
        by the triangle inequality.
        Since this holds for any $k$ and all series converge (by the first item proven), we know that the inequality holds for the infinite series as well, i.e.\ that:
        $$d(x, y) \leq d(x, z) + d(y, z)$$
    \end{itemize}
    Therefore $d$ is a metric on the Hilbert cube.

    (ii) Pick any two sequences $x, y$ in the Hilbert cube and $k \in \mathbb{N}$.
    We know then by definition that $M_k \geq \lvert x_n - y_n \rvert$ for $i \leq k$.
    In addition, for $n > k$, the $n$-th term of the series that forms the metric $d$ is $2^{-n}\lvert x_n - y_n \rvert \leq 2^{-n+1}$.
    Thus:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert \leq M_k \sum_{n=1}^{k}2^{-n} + \sum_{n=k+1}^{\infty} 2^{-n + 1}$$
    $$ = M_k(1 - 2^{-k}) + 2(2 - 1 - (1 - 2^{-k})) = M_k - 2^{-k}M_k + 2^{-k+1} \leq M_k + 2^{-k+1},$$
    where we used the fact that the geometric series with ratio $r=1/2$ sums to 2, and our second sum here was thus 2 minus the sum of the first $k$ terms minus 1 since the series starts at 1 instead of 0.
    In the last step we also use the fact that $M_k \geq 0$.
    For the other inequality, we have again that:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert$$
    Notice that in order for $x, y$ to minimize this, it should be the case that $x_n - y_n = 0, n > k$, and that the maximum absolute difference $M_k$ is multiplied with the smallest possible quantity up to $k$, that is, with $2^{-k}$, while for all other $n < k, x_n - y_n = 0$.
    This would yield that $d(x, y) \geq M_k2^{-k} + 0 + 0 = M_k2^{-k}$.

\end{solution}

\begin{exercise}{12}
    Check that $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert$ defines a metric on $C[a, b]$, the collection of all continuous, real valued functions defined on the closed interval $[a, b]$.
\end{exercise}

\begin{solution}
    
    Note that here continuity is important to ensure that the difference of any two functions is continuous, and as such achieves a maximum value in any closed interval.
    Using exercise 3, we need to check the following:
    \begin{itemize}
        \item One, that $d(f, g) = 0$ iff $f = g$.
        Suppose first that $d(f, g) = 0$, that is, $\max_{a \leq b} \lvert f(t) - g(t) \rvert = 0$.
        If $f, g$ were not equal, there would exist $x \in [a, b]$ such that $f(x) \neq g(x)$.
        Clearly then $\max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert \geq \lvert f(x) - g(x) \rvert > 0$, which is a contradiction.
        Therefore $f = g$.
        In the other direction, if $f = g$, we equivalently have that $f - g$ is the zero function on $[a, b]$, and therefore $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = 0$.
        \item Two, that for any three $f, g, h \in C[a, b], d(f, h) \leq d(f, g) + d(g, h)$.
        We have first that $d(f, h) = \max_{a \leq t \leq b} \lvert f(t) - h(t) \rvert$.
        Suppose that this function achieves its maximum value on $t_M \in [a, b]$ (if there are more than one, pick any).
        Then $d(f, h) = \lvert f(t_M) - h_(t_M) \rvert = \lvert f(t_M) - g(t_M) + g(t_M) - h(t_M) \rvert \leq \lvert f(t_M) - g(t_M) \rvert + \lvert g(t_M) - h(t_M) \rvert$.
        By definition, it holds that $\lvert f(t_M) - g(t_M) \rvert \leq \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = d(f, g)$, and similarly $\lvert g(t_M) - h(t_M) \rvert \leq d(g, h)$.
        We therefore obtain that $d(f, h) \leq d(f, g) + d(g, h)$.
    \end{itemize}
    Therefore $d$ is indeed a metric on $C[a, b]$.
\end{solution}

\begin{exercise}{14}
    We say that a subset $A$ of a metric space is \textbf{bounded} if there is some $x_0 \in M$ and some constant $C < \infty$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Show that a finite union of bounded sets is again bounded.
\end{exercise}

\begin{solution}
    
    Suppose we have the finite union 
    $$U = A_1 \cup A_2 \cup \ldots \cup A_n$$
    of $n$ bounded subsets of a metric space $M$.
    Let then $x_1, x_2, \ldots, x_n, C_1, \ldots, C_n$ be such that $d(a, x_i) \leq C$ for $a \in A_i$.
    Form the set $\{d(x_1, x_1), d(x_1, x_2), \ldots, d(x_n, x_1)\}$, which, crucially, has a finite number of elements, and therefore also has a maximum element, $M$.
    By the same reasoning, there exists $C$ such that $C = \max\{C_1, \ldots, C_n\}$.
    Now pick any $a \in U$, which must belong in at least one $A_i$.
    Therefore:
    $$d(a, x_1) \leq d(a, x_i) + d(x_i, x_1) \leq C_i + M \leq C + M$$
    If we thus set $x_U = x_1$ and $C_U = C + M$, we have shown precisely that $U$ is bounded.
\end{solution}

\begin{exercise}{15}
    We define the \textbf{diameter} of a nonempty subset $A$ of $M$ by $\text{diam}(A) = \sup\{d(a, b) : a, b \in A\}$.
    Show that $A$ is bounded if and only if $\text{diam}(A)$ is finite.
\end{exercise}

\begin{solution}
    
    $\implies$: Suppose first that $A$ is bounded.
    Then there exists $x_0 \in M, C \in \mathbb{R}$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Pick any two $a, b \in A$.
    We then have that $d(a, b) \leq d(a, x_0) + d(x_0, b) \leq 2C$.
    Therefore the set $\{d(a, b): a, b \in A$\} has an upper bound, namely, $2C$, and therefore also a finite least upper bound, which means precisely that $\text{diam}(A)$ is finite.

    $\impliedby$: Now suppose $\text{diam}(A) = C \in \mathbb{R}$.
    Fix an $a \in A$ (which exists since $A$ is nonempty).
    Pick any $b \in A$, in which case we have that:
    $$d(a, b) \in \{d(x, y): x, y \in A\} \implies d(a, b) \leq \text{diam}(A) = C$$
    If thus set $x_0 = a$ and $C$ the respective constant, we see that the definition of $A$ being bounded is fulfilled.
\end{solution}

\section{Normed Vector Spaces}

\begin{exercise}{16}
    Let $V$ be a vector space, and let $d$ be a metric on $V$ satisfying $d(x, y) = d(x - y, 0)$ and $d(ax, ay) = \lvert a \rvert d(x, y)$ for every $x, y \in V$ and every scalar $a$.
    Show that $\lvert \lvert x \rvert \rvert = d(x, 0)$ defines a norm on $V$ (that has $d$ as its ``usual'' metric).
    Give an example of a metric on the vector space $\mathbb{R}$ that fails to be associated with a norm in this way.
\end{exercise}

\begin{solution}
    Let us examine the properties that would make $\lvert \lvert \cdot \rvert \rvert$ one by one:
    \begin{itemize}
        \item Suppose $x \in V$.
        Then $\lvert \lvert x \rvert \rvert = d(x, 0) \geq 0$, since $d$ is a metric (and of course $\lvert \lvert x \rvert \rvert$ is well-defined as a finite real number for the same reason).
        \item Suppose $\lvert \lvert x \rvert \rvert = 0$.
        Then $d(x, 0) = 0$, which by the properties of metrics we know is true iff $x = 0$.
        Conversely, if $x = 0$, by the same argument $d(x, 0) = 0 \implies \lvert \lvert x \rvert \rvert = 0$.
        Thus $\lvert \lvert x \rvert \rvert = 0 \iff x = 0$.
        \item For any scalar $a$, we have that $\lvert \lvert a x \rvert \rvert = d(ax, 0) = d(ax, a0) = \lvert a \rvert d(x, 0) = \lvert a \rvert \lvert \lvert x \rvert \rvert$ by the exercise hypothesis.
        \item Pick any two $x, y \in V$.
        Then: 
        $$\lvert \lvert x +y \rvert \rvert = d(x + y, 0) \leq d(x + y, y) + d(y, 0) = d(x + y - y, 0) + d(y, 0) = d(x, 0) + d(y, 0) = \lvert \lvert x \rvert \rvert + \lvert \lvert y \rvert \rvert,$$
        where we used the triangle inequality property for metrics and the fact that $d(x, y) = d(x - y, 0)$.
        Therefore the triangle inequality holds for the proposed norm.
    \end{itemize}
    We have thus shown that $\lvert \lvert \cdot \rvert \rvert$ is indeed a norm, and its usual metric will of course be $d'(x, y) = \lvert \lvert x - y \rvert \rvert = d(x - y, 0) = d(x, y)$, i.e.,\ $d$.

    For the requested example, consider the metric $\sigma(x, y) = \lvert x - y \rvert/(1 + \lvert x - y \rvert)$ from exercise 6 of section 3.1.
    Notice that the ``proposed'' norm would then be $\lvert \lvert x \rvert \rvert = \sigma(x, 0) = \lvert x \rvert/(1 + \lvert x \rvert)$.
    Then for a scalar $a$:
    $$\lvert \lvert a x \rvert \rvert = \lvert a x \rvert/(1 + \lvert a x \rvert) = \lvert a \rvert \cdot \lvert x \rvert/(1 + \lvert a \rvert \cdot \lvert x \rvert),$$
    which we can see does not necessarily equal $\lvert a \rvert \cdot \lvert \lvert x \rvert \rvert$, and therefore the ``proposed'' norm is not really a norm.
    The cause for this is the fact that $\sigma(ax, ay) \neq \lvert a \rvert \sigma(x, y)$ in general, for similar reasons.
\end{solution}

\begin{exercise}{18}
    Show that $\lvert \lvert x \rvert \rvert_{\infty} \leq \lvert \lvert x \rvert \rvert_2 \leq \lvert \lvert x \rvert \rvert_1$ for any $x \in \mathbb{R}^n$.
    Also check that $\lvert \lvert x \rvert \rvert_1 \leq n \lvert \lvert x \rvert \rvert_{\infty}$ and $\lvert \lvert x \rvert \rvert_1 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2$.
\end{exercise}

\begin{solution}
    
    We have that $\lvert \lvert x \rvert \rvert_{\infty} = \max_{1 \leq i \leq n} \lvert x_i \rvert$, while $\lvert \lvert x \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} \lvert x_i \rvert^2}$.
    By definition, there must exist at least one $j \in \{1, 2, \ldots, n\}$ such that $\lvert x_j \rvert = \max_{1 \leq i \leq n} \lvert x_i \rvert$.
    In addition, the square is an increasing function for $x \geq 0$, which would mean that $(\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 = \max_{1 \leq i \leq n} \lvert x_i \rvert^2$.
    We therefore have that:
    $$\sum_{i=1}^{n} \lvert \lvert x_i \rvert \rvert^2 = (\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 + X,$$
    for $X \geq 0$ (the sum of the absolute values of the remaining coordinates).
    Thus $(\lvert \lvert x \rvert \rvert_2)^2 \geq (\lvert \lvert x \rvert \rvert_{\infty})^2$, and by taking square roots we have the first desired result.
    
    We also have that:
    $$(\lvert \lvert x \rvert \rvert_1)^2 = \Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr)\Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr) \geq \sum_{i=1}^{n} \lvert x_i \rvert^2 = (\lvert \lvert x_2 \rvert \rvert)^2,$$
    which we obtain by observing that the product results in a sum of non-negative terms over all pairs of indices $i, j \in \{1, 2, \ldots, n\}$, which of course includes all terms for which $i = j$.
    Taking square roots yields the second desired result.

    For the third result, observe that for each $i, \lvert x_i \rvert \leq \max_{1 \leq i \leq n} \lvert x_i \rvert = \lvert \lvert x \rvert \rvert_{\infty}$.
    This means of course that:
    $$\lvert \lvert x \rvert \rvert_1 = \sum_{i=1}^{n} \lvert x_i \rvert \leq n \lvert \lvert x \rvert \rvert_{\infty}$$
    
    Lastly, we consider the following ``trick'' for the last part.
    For a given $x = (x_1, x_2, \ldots, x_n)$, consider the vectors $y, z \in \mathbb{R}^{n^2}$:
    $$y = \begin{pmatrix}
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}, z = \begin{pmatrix}
        \lvert x_1 \rvert  \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \lvert x_n \rvert\\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}$$
    We have that $\lvert \lvert z \rvert \rvert_2 = \lvert \lvert y \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} n\lvert x_i \rvert^2 } = \sqrt{n} \lvert \lvert x \rvert \rvert_2$,
    while $\langle y, z \rangle = \sum_{i=1}^{n} \sum_{j=1}^{n} \lvert x_i \rvert \cdot \lvert x_j \rvert = (\sum_{i=1}^{n} \lvert x_i \rvert)^2 = (\lvert \lvert x \rvert \rvert_1)^2$.
    Applying the Cauchy-Schwarz inequality on $y, z$, we then have that:
    $$\langle y, z \rangle \leq \lvert \lvert y \rvert \rvert \cdot \lvert \lvert z \rvert \rvert \implies (\lvert \lvert x \rvert \rvert_1)^2 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2 \sqrt{n} \lvert \lvert x \rvert \rvert_2,$$
    which, once more by taking square roots, gives us the last desired result.
\end{solution}

\begin{exercise}{19}
    Show that we have $\sum_{i=1}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (equality in the Cauchy-Schwarz inequality) if and only if $x, y$ are \textit{proportional}, that is, if and only if $x = ay$ or $y = ax$ for some $a \geq 0$.
\end{exercise}

\begin{solution}

    $\impliedby$: Suppose $x = ay$ for some $a \geq 0$ (everything is symmetric for $y = ax$).
    Then:
    $$\lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2 = \lvert \lvert a y \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2 = \lvert a \rvert \cdot \lvert \lvert y \rvert \rvert_2^2,$$
    where we used the ``scalar multiplication''/positive homogeneity property of the norm.
    Furthermore:
    $$\sum_{i = 1}^{n} x_i y_i = \sum_{i=1}^{n} (a y_i) y_i = a \sum_{i=1}^{n} y_i^2 = a \lvert \lvert y \rvert \rvert_2^2,$$ 
    by the definition of the 2-norm on sequences, which yields the desired equality.

    $\implies$: First, if $y = 0$ then this is trivially true. If $y \neq 0$, it's also the case that $\lvert \lvert y \rvert \rvert_2 \neq 0$ and we have the following.
    Consider the beginning of the proof of the Cauchy-Schwarz inequality:
    $$0 \leq \lvert \lvert x + ty \rvert \rvert_2^2 = \lvert \lvert x \rvert \rvert_2^2 + 2t \langle x, y \rangle + t^2 \lvert \lvert y \rvert \rvert_2^2,$$
    for any $t \in \mathbb{R}$. 
    As we saw, the discriminant here is $\Delta = (2\langle x, y \rangle)^2 - 4 \lvert \lvert x \rvert \rvert_2^2 \lvert \lvert y \rvert \rvert_2^2 = 0$, by our hypothesis that $\sum_{i=1}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (since $\langle x, y \rangle = \sum_{i=1}^{n} x_i y_i$).
    Then this means that the corresponding second-degree polynomial of $t$ has a \textit{unique} solution $a$:
    $$a = \frac{-2 \langle x, y \rangle}{2\lvert \lvert y \rvert \rvert_2^2} = -\frac{ \langle x, y \rangle}{\lvert \lvert y \rvert \rvert_2^2}$$
    By the above equation, it must also be the case that:
    $$0 = \lvert \lvert x + ay \rvert \rvert_2^2,$$
    which, by the properties of the norm, means that by necessity $x = -ay$.
    Now we only need to show that $a \leq 0$.
    Observe that the denominator of $a$ is clearly positive as the norm of $y$.
    The numerator equals, by definition, $\sum_{i=}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (by the hypothesis), so it's clearly also non-negative.
    The minus sign in front of the fraction makes $a$ non-positive, which completes the proof.

\end{solution}

\section{More Inequalities}

\begin{exercise}{24}
    The conclusion of Lemma 3.7 (Hölder's inequality) also holds in the case $p = 1$ and $q = \infty$.
    Why?
\end{exercise}

\begin{solution}
    
    As a reminder, we say that a sequence $y$ is in $l_{\infty}$ if $y$ is bounded above, and in that case we define $\lvert \lvert y \rvert \rvert_{\infty} = \sup_{n} \lvert y_n \rvert$. 
    Let us first formally state what Hölder's inequality would assert in this case:

    Given $x \in l_1$ and $y \in l_{\infty}$, we have $\sum_{i=1}^{\infty} \lvert x_i y_i \rvert \leq \lvert \lvert x \rvert \rvert_1 \lvert \lvert y \rvert \rvert_{\infty}$.

    Inded, pick any $n \geq 1$.
    We then have that:
    $$\sum_{i=1}^{n} \lvert x_i y_i \rvert = \sum_{i=1}^{n} \lvert x_i \rvert \cdot \lvert y_i \rvert \leq \sum_{i=1}^{n} \lvert x_i \rvert \cdot \sup_{k} \lvert y_k \rvert = \sup_{k} \lvert y_k \rvert \cdot \sum_{i=1}^{n} \lvert x_i \rvert \leq \lvert \lvert y \rvert \rvert_{\infty} \cdot \lvert \lvert x \rvert \rvert_1,$$
    where we used the definition of the supremum and the fact that $\lvert \lvert x \rvert \rvert_1$ is well-defined.
    Since the partial sums are non-decreasing and bounded, the LHS converges and the inequality (i.e., Hölder's) holds for the infinite series as well.
\end{solution}

\begin{exercise}{25}
    The same techniques can be used to show that $\lvert \lvert f \rvert \rvert_p = (\int_{0}^{1} \lvert f(t) \rvert^p dt)^{1/p}$ defines a norm on $C[0, 1]$ for any $1 < p < \infty$.
    State and prove the analogues of Lemma 3.7 and Theorem 3.8 in this case.
    (Does Lemma 3.7 still hold in this setting for $p = 1, q = \infty$?)
\end{exercise}

\begin{solution}
    
    We begin by noting that due to the absolute value, the functions being integrated are always non-negative.
    For any $t \in (0, 1)$, we can thus apply Young's inequality on $a = \frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}, b = \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q}$:

    $$\frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}\cdot \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q} \leq \frac{1}{p}\cdot \frac{\lvert f(t) \rvert^p}{\lvert \lvert f \rvert \rvert_p^p} + \frac{1}{q} \cdot \frac{\lvert g(t) \rvert^q}{\lvert \lvert g \rvert \rvert_q^{q}}$$

    Since this is true for all $t \in (0, 1)$, we can integrate both sides wrt. $t$ to obtain:
    
    $$\int_{0}^{1}\frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}\cdot \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q} dt \leq \frac{1}{p}\cdot \int_{0}^{1} \frac{\lvert f(t) \rvert^p}{\lvert \lvert f \rvert \rvert_p^p} dt + \frac{1}{q} \cdot \int_{0}^{1} \frac{\lvert g(t) \rvert^q}{\lvert \lvert g \rvert \rvert_q^{q}} dt \implies$$
    $$\frac{1}{\lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert g \rvert \rvert_q} \int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \frac{1}{p}\cdot \frac{1}{\lvert \lvert f \rvert \rvert_p^p}\int_{0}^{1} \lvert f(t) \rvert^p dt + \frac{1}{q} \cdot \frac{1}{\lvert \lvert g \rvert \rvert_q^{q}} \int_{0}^{1} \lvert g(t) \rvert^q dt = \frac{1}{p} + \frac{1}{q} = 1$$

    This yields $\int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert g \rvert \rvert_q$, which would be the equivalent of Hölder's inequality.

    Now, Minkowski's inequality would state firstly that for any $1 < p < \infty$, if the $p$-norm exists for $f, g \in C[0, 1]$, it also exists for $f + g$, and secondly that $\lvert \lvert f + g \rvert \rvert_p \leq \lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p$.

    For the analogue of Minkowski's inequality (``if $f, g \in C[0, 1]$ and $\lvert \lvert f \rvert \rvert_p, \lvert \lvert g \rvert \rvert_p$ both exist, then $\lvert \lvert f + g \rvert \rvert_p$ exists and $\lvert \lvert f + g \rvert \rvert_p \leq \lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p$''), we first prove an analogue of Lemma 3.5 that allows us to immediately obtain that $\lvert \lvert f + g \rvert \rvert_p$ exists.
    More specifically, for any $t \in (0, 1)$, from Lemma 3.5 applied on $a = \lvert f(t) \rvert, b = \lvert g(t) \rvert$, we have that:

    $$(\lvert f(t) \rvert + \lvert g(t) \rvert)^p \leq 2^p(\lvert f(t) \rvert^p + \lvert g(t) \rvert^p)$$

    By the triangle inequality of the absolute value, we also have that $\lvert f(t) + g(t) \rvert^p \leq (\lvert f(t) \rvert+ \lvert g(t) \rvert)^p$ since $p > 1$.
    Therefore, since these hold for any $t \in (0, 1)$:

    $$\lvert f(t) + g(t) \rvert^p \leq 2^p(\lvert f(t) \rvert^p + \lvert g(t) \rvert^p)\implies \int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq 2^p(\int_{0}^{1} \lvert f(t) \rvert^p dt + \int_{0}^{1} \lvert g(t) \rvert^p dt)$$
    $$\implies \lvert \lvert f + g \rvert \rvert_p^p \leq 2^p(\lvert \lvert f \rvert \rvert_p^p + \lvert \lvert g \rvert \rvert_p^p),$$
    
    which imposes a bound on $\lvert \lvert f + g \rvert \rvert_p$, thus showing it exists.

    Now, following the proof in the book, we observe the following regarding $\lvert \lvert f \rvert \rvert_p^{p-1}$, for $1/p + 1/q = 1$:

    $$\lvert \lvert f^{p-1} \rvert \rvert_q = (\int_{0}^{1} \lvert f^{q(p-1)}(t) \rvert dt)^{1/q} = (\int_{0}^{1} \lvert f(t) \rvert^{p} dt)^{1/q} = (\int_{0}^{1} \lvert f(t) \rvert^{p} dt)^{\frac{p-1}{p}} = \lvert \lvert f \rvert \rvert_p^{p - 1}$$

    Now, for any $t \in (0, 1)$:

    $$\lvert f(t) + g(t) \rvert^p = \lvert f(t) + g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1} \leq \lvert f(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1} + \lvert g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1},$$

    which means we can integrate to obtain that:

    $$\int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq \int_{0}^{1} \lvert f(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1}dt + \int_{0}^{1} \lvert g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1}dt$$

    Now, define $q$ such that $1/p + 1/q = 1$, and by Hölder's inequality and the observation above:

    $$\int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert (f + g)^{p-1} \rvert \rvert_q + \lvert \lvert g \rvert \rvert_p \cdot \lvert \lvert (f + g)^{p-1} \rvert \rvert_q \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert f + g \rvert \rvert_{p}^{p-1} + \lvert \lvert g \rvert \rvert_p \cdot \lvert \lvert f + g \rvert \rvert_p^{p-1}$$
    $$\implies \lvert \lvert f + g \rvert \rvert^p \leq \lvert \lvert f + g \rvert \rvert_p^{p-1}(\lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p),$$

    from which Minkowski's inequality follows directly.
    For the case $p = 1, q = \infty$, we have that for any $t \in (0, 1)$:

    $$\lvert f(t) g(t) \rvert = \lvert f(t) \rvert \cdot \lvert g(t) \rvert \leq \lvert f(t) \rvert \cdot \max_{0 \leq t \leq 1} \lvert g(t) \rvert = \lvert f(t) \rvert \cdot \lvert \lvert g \rvert \rvert_{\infty}$$
    $$\implies \int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \lvert \lvert g \rvert \rvert_{\infty} \int_{0}^{1} \lvert f(t) \rvert dt = \lvert \lvert g \rvert \rvert_{\infty} \cdot \lvert \lvert f \rvert \rvert_{1},$$

    showing that Hölder's inequality does indeed hold again.
\end{solution}

\begin{exercise}{26}
    Given $a, b > 0$, show that $\lim_{p \rightarrow \infty}(a^p + b^p)^{1/p} = \max\{a, b\}$. [Hint: If $a < b$ and $r = a/b$ show that $(1/p)\log(1 + r^p) \rightarrow 0$ as $p \rightarrow \infty$.] What happens as $p \rightarrow 0$? as $p \rightarrow -1$? as $p \rightarrow -\infty$?
\end{exercise}

\begin{solution}
    
    First, if $a = b$, the statement is obvious: the quantity inside the limit simplifies to $(2a^p)^{1/p} = 2^{1/p}a$, and as $p \rightarrow \infty, 1/p \rightarrow 0$ thus $2^{1/p}a \rightarrow 1a = a = \max\{a, b\}$.
    Therefore we can assume from now on that $a < b$, and, as indicated in the hint, set $r = a / b < 1$.
    Since $r < 1$, we have that $\lim{_p \rightarrow \infty} r^p \rightarrow 0$ (an exponential with a base less than 1).
    Therefore, by standard limit rules, $\lim_{p \rightarrow \infty} \log(1 + r^p) = \log(1) = 0$. 
    Furthermore $\lim_{p \rightarrow \infty} \frac{1}{p} = 0$, which means $\lim_{p \rightarrow \infty} (1/p)\log(1 + r^p) = 0$.
    Now we apply the following ``trick'':

    $$(a^p + b^p)^{1/p} = e^{\log(a^p + b^p)^{1/p}} = e^{\frac{\log(a^p + b^p)}{p}} = e^{\frac{\log((rb)^p + b^p)}{p}} = e^{\frac{\log(b^p) + \log(1 + r^p)}{p}} = e^{\log(b) + \frac{\log(1 + r^p)}{p}}$$
    
    Clearly, what we showed based on the hint allows us to take limits on both sides and easily obtain that:
    $$\lim_{p \rightarrow \infty}(a^p + b^p)^{1/p} = e^{\log(b)} = b = \max\{a, b\}$$

    Now, for the case of $p \rightarrow 0$, if $a = b, \lim_{p \rightarrow 0}(a^p + b^p)^{1/p} = \lim_{p \rightarrow 0}(2a^p)^{1/p} = \lim_{p \rightarrow 0}(2^{1/p}a)$, and as $p \rightarrow 0+$ this quantity will tend to positive infinity, whereas if $p \rightarrow 0-$ it will tend to zero, thus the limit does not exist.
    If $a < b$, based on the above observation what interests us is $\lim_{p \rightarrow 0} \frac{\log(1 + r^p)}{p}$.
    In this case, $r^p \rightarrow 1$ as $p \rightarrow 0$, and thus the numerator tends to $\log(2)$, thus the entire fraction tends to positive infinity as $p \rightarrow 0+$ and negative infinity as $p \rightarrow 0-$, which means the limit does not exist.

    The expression inside the limit is continuous as a function of $p$ at -1, since $a, b > 0$.
    Therefore $\lim_{p \rightarrow -1}(a^p + b^p)^{1/p} = (\frac{1}{a} + \frac{1}{b})^{-1} = \frac{1}{\frac{1}{a} + \frac{1}{b}} = \frac{ab}{a + b}$.

    Lastly, for $p \rightarrow -\infty$, for $a = b$ we are interested in $\lim_{p \rightarrow -\infty}(2^{1/p}a)$, which is easily seen to equal $a$.
    For $a < b$, we are interested in $\lim_{p \rightarrow -\infty}\frac{\log(1 + r^p)}{p}$.
    Because $0 < r < 1$, the quantity inside the logarithm will tend to positive infinity, whereas the denominator tends to negative infinity.
    Applying L'Hospital's rule and calling this limit $L$ we have that:

    $$L = \lim_{p \rightarrow -\infty} \frac{\log(r)r^p}{1 + r^p} = \lim_{p \rightarrow -\infty} \frac{\log(r)}{1 + \frac{1}{r^p}}$$

    Here the numerator is constant, whereas since $0 < r < 1, r^p \rightarrow \infty$, and thus the denominator will tend to 1.
    This means that $L = \log(r) = \log(a/b) = \log(a) - \log(b)$.
    Going back to our original limit we would have that $\lim_{p \rightarrow -\infty}(a^p + b^p)^{1/p} = e^{\log(b) + \log(a) - \log(b)} = a = \min\{a, b\}$, which we can see was also true for $a = b$.
\end{solution}