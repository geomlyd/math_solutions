\chapter{Metrics and Norms}

\section{Metric Spaces}


\begin{exercise}{2}
    If $d$ is a metric on $M$, show that $\lvert d(x, z) - d(y, z) \rvert \leq d(x, y) $ for any $x, y, z \in M$.
\end{exercise}

\begin{solution}
    
    First, we apply the triangle inequality as follows:
    $$d(x, z) \leq d(x, y) + d(y ,z) \implies d(x, z) - d(y, z) \leq d(x, y)$$
    Another application of it yields:
    $$d(y, z) \leq d(y, x) + d(x, z) \implies -d(y, x) \leq d(x, z) - d(y, z) \implies -d(x, y) \leq d(x, z) - d(y,z),$$
    where we used the symmetry property of metric $d$.
    Putting the two inequalities together results in:
    $$\lvert d(x, z) - d(y, z) \rvert \leq d(x, y)$$
\end{solution}

\begin{exercise}{3}
    As it happens, some of our requirements for a metric are redundant.
    To see why this is so, let $M$ be a set and suppose $d: M \times M \rightarrow \mathbb{R}$ satisfies $d(x, y) = 0$ if and only if $x = y$, and $d(x, y) \leq d(x, z) + d(y, z)$ for all $x, y, z \in M$.
    Prove that $d$ is a metric; that is, show that $d(x, y) \geq 0$ and $d(x, y) = d(y, x)$ hold for all $x, y$.
\end{exercise}

\begin{solution}
    
    Pick any two $x, y \in M$.
    We know then that for any $z \in M$ it holds that $d(x, z) \leq d(x, y) + d(z, y)$.
    More specifically, this holds for $z = x$, in which case we obtain:
    $$d(x, x) \leq d(x, y) + d(x, y) \implies 0 \leq 2d(x, y),$$
    which means that for any two $x, y \in M, d(x, y) \geq 0$.

    For the symmetry propety, once again pick any two $x, y \in M$.
    Then $d(x, y) \leq d(x, z) + d(y, z)$ for any $z$.
    Pick then $z = x$, in which case we obtain $d(x, y) \leq d(x, x) + d(y, x) \implies d(x, y) \leq d(y, x)$.
    By exchanging the roles of $x, y$, we have that $d(y, x) \leq d(y, z) + d(x, z)$ for any $z$.
    Pick $z = y$ to observe that $d(y, x) \leq d(y, y) + d(x, y) \implies d(y, x) \leq d(x, y)$.
    But then $d(x, y) \leq d(y, x) \leq d(x, y)$, thus the only possibility is that $d(x, y) = d(y, x)$.
    Therefore $d$ is indeed a metric.
\end{solution}

\newpage

\begin{exercise}{6}
    If $d$ is any metric on $M$, show that $\rho(x, y) = \sqrt{d(x, y)}, \sigma(x, y) = d(x, y)/(1 + d(x, y))$ and $\tau(x, y) = \min\{d(x, y), 1\}$ are also metrics on $M$.
    [Hint: $\sigma(x, y) = F(d(x, y))$, where $F$ is as in exercise 5.]
\end{exercise}

\begin{solution}
    
    We will solve this as a simple application of 7.
    If $F_1: [0, \infty) \rightarrow [0, \infty), F_1(x) = \sqrt{x}$, then $\rho(x, y) = F_1(d(x, y))$.
    Then $F_1(x)/x = 1/\sqrt{x}$, which is clearly decreasing for $x> 0$, and therefore exercise 7 guarantees that $\rho$ is a metric.

    Similarly, if $F_2: [0, \infty) \rightarrow [0, \infty), F_2(x) = \frac{x}{1 + x}$, then $F_2'(x) = \frac{x + 1 - x}{(1 + x)^2} = \frac{1}{(x + 1)^2}$, which is clearly decreasing as for $x \geq 0$, and therefore $\sigma$ is also a metric.

    Lastly, if $F_3: [0, \infty) \rightarrow [0, \infty), F_3(x) = \min\{x, 1\}$, then $\tau(x, y) = F_3(d(x, y))$ and:
    $$F_3(x)/x = \begin{cases}
        1, & x \leq 1 \\
        \frac{1}{x}, & x > 1
    \end{cases},$$
    which is of course a decreasing function for $ x > 0$, and therefore $\tau$ is a metric.
\end{solution}
    
\begin{exercise}{7}
    Here is a generalization of exercises 5 and 6.
    Let $f: [0, \infty) \rightarrow [0, \infty)$ be increasing and satisfy $f(0) = 0$, and $f(x) > 0$ for all $x > 0$.
    If $f$ also satisfies $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$, then $f \circ d$ is a metric whenever $d$ is a metric.
    Show that each of the following conditions is sufficient to ensure that $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$:

    (a) $f$ has a second derivative satisfying $f'' \leq 0$

    (b) $f$ has a decreasing first derivative

    (c) $f(x)/x$ is decreasing for $x > 0$
    
    [Hint: First show that (a) $\implies$ (b) $\implies$ (c).]
\end{exercise}

\begin{solution}
    
    As indicated in the hint, we first show that (a) $\implies$ (b) $\implies$ (c).
    If, after that, we can show that (c) implies the triangle inequality for $f$, we know also that any of (a), (b) imply it as well (since they imply (c)).

    (a) $\implies$ (b): Since $f'' \leq 0$, $f$ more specifically has a first derivative on every point of $[0, \infty)$.
    From calculus 1, it's known also that this implies that $f'$ is decreasing (one would prove this via the Mean Value Theorem).

    (b) $\implies$ (c): (b) implies that $f$ is differentiable, thus $g(x) = f(x)/x, x > 0$ is also differentiable, with $g'(x) = \frac{f'(x)x - f(x)}{x^2}$.
    If we can show that the numerator here is non-positive for $x > 0$, we'll have shown that $g$ is decreasing.
    Pick any $x > 0$, and apply the Mean Value Theorem on $f$ in the interval $[0, x]$.
    This means that there exists $y \in (0, x)$ such that:
    $$f'(y) = \frac{f(x) - f(0)}{x - 0} = \frac{f(x)}{x},$$
    where we used the fact that $f(0) = 0$.
    Therefore, we have that:
    $$f'(x)x - f(x) = f'(x)x - xf'(y) = x(f'(x) - f'(y)) \leq 0,$$
    since $f'$ is decreasing and $y < x$.
    This completes the proof that (b) $\implies$ (c).

    We now need to show that the triangle inequality for $f$ follows from (c).
    Pick any two $x, y \geq 0$.
    If any of them, or both, are zero, the triangle inequality follows from the fact that $f(0) = 0$.
    If $x, y \neq 0$, then we know that $x \leq x + y, y \leq x + y$, and thus that $\frac{f(x)}{x} \geq \frac{f(x + y)}{x + y}, \frac{f(y)}{y} \geq \frac{f(x + y)}{x + y}$ (by the fact that $f(x)/x$ is decreasing). 
    We can rewrite these as:
    $$(x + y)f(x) \geq xf(x+ y), (x+ y)f(y) \geq yf(x + y),$$
    and now we can add them to obtain that:
    $$(x + y)(f(x) + f(y)) \geq f(x + y)(x + y) \implies f(x) + f(y) \geq f(x + y),$$
    which is of course what we wanted to show.
\end{solution}

\begin{exercise}{10}
    The \textit{Hilbert cube}, $H^{\infty}$, is the collection of all real sequences $x = (x_n)$ with $\lvert x_n \rvert \leq 1, n = 1, 2, \ldots$.

    (i) Show that $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - y_n \rvert$ defines a metric on $H^{\infty}$.

    (ii) Given $x, y \in H^{\infty}$ and $k \in \mathbb{N}$, let $M_k = \max\{ \lvert x_1 - y_1 \rvert, \ldots, \lvert x_k - y_k \rvert\}$.
    Show that $2^{-k}M_k \leq d(x, y) \leq M_k + 2^{-k + 1}$.
\end{exercise}

\begin{solution}
    
    (i) We will make use of exercise 3, which allows us to conclude that $d$ is a metric if the following three things hold:
    \begin{itemize}
        \item First, that $d(x, y)$ is a non-negative real numer for any $x, y$, which, due to the infinite sum, is non-trivial here. 
        Consider any two sequences $x, y$. We know that $\lvert x_n \rvert \leq 1, \lvert y_n \rvert \leq 1$, for all $n$, therefore the maximum value that $\lvert x_n - y_n \rvert$ is 2.
        This means that $2^{-n} \lvert x_n - y_n \rvert \leq 2^{-n+1}$.
        The partial sums that correspond to $d$ are therefore non-decreasing and upper bounded by a convergent geometric series, therefore the series that defines $d$ also converges.
        \item Second, that $d(x, y) = 0$ iff $x = y$.
        If two sequences $x, y$ are equal, then $x_n = y_n$ for all $n$, thus $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - x_n \rvert = 0$.
        Conversely, if $d(x, y) = 0$, then we have a non-decreasing sequence of non-negative partial sums that converges to zero.
        The only way this can happen is if each term of the sequence is zero, i.e.\ if $\lvert x_n - y_n \rvert = 0$ for all $n$, that is, if $x_n = y_n$ for all $n$.
        But this means precisely that $x = y$.
        \item Third, that the triangle inequality holds.
        Pick any three sequences $x, y, z$, and $k \in \mathbb{N}^+$.
        Then:
        $$\sum_{n = 1}^{k} 2^{-n} \lvert x_n - y_n \rvert = \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n + z_n - y_n \rvert \leq \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n \rvert + \sum_{n=1}^{k} 2^{-n} \lvert z_n - y_n \rvert,$$
        by the triangle inequality.
        Since this holds for any $k$ and all series converge (by the first item proven), we know that the inequality holds for the infinite series as well, i.e.\ that:
        $$d(x, y) \leq d(x, z) + d(y, z)$$
    \end{itemize}
    Therefore $d$ is a metric on the Hilbert cube.

    (ii) Pick any two sequences $x, y$ in the Hilbert cube and $k \in \mathbb{N}$.
    We know then by definition that $M_k \geq \lvert x_n - y_n \rvert$ for $i \leq k$.
    In addition, for $n > k$, the $n$-th term of the series that forms the metric $d$ is $2^{-n}\lvert x_n - y_n \rvert \leq 2^{-n+1}$.
    Thus:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert \leq M_k \sum_{n=1}^{k}2^{-n} + \sum_{n=k+1}^{\infty} 2^{-n + 1}$$
    $$ = M_k(1 - 2^{-k}) + 2(2 - 1 - (1 - 2^{-k})) = M_k - 2^{-k}M_k + 2^{-k+1} \leq M_k + 2^{-k+1},$$
    where we used the fact that the geometric series with ratio $r=1/2$ sums to 2, and our second sum here was thus 2 minus the sum of the first $k$ terms minus 1 since the series starts at 1 instead of 0.
    In the last step we also use the fact that $M_k \geq 0$.
    For the other inequality, we have again that:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert$$
    Notice that in order for $x, y$ to minimize this, it should be the case that $x_n - y_n = 0, n > k$, and that the maximum absolute difference $M_k$ is multiplied with the smallest possible quantity up to $k$, that is, with $2^{-k}$, while for all other $n < k, x_n - y_n = 0$.
    This would yield that $d(x, y) \geq M_k2^{-k} + 0 + 0 = M_k2^{-k}$.

\end{solution}

\begin{exercise}{12}
    Check that $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert$ defines a metric on $C[a, b]$, the collection of all continuous, real valued functions defined on the closed interval $[a, b]$.
\end{exercise}

\begin{solution}
    
    Note that here continuity is important to ensure that the difference of any two functions is continuous, and as such achieves a maximum value in any closed interval.
    Using exercise 3, we need to check the following:
    \begin{itemize}
        \item One, that $d(f, g) = 0$ iff $f = g$.
        Suppose first that $d(f, g) = 0$, that is, $\max_{a \leq b} \lvert f(t) - g(t) \rvert = 0$.
        If $f, g$ were not equal, there would exist $x \in [a, b]$ such that $f(x) \neq g(x)$.
        Clearly then $\max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert \geq \lvert f(x) - g(x) \rvert > 0$, which is a contradiction.
        Therefore $f = g$.
        In the other direction, if $f = g$, we equivalently have that $f - g$ is the zero function on $[a, b]$, and therefore $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = 0$.
        \item Two, that for any three $f, g, h \in C[a, b], d(f, h) \leq d(f, g) + d(g, h)$.
        We have first that $d(f, h) = \max_{a \leq t \leq b} \lvert f(t) - h(t) \rvert$.
        Suppose that this function achieves its maximum value on $t_M \in [a, b]$ (if there are more than one, pick any).
        Then $d(f, h) = \lvert f(t_M) - h_(t_M) \rvert = \lvert f(t_M) - g(t_M) + g(t_M) - h(t_M) \rvert \leq \lvert f(t_M) - g(t_M) \rvert + \lvert g(t_M) - h(t_M) \rvert$.
        By definition, it holds that $\lvert f(t_M) - g(t_M) \rvert \leq \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = d(f, g)$, and similarly $\lvert g(t_M) - h(t_M) \rvert \leq d(g, h)$.
        We therefore obtain that $d(f, h) \leq d(f, g) + d(g, h)$.
    \end{itemize}
    Therefore $d$ is indeed a metric on $C[a, b]$.
\end{solution}

\begin{exercise}{14}
    We say that a subset $A$ of a metric space is \textbf{bounded} if there is some $x_0 \in M$ and some constant $C < \infty$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Show that a finite union of bounded sets is again bounded.
\end{exercise}

\begin{solution}
    
    Suppose we have the finite union 
    $$U = A_1 \cup A_2 \cup \ldots \cup A_n$$
    of $n$ bounded subsets of a metric space $M$.
    Let then $x_1, x_2, \ldots, x_n, C_1, \ldots, C_n$ be such that $d(a, x_i) \leq C$ for $a \in A_i$.
    Form the set $\{d(x_1, x_1), d(x_1, x_2), \ldots, d(x_n, x_1)\}$, which, crucially, has a finite number of elements, and therefore also has a maximum element, $M$.
    By the same reasoning, there exists $C$ such that $C = \max\{C_1, \ldots, C_n\}$.
    Now pick any $a \in U$, which must belong in at least one $A_i$.
    Therefore:
    $$d(a, x_1) \leq d(a, x_i) + d(x_i, x_1) \leq C_i + M \leq C + M$$
    If we thus set $x_U = x_1$ and $C_U = C + M$, we have shown precisely that $U$ is bounded.
\end{solution}

\begin{exercise}{15}
    We define the \textbf{diameter} of a nonempty subset $A$ of $M$ by $\text{diam}(A) = \sup\{d(a, b) : a, b \in A\}$.
    Show that $A$ is bounded if and only if $\text{diam}(A)$ is finite.
\end{exercise}

\begin{solution}
    
    $\implies$: Suppose first that $A$ is bounded.
    Then there exists $x_0 \in M, C \in \mathbb{R}$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Pick any two $a, b \in A$.
    We then have that $d(a, b) \leq d(a, x_0) + d(x_0, b) \leq 2C$.
    Therefore the set $\{d(a, b): a, b \in A$\} has an upper bound, namely, $2C$, and therefore also a finite least upper bound, which means precisely that $\text{diam}(A)$ is finite.

    $\impliedby$: Now suppose $\text{diam}(A) = C \in \mathbb{R}$.
    Fix an $a \in A$ (which exists since $A$ is nonempty).
    Pick any $b \in A$, in which case we have that:
    $$d(a, b) \in \{d(x, y): x, y \in A\} \implies d(a, b) \leq \text{diam}(A) = C$$
    If thus set $x_0 = a$ and $C$ the respective constant, we see that the definition of $A$ being bounded is fulfilled.
\end{solution}

\section{Normed Vector Spaces}

\begin{exercise}{16}
    Let $V$ be a vector space, and let $d$ be a metric on $V$ satisfying $d(x, y) = d(x - y, 0)$ and $d(ax, ay) = \lvert a \rvert d(x, y)$ for every $x, y \in V$ and every scalar $a$.
    Show that $\lvert \lvert x \rvert \rvert = d(x, 0)$ defines a norm on $V$ (that has $d$ as its ``usual'' metric).
    Give an example of a metric on the vector space $\mathbb{R}$ that fails to be associated with a norm in this way.
\end{exercise}

\begin{solution}
    Let us examine the properties that would make $\lvert \lvert \cdot \rvert \rvert$ one by one:
    \begin{itemize}
        \item Suppose $x \in V$.
        Then $\lvert \lvert x \rvert \rvert = d(x, 0) \geq 0$, since $d$ is a metric (and of course $\lvert \lvert x \rvert \rvert$ is well-defined as a finite real number for the same reason).
        \item Suppose $\lvert \lvert x \rvert \rvert = 0$.
        Then $d(x, 0) = 0$, which by the properties of metrics we know is true iff $x = 0$.
        Conversely, if $x = 0$, by the same argument $d(x, 0) = 0 \implies \lvert \lvert x \rvert \rvert = 0$.
        Thus $\lvert \lvert x \rvert \rvert = 0 \iff x = 0$.
        \item For any scalar $a$, we have that $\lvert \lvert a x \rvert \rvert = d(ax, 0) = d(ax, a0) = \lvert a \rvert d(x, 0) = \lvert a \rvert \lvert \lvert x \rvert \rvert$ by the exercise hypothesis.
        \item Pick any two $x, y \in V$.
        Then: 
        $$\lvert \lvert x +y \rvert \rvert = d(x + y, 0) \leq d(x + y, y) + d(y, 0) = d(x + y - y, 0) + d(y, 0) = d(x, 0) + d(y, 0) = \lvert \lvert x \rvert \rvert + \lvert \lvert y \rvert \rvert,$$
        where we used the triangle inequality property for metrics and the fact that $d(x, y) = d(x - y, 0)$.
        Therefore the triangle inequality holds for the proposed norm.
    \end{itemize}
    We have thus shown that $\lvert \lvert \cdot \rvert \rvert$ is indeed a norm, and its usual metric will of course be $d'(x, y) = \lvert \lvert x - y \rvert \rvert = d(x - y, 0) = d(x, y)$, i.e.,\ $d$.

    For the requested example, consider the metric $\sigma(x, y) = \lvert x - y \rvert/(1 + \lvert x - y \rvert)$ from exercise 6 of section 3.1.
    Notice that the ``proposed'' norm would then be $\lvert \lvert x \rvert \rvert = \sigma(x, 0) = \lvert x \rvert/(1 + \lvert x \rvert)$.
    Then for a scalar $a$:
    $$\lvert \lvert a x \rvert \rvert = \lvert a x \rvert/(1 + \lvert a x \rvert) = \lvert a \rvert \cdot \lvert x \rvert/(1 + \lvert a \rvert \cdot \lvert x \rvert),$$
    which we can see does not necessarily equal $\lvert a \rvert \cdot \lvert \lvert x \rvert \rvert$, and therefore the ``proposed'' norm is not really a norm.
    The cause for this is the fact that $\sigma(ax, ay) \neq \lvert a \rvert \sigma(x, y)$ in general, for similar reasons.
\end{solution}

\begin{exercise}{18}
    Show that $\lvert \lvert x \rvert \rvert_{\infty} \leq \lvert \lvert x \rvert \rvert_2 \leq \lvert \lvert x \rvert \rvert_1$ for any $x \in \mathbb{R}^n$.
    Also check that $\lvert \lvert x \rvert \rvert_1 \leq n \lvert \lvert x \rvert \rvert_{\infty}$ and $\lvert \lvert x \rvert \rvert_1 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2$.
\end{exercise}

\begin{solution}
    
    We have that $\lvert \lvert x \rvert \rvert_{\infty} = \max_{1 \leq i \leq n} \lvert x_i \rvert$, while $\lvert \lvert x \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} \lvert x_i \rvert^2}$.
    By definition, there must exist at least one $j \in \{1, 2, \ldots, n\}$ such that $\lvert x_j \rvert = \max_{1 \leq i \leq n} \lvert x_i \rvert$.
    In addition, the square is an increasing function for $x \geq 0$, which would mean that $(\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 = \max_{1 \leq i \leq n} \lvert x_i \rvert^2$.
    We therefore have that:
    $$\sum_{i=1}^{n} \lvert \lvert x_i \rvert \rvert^2 = (\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 + X,$$
    for $X \geq 0$ (the sum of the absolute values of the remaining coordinates).
    Thus $(\lvert \lvert x \rvert \rvert_2)^2 \geq (\lvert \lvert x \rvert \rvert_{\infty})^2$, and by taking square roots we have the first desired result.
    
    We also have that:
    $$(\lvert \lvert x \rvert \rvert_1)^2 = \Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr)\Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr) \geq \sum_{i=1}^{n} \lvert x_i \rvert^2 = (\lvert \lvert x_2 \rvert \rvert)^2,$$
    which we obtain by observing that the product results in a sum of non-negative terms over all pairs of indices $i, j \in \{1, 2, \ldots, n\}$, which of course includes all terms for which $i = j$.
    Taking square roots yields the second desired result.

    For the third result, observe that for each $i, \lvert x_i \rvert \leq \max_{1 \leq i \leq n} \lvert x_i \rvert = \lvert \lvert x \rvert \rvert_{\infty}$.
    This means of course that:
    $$\lvert \lvert x \rvert \rvert_1 = \sum_{i=1}^{n} \lvert x_i \rvert \leq n \lvert \lvert x \rvert \rvert_{\infty}$$
    
    Lastly, we consider the following ``trick'' for the last part.
    For a given $x = (x_1, x_2, \ldots, x_n)$, consider the vectors $y, z \in \mathbb{R}^{n^2}$:
    $$y = \begin{pmatrix}
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}, z = \begin{pmatrix}
        \lvert x_1 \rvert  \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \lvert x_n \rvert\\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}$$
    We have that $\lvert \lvert z \rvert \rvert_2 = \lvert \lvert y \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} n\lvert x_i \rvert^2 } = \sqrt{n} \lvert \lvert x \rvert \rvert_2$,
    while $\langle y, z \rangle = \sum_{i=1}^{n} \sum_{j=1}^{n} \lvert x_i \rvert \cdot \lvert x_j \rvert = (\sum_{i=1}^{n} \lvert x_i \rvert)^2 = (\lvert \lvert x \rvert \rvert_1)^2$.
    Applying the Cauchy-Schwarz inequality on $y, z$, we then have that:
    $$\langle y, z \rangle \leq \lvert \lvert y \rvert \rvert \cdot \lvert \lvert z \rvert \rvert \implies (\lvert \lvert x \rvert \rvert_1)^2 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2 \sqrt{n} \lvert \lvert x \rvert \rvert_2,$$
    which, once more by taking square roots, gives us the last desired result.
\end{solution}

\begin{exercise}{19}
    Show that we have $\sum_{i=1}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (equality in the Cauchy-Schwarz inequality) if and only if $x, y$ are \textit{proportional}, that is, if and only if $x = ay$ or $y = ax$ for some $a \geq 0$.
\end{exercise}

\begin{solution}

    $\impliedby$: Suppose $x = ay$ for some $a \geq 0$ (everything is symmetric for $y = ax$).
    Then:
    $$\lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2 = \lvert \lvert a y \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2 = \lvert a \rvert \cdot \lvert \lvert y \rvert \rvert_2^2,$$
    where we used the ``scalar multiplication''/positive homogeneity property of the norm.
    Furthermore:
    $$\sum_{i = 1}^{n} x_i y_i = \sum_{i=1}^{n} (a y_i) y_i = a \sum_{i=1}^{n} y_i^2 = a \lvert \lvert y \rvert \rvert_2^2,$$ 
    by the definition of the 2-norm on sequences, which yields the desired equality.

    $\implies$: First, if $y = 0$ then this is trivially true. If $y \neq 0$, it's also the case that $\lvert \lvert y \rvert \rvert_2 \neq 0$ and we have the following.
    Consider the beginning of the proof of the Cauchy-Schwarz inequality:
    $$0 \leq \lvert \lvert x + ty \rvert \rvert_2^2 = \lvert \lvert x \rvert \rvert_2^2 + 2t \langle x, y \rangle + t^2 \lvert \lvert y \rvert \rvert_2^2,$$
    for any $t \in \mathbb{R}$. 
    As we saw, the discriminant here is $\Delta = (2\langle x, y \rangle)^2 - 4 \lvert \lvert x \rvert \rvert_2^2 \lvert \lvert y \rvert \rvert_2^2 = 0$, by our hypothesis that $\sum_{i=1}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (since $\langle x, y \rangle = \sum_{i=1}^{n} x_i y_i$).
    Then this means that the corresponding second-degree polynomial of $t$ has a \textit{unique} solution $a$:
    $$a = \frac{-2 \langle x, y \rangle}{2\lvert \lvert y \rvert \rvert_2^2} = -\frac{ \langle x, y \rangle}{\lvert \lvert y \rvert \rvert_2^2}$$
    By the above equation, it must also be the case that:
    $$0 = \lvert \lvert x + ay \rvert \rvert_2^2,$$
    which, by the properties of the norm, means that by necessity $x = -ay$.
    Now we only need to show that $a \leq 0$.
    Observe that the denominator of $a$ is clearly positive as the norm of $y$.
    The numerator equals, by definition, $\sum_{i=}^{n} x_i y_i = \lvert \lvert x \rvert \rvert_2 \lvert \lvert y \rvert \rvert_2$ (by the hypothesis), so it's clearly also non-negative.
    The minus sign in front of the fraction makes $a$ non-positive, which completes the proof.

\end{solution}

\begin{exercise}{21}
    Recall that we defined $l_1$ to be the collection of all absolutely summable sequences under the norm $\lvert \lvert x \rvert \rvert_1 = \sum_{n=1}^{\infty} \lvert x_n \rvert$, and we defined $l_{\infty}$ to be the collection of all bounded sequences under the norm $\lvert \lvert x \rvert \rvert_{\infty} = \sup_{n \geq 1} \lvert x_n \rvert$.
    Fill in the details showing that each of these spaces is in fact a normed vector space.
\end{exercise}

\begin{solution}
    
    For each of the two proposed norms, we need to show that they are in fact norms, and also that the sequences which converge under them form a vector space.
    We begin with showing that $l_1$ defines a norm:
    \begin{itemize}
        \item It's obvious from the definition that $\lvert \lvert x \rvert \rvert_1 \geq 0$ whenever it exists, as a sum of non-negative terms.
        \item For any scalar $a$, we have that if for a sequence $x$ the series $\sum_{n=1}^{\infty} \lvert x_n \rvert$ converges to $\lvert \lvert x \rvert \rvert _1$, then by well-known properties of limits, the series $\sum_{n=1}^{\infty} \lvert a x_n \rvert$ will converge to $\lvert a \rvert \cdot \lvert \lvert x \rvert \rvert _1$.
        \item If $\sum_{i=0}^{\infty} \lvert x_n \rvert$ converges to zero, then since all terms are strictly non-negative, it must be the case that all of them are zero, thus that $\lvert \lvert x \rvert \rvert _1 = 0$ implies $x = 0$.
        The converse is obvious.
        \item We now need to prove the triangle inequality.
        Pick $x, y \in l_1$ and any $n > 0$.
        Then, by the triangle inequality for absolute values:

        $$ \sum_{i=1}^{n} \lvert x_i + y_i \rvert \leq \sum_{i=1}^{n} \lvert x_i \rvert + \sum_{i=1}^{n} \lvert y_i \rvert$$

        Notice that as $n \rightarrow \infty$, the two sums on the RHS converge to $\lvert \lvert x \rvert \rvert _1, \lvert \lvert y \rvert \rvert _1$ respectively.
        This imposes a bound on the series corresponding to the LHS, whose partial sums are also non-decreasing.
        Hence, the LHS also converges, and in fact the inequality holds at infinity, yielding $\lvert \lvert x + y \rvert \rvert _1 \leq \lvert \lvert x \rvert \rvert _1 + \lvert \lvert y \rvert \rvert _1$.
    \end{itemize}
    Therefore, $\lvert \lvert x \rvert \rvert_1$ is a norm.
    The third point above showed that $0 \in l_1$, while the triangle inequality proof showed that $l_1$ is closed under addition.
    Lastly, the second point above showed that $l_1$ is closed under scalar multiplication.
    Therefore, we've already shown that $l_1$ is a vector space, and thus $(l_1, \lvert \lvert \cdot \rvert \rvert_1)$ is a normed vector space.

    Now we examine $\lvert \lvert \cdot \rvert \rvert _{\infty}$ in a similar fashion.
    \begin{itemize}
        \item Again, from the definition it is obvious that whenever $x$ is a bounded sequence, $\lvert \lvert x \rvert \rvert _{\infty} \geq 0$.
        \item For any scalar $a$, and any bounded sequence $x$, the sequence $(\lvert a x_i \rvert)$ will have as supremum $\lvert a \rvert \cdot \lvert \lvert x \rvert \rvert _{\infty}$. 
        If this were not the case, one would get a contradiction for $x$ by dividing with $\lvert a \rvert$ (and if $a = 0$, then the supremum of $(\lvert a x_i \rvert)$ is obviously zero).
        \item It's clear that $\lvert \lvert x \rvert \rvert _{\infty}$ is zero iff $x$ is the zero sequence, since the supremum of a set of non-negative numbers is zero iff all of them are zero.
        \item Lastly, the triangle inequality in this case arises as follows.
        Pick any $x, y \in l_{\infty}$.
        Then, for any $i$ we have that $\lvert x_i + y_i \rvert \leq \lvert x_i \rvert + \lvert y_i \rvert \leq \lvert \lvert x \rvert \rvert _{\infty} + \lvert \lvert y \rvert \rvert _{\infty}$ by the definition of the infinity norm.
        But then this sum constitutes an upper bound for $\lvert x_i + y_i \rvert$ for all $i$, which means that by definition the supremum $\sup_{i} \lvert x_i + y_i \rvert$ both exists and is such that $\lvert \lvert x + y \rvert \rvert _{\infty} \leq \lvert \lvert x \rvert \rvert _{\infty} + \lvert \lvert y \rvert \rvert _{\infty}$.
    \end{itemize}
    The above shows both that $\lvert \lvert \cdot \rvert \rvert _{\infty}$ is a norm, and that $l_{\infty}$ is a vector space, thus that $(l_{\infty}, \lvert \lvert \cdot \rvert \rvert _{\infty})$ is a normed vector space.
\end{solution}

\section{More Inequalities}

\begin{exercise}{24}
    The conclusion of Lemma 3.7 (Hölder's inequality) also holds in the case $p = 1$ and $q = \infty$.
    Why?
\end{exercise}

\begin{solution}
    
    As a reminder, we say that a sequence $y$ is in $l_{\infty}$ if $y$ is bounded above, and in that case we define $\lvert \lvert y \rvert \rvert_{\infty} = \sup_{n} \lvert y_n \rvert$. 
    Let us first formally state what Hölder's inequality would assert in this case:

    Given $x \in l_1$ and $y \in l_{\infty}$, we have $\sum_{i=1}^{\infty} \lvert x_i y_i \rvert \leq \lvert \lvert x \rvert \rvert_1 \lvert \lvert y \rvert \rvert_{\infty}$.

    Inded, pick any $n \geq 1$.
    We then have that:
    $$\sum_{i=1}^{n} \lvert x_i y_i \rvert = \sum_{i=1}^{n} \lvert x_i \rvert \cdot \lvert y_i \rvert \leq \sum_{i=1}^{n} \lvert x_i \rvert \cdot \sup_{k} \lvert y_k \rvert = \sup_{k} \lvert y_k \rvert \cdot \sum_{i=1}^{n} \lvert x_i \rvert \leq \lvert \lvert y \rvert \rvert_{\infty} \cdot \lvert \lvert x \rvert \rvert_1,$$
    where we used the definition of the supremum and the fact that $\lvert \lvert x \rvert \rvert_1$ is well-defined.
    Since the partial sums are non-decreasing and bounded, the LHS converges and the inequality (i.e., Hölder's) holds for the infinite series as well.
\end{solution}

\begin{exercise}{25}
    The same techniques can be used to show that $\lvert \lvert f \rvert \rvert_p = (\int_{0}^{1} \lvert f(t) \rvert^p dt)^{1/p}$ defines a norm on $C[0, 1]$ for any $1 < p < \infty$.
    State and prove the analogues of Lemma 3.7 and Theorem 3.8 in this case.
    (Does Lemma 3.7 still hold in this setting for $p = 1, q = \infty$?)
\end{exercise}

\begin{solution}
    
    We begin by noting that due to the absolute value, the functions being integrated are always non-negative.
    For any $t \in (0, 1)$, we can thus apply Young's inequality on $a = \frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}, b = \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q}$:

    $$\frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}\cdot \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q} \leq \frac{1}{p}\cdot \frac{\lvert f(t) \rvert^p}{\lvert \lvert f \rvert \rvert_p^p} + \frac{1}{q} \cdot \frac{\lvert g(t) \rvert^q}{\lvert \lvert g \rvert \rvert_q^{q}}$$

    Since this is true for all $t \in (0, 1)$, we can integrate both sides wrt. $t$ to obtain:
    
    $$\int_{0}^{1}\frac{\lvert f(t) \rvert}{\lvert \lvert f \rvert \rvert_p}\cdot \frac{\lvert g(t) \rvert}{\lvert \lvert g \rvert \rvert_q} dt \leq \frac{1}{p}\cdot \int_{0}^{1} \frac{\lvert f(t) \rvert^p}{\lvert \lvert f \rvert \rvert_p^p} dt + \frac{1}{q} \cdot \int_{0}^{1} \frac{\lvert g(t) \rvert^q}{\lvert \lvert g \rvert \rvert_q^{q}} dt \implies$$
    $$\frac{1}{\lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert g \rvert \rvert_q} \int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \frac{1}{p}\cdot \frac{1}{\lvert \lvert f \rvert \rvert_p^p}\int_{0}^{1} \lvert f(t) \rvert^p dt + \frac{1}{q} \cdot \frac{1}{\lvert \lvert g \rvert \rvert_q^{q}} \int_{0}^{1} \lvert g(t) \rvert^q dt = \frac{1}{p} + \frac{1}{q} = 1$$

    This yields $\int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert g \rvert \rvert_q$, which would be the equivalent of Hölder's inequality.

    Now, Minkowski's inequality would state firstly that for any $1 < p < \infty$, if the $p$-norm exists for $f, g \in C[0, 1]$, it also exists for $f + g$, and secondly that $\lvert \lvert f + g \rvert \rvert_p \leq \lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p$.

    For the analogue of Minkowski's inequality (``if $f, g \in C[0, 1]$ and $\lvert \lvert f \rvert \rvert_p, \lvert \lvert g \rvert \rvert_p$ both exist, then $\lvert \lvert f + g \rvert \rvert_p$ exists and $\lvert \lvert f + g \rvert \rvert_p \leq \lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p$''), we first prove an analogue of Lemma 3.5 that allows us to immediately obtain that $\lvert \lvert f + g \rvert \rvert_p$ exists.
    More specifically, for any $t \in (0, 1)$, from Lemma 3.5 applied on $a = \lvert f(t) \rvert, b = \lvert g(t) \rvert$, we have that:

    $$(\lvert f(t) \rvert + \lvert g(t) \rvert)^p \leq 2^p(\lvert f(t) \rvert^p + \lvert g(t) \rvert^p)$$

    By the triangle inequality of the absolute value, we also have that $\lvert f(t) + g(t) \rvert^p \leq (\lvert f(t) \rvert+ \lvert g(t) \rvert)^p$ since $p > 1$.
    Therefore, since these hold for any $t \in (0, 1)$:

    $$\lvert f(t) + g(t) \rvert^p \leq 2^p(\lvert f(t) \rvert^p + \lvert g(t) \rvert^p)\implies \int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq 2^p(\int_{0}^{1} \lvert f(t) \rvert^p dt + \int_{0}^{1} \lvert g(t) \rvert^p dt)$$
    $$\implies \lvert \lvert f + g \rvert \rvert_p^p \leq 2^p(\lvert \lvert f \rvert \rvert_p^p + \lvert \lvert g \rvert \rvert_p^p),$$
    
    which imposes a bound on $\lvert \lvert f + g \rvert \rvert_p$, thus showing it exists.

    Now, following the proof in the book, we observe the following regarding $\lvert \lvert f \rvert \rvert_p^{p-1}$, for $1/p + 1/q = 1$:

    $$\lvert \lvert f^{p-1} \rvert \rvert_q = (\int_{0}^{1} \lvert f^{q(p-1)}(t) \rvert dt)^{1/q} = (\int_{0}^{1} \lvert f(t) \rvert^{p} dt)^{1/q} = (\int_{0}^{1} \lvert f(t) \rvert^{p} dt)^{\frac{p-1}{p}} = \lvert \lvert f \rvert \rvert_p^{p - 1}$$

    Now, for any $t \in (0, 1)$:

    $$\lvert f(t) + g(t) \rvert^p = \lvert f(t) + g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1} \leq \lvert f(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1} + \lvert g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1},$$

    which means we can integrate to obtain that:

    $$\int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq \int_{0}^{1} \lvert f(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1}dt + \int_{0}^{1} \lvert g(t) \rvert \cdot \lvert f(t) + g(t) \rvert^{p-1}dt$$

    Now, define $q$ such that $1/p + 1/q = 1$, and by Hölder's inequality and the observation above:

    $$\int_{0}^{1} \lvert f(t) + g(t) \rvert^p dt \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert (f + g)^{p-1} \rvert \rvert_q + \lvert \lvert g \rvert \rvert_p \cdot \lvert \lvert (f + g)^{p-1} \rvert \rvert_q \leq \lvert \lvert f \rvert \rvert_p \cdot \lvert \lvert f + g \rvert \rvert_{p}^{p-1} + \lvert \lvert g \rvert \rvert_p \cdot \lvert \lvert f + g \rvert \rvert_p^{p-1}$$
    $$\implies \lvert \lvert f + g \rvert \rvert^p \leq \lvert \lvert f + g \rvert \rvert_p^{p-1}(\lvert \lvert f \rvert \rvert_p + \lvert \lvert g \rvert \rvert_p),$$

    from which Minkowski's inequality follows directly.
    For the case $p = 1, q = \infty$, we have that for any $t \in (0, 1)$:

    $$\lvert f(t) g(t) \rvert = \lvert f(t) \rvert \cdot \lvert g(t) \rvert \leq \lvert f(t) \rvert \cdot \max_{0 \leq t \leq 1} \lvert g(t) \rvert = \lvert f(t) \rvert \cdot \lvert \lvert g \rvert \rvert_{\infty}$$
    $$\implies \int_{0}^{1} \lvert f(t) g(t) \rvert dt \leq \lvert \lvert g \rvert \rvert_{\infty} \int_{0}^{1} \lvert f(t) \rvert dt = \lvert \lvert g \rvert \rvert_{\infty} \cdot \lvert \lvert f \rvert \rvert_{1},$$

    showing that Hölder's inequality does indeed hold again.
\end{solution}

\begin{exercise}{26}
    Given $a, b > 0$, show that $\lim_{p \rightarrow \infty}(a^p + b^p)^{1/p} = \max\{a, b\}$. [Hint: If $a < b$ and $r = a/b$ show that $(1/p)\log(1 + r^p) \rightarrow 0$ as $p \rightarrow \infty$.] What happens as $p \rightarrow 0$? as $p \rightarrow -1$? as $p \rightarrow -\infty$?
\end{exercise}

\begin{solution}
    
    First, if $a = b$, the statement is obvious: the quantity inside the limit simplifies to $(2a^p)^{1/p} = 2^{1/p}a$, and as $p \rightarrow \infty, 1/p \rightarrow 0$ thus $2^{1/p}a \rightarrow 1a = a = \max\{a, b\}$.
    Therefore we can assume from now on that $a < b$, and, as indicated in the hint, set $r = a / b < 1$.
    Since $r < 1$, we have that $\lim{_p \rightarrow \infty} r^p \rightarrow 0$ (an exponential with a base less than 1).
    Therefore, by standard limit rules, $\lim_{p \rightarrow \infty} \log(1 + r^p) = \log(1) = 0$. 
    Furthermore $\lim_{p \rightarrow \infty} \frac{1}{p} = 0$, which means $\lim_{p \rightarrow \infty} (1/p)\log(1 + r^p) = 0$.
    Now we apply the following ``trick'':

    $$(a^p + b^p)^{1/p} = e^{\log(a^p + b^p)^{1/p}} = e^{\frac{\log(a^p + b^p)}{p}} = e^{\frac{\log((rb)^p + b^p)}{p}} = e^{\frac{\log(b^p) + \log(1 + r^p)}{p}} = e^{\log(b) + \frac{\log(1 + r^p)}{p}}$$
    
    Clearly, what we showed based on the hint allows us to take limits on both sides and easily obtain that:
    $$\lim_{p \rightarrow \infty}(a^p + b^p)^{1/p} = e^{\log(b)} = b = \max\{a, b\}$$

    Now, for the case of $p \rightarrow 0$, if $a = b, \lim_{p \rightarrow 0}(a^p + b^p)^{1/p} = \lim_{p \rightarrow 0}(2a^p)^{1/p} = \lim_{p \rightarrow 0}(2^{1/p}a)$, and as $p \rightarrow 0+$ this quantity will tend to positive infinity, whereas if $p \rightarrow 0-$ it will tend to zero, thus the limit does not exist.
    If $a < b$, based on the above observation what interests us is $\lim_{p \rightarrow 0} \frac{\log(1 + r^p)}{p}$.
    In this case, $r^p \rightarrow 1$ as $p \rightarrow 0$, and thus the numerator tends to $\log(2)$, thus the entire fraction tends to positive infinity as $p \rightarrow 0+$ and negative infinity as $p \rightarrow 0-$, which means the limit does not exist.

    The expression inside the limit is continuous as a function of $p$ at -1, since $a, b > 0$.
    Therefore $\lim_{p \rightarrow -1}(a^p + b^p)^{1/p} = (\frac{1}{a} + \frac{1}{b})^{-1} = \frac{1}{\frac{1}{a} + \frac{1}{b}} = \frac{ab}{a + b}$.

    Lastly, for $p \rightarrow -\infty$, for $a = b$ we are interested in $\lim_{p \rightarrow -\infty}(2^{1/p}a)$, which is easily seen to equal $a$.
    For $a < b$, we are interested in $\lim_{p \rightarrow -\infty}\frac{\log(1 + r^p)}{p}$.
    Because $0 < r < 1$, the quantity inside the logarithm will tend to positive infinity, whereas the denominator tends to negative infinity.
    Applying L'Hospital's rule and calling this limit $L$ we have that:

    $$L = \lim_{p \rightarrow -\infty} \frac{\log(r)r^p}{1 + r^p} = \lim_{p \rightarrow -\infty} \frac{\log(r)}{1 + \frac{1}{r^p}}$$

    Here the numerator is constant, whereas since $0 < r < 1, r^p \rightarrow \infty$, and thus the denominator will tend to 1.
    This means that $L = \log(r) = \log(a/b) = \log(a) - \log(b)$.
    Going back to our original limit we would have that $\lim_{p \rightarrow -\infty}(a^p + b^p)^{1/p} = e^{\log(b) + \log(a) - \log(b)} = a = \min\{a, b\}$, which we can see was also true for $a = b$.
\end{solution}

\begin{exercise}{ - Unlisted; Arose from a discussion of exercise 26}
    a) Prove that if $1 \leq p \leq q \leq \infty$, then $l^p \subset l^q$.

    b) If $x \in l_p \subset l_q$ for $1 \leq p \leq q \leq \infty$, then $\lvert \lvert x \rvert \rvert_p \geq \lvert \lvert x \rvert \rvert_q$.

    c) If $x \in l^{p_0}$ for some $p_0$, prove that $\lim_{p \rightarrow \infty} \lvert \lvert x \rvert \rvert_p = \lvert \lvert x \rvert \rvert _\infty$.
\end{exercise}

\begin{solution}
    
    a) Let $x$ be a sequence in $l^p$.
    If $p = q$, then the statement is obvious, so we continue the proof assuming that $p \neq q$.
    In the case where $q = \infty$, we know that if, for some $1 \leq p < \infty, \lvert \lvert x \rvert \rvert _p$ exists, then the sequence of partial sums corresponding to $(\lvert x_i \rvert^p)$ must be bounded and non-decreasing.
    But then the same must hold true for the sequence $(\lvert x_i \rvert)$, which leads us to conclude that $x$ is in fact bounded, and thus has a supremum.
    This means then that $x \in l_q$, since $q = \infty$ and the infinity norm is defined as the supremum of absolute values.

    Now, in the remaining case we have that $1 \leq p < q$ and both are real numbers.
    Note that, by using e.g. the Archimidean property of $\mathbb{R}$, we can write $q = np + \epsilon$, where $n$ is a positive integer and $\epsilon > 0$.
    Consider now examining the first $m$ terms of $x$ in the following way:

    $$\sum_{i=1}^{m} \lvert x_i \rvert^q = \sum_{i=1}^{m} \lvert x_i \rvert^{np + \epsilon} = \sum_{i=1}^{m} \lvert x_i \rvert^{np} \cdot \lvert x_i \rvert^{\epsilon}$$

    Recall that we already showed that if $x \in l_p$, then $\lvert x_i \rvert$ are bounded above, and thus we can say that for every $i, \lvert x_i \rvert^\epsilon \leq S^\epsilon, S =\sup_{j} \lvert x_j \rvert$.
    Furthermore, since $n$ is a positive integer, we have that:
    
    $$\sum_{i=1}^{m} \lvert x_i \rvert^{np} \leq (\sum_{i=1}^{m} \lvert x_i \rvert^p)^n,$$
    since if one expands the power of the RHS, we get at least all terms of the LHS plus possibly more, all of which are non-negative.
    Combining these two facts, we have that:

    $$\sum_{i=1}^{m} \lvert x_i \rvert^q \leq (\sum_{i=1}^{m} \lvert x_i \rvert^p)^n S^\epsilon \leq \lvert \lvert x \rvert \rvert_p^{pn} S^{\epsilon},$$

    which we can safely conclude since the $p$-norm exists.
    But then the partial sums are bounded above for each $m$ and are non-decrasing, meaning that the LHS converges as $m \rightarrow \infty$, and this equals precisely $\lvert \lvert x \rvert \rvert_q^q$.
    Thus $x$ is indeed also in $l^q$.

    b) Consider a sequence $x = (x_1, x_2, \ldots) \in l_p \subset l_q$. 
    The statement is obvious if $p = q$, and also if $x$ is the zero sequence.
    Therefore from now on we assume $p < q, x \neq 0$, which means also $\lvert \lvert x \rvert \rvert _p > 0, \lvert \lvert x \rvert \rvert _q > 0$.
    In the case where $q = \infty$, we have that $\lvert \lvert x \rvert \rvert_q = \sup_{i} \lvert x_i \rvert$.
    For any $m > 0$, we have that:

    $$\sum_{i=1}^{m} \lvert x_i \rvert^p \geq \max_{1 \leq i \leq m} \lvert x \rvert^p$$

    If we take limits on both sides, and by thinking about the $\epsilon$-based definition of the supremum, we can see that this yields $\lvert \lvert x \rvert \rvert _p^p \geq \lvert \lvert x \rvert \rvert _\infty^p \implies \lvert \lvert x \rvert \rvert_p \geq \lvert \lvert x \rvert \rvert_\infty$.


    In the case where $p < q < \infty$, consider the following.
    Let $y$ be the sequence formed by $y_i = \frac{x_i}{\lvert \lvert x \rvert \rvert _p}$.
    Notice first that $\lvert \lvert y \rvert \rvert _p = 1$.
    Notice also that for any $i, \lvert y_i \rvert^p \leq 1$, since $\lvert \lvert y \rvert \rvert _p = 1 \implies \sum_{i=1}^{\infty} \lvert y_i \rvert^p = 1^p$, so any individual term of the series must be at most 1.
    Since $q > p \geq 1$, we have that it must be the case that $q = rp$ for some $r > 1$.
    Then, by using properties of powers, for any $i$:
    
    $$\lvert y_i \rvert^p \leq 1 \implies \lvert y_i \rvert^{rp} \leq \lvert y_i \rvert^p \implies \lvert y_i \rvert^q \leq \lvert y_i \rvert^p $$

    This in turn implies that for any $m > 0$:

    $$\sum_{i=1}^{m} \lvert y_i \rvert^q \leq \sum_{i=1}^{m} \lvert y_i \rvert^p \leq \lvert \lvert y \rvert \rvert _p^p = 1$$

    Since this holds for any $m$, it also holds at infinity, meaning that $\lvert \lvert y \rvert \rvert _q^q \leq 1 = \lvert \lvert y \rvert \rvert _p ^q$, and thus by taking $q$-roots we obtain that $\lvert \lvert y \rvert \rvert_q \leq \lvert \lvert y \rvert \rvert_p$.
    But then by using the definition of $y$:
    
    $$\lvert \lvert y \rvert \rvert_q \leq \lvert \lvert y \rvert \rvert _p \implies \Biggl\lvert \Biggl \lvert \frac{x}{\lvert \lvert x \rvert \rvert _p} \Biggr \rvert \Biggr \rvert _q \leq \Biggl \lvert \Biggl \lvert \frac{x}{\lvert \lvert x \rvert \rvert _p} \Biggr \rvert \Biggr \rvert _p \implies \lvert \lvert x \rvert \rvert _q \leq \lvert \lvert x \rvert \rvert _p,$$

    where we used the ``multiplication by scalar'' property of norms.
    This completes the proof.

    c) First of all, we observe from part (a) that since $\lvert \lvert x \rvert \rvert_{p_0}$ exists for some $p_0$, it will also be the case that $x \in l^p$ for all $p \geq p_0$, as well as that $x \in l^{\infty}$.
    We now have the following:

    $$\lim_{p \rightarrow \infty} \frac{\lvert \lvert x \rvert \rvert _p}{\lvert \lvert x \rvert \rvert _\infty} = \lim_{p \rightarrow \infty} e^{\log \Bigl (\frac{\lvert \lvert x \rvert \rvert _p}{\lvert \lvert x \rvert \rvert _{\infty}} \Bigr )}$$

    We focus on the exponent:

    $$\lim_{p \rightarrow \infty} \log \Biggl( \frac{\lvert \lvert x \rvert \rvert _p}{\lvert \lvert x \rvert \rvert _{\infty}}\Biggr) = \lim_{p \rightarrow \infty} \log \Biggl( \frac{(\sum_{i=1}^{\infty} \lvert x_i \rvert^p)^{1/p}}{(\lvert \lvert x \rvert \rvert _{\infty}^p)^{1/p}}\Biggr) = \lim_{p \rightarrow \infty} \frac{1}{p} \cdot \log \Biggl( \sum_{i=1}^{\infty} \frac{\lvert x_i \rvert^p}{\lvert \lvert x \rvert \rvert _{\infty}^{p}} \Biggr)$$

    Now we make the observation that since the series corresponding to $\lvert \lvert x \rvert \rvert_p^p$ converges, the individual terms tend to zero.
    This means that must exist at least one $j$ such that $\lvert x_j \rvert$ equals the supremum of $x$.
    This is because the only other possibility would be for $\lvert x_i \rvert$ to tend to their supremum without achieving it, but then the series would not converge.
    Therefore, the argument of the logarithm is at least 1 (since $\frac{\lvert x_j \rvert^p}{\lvert \lvert x \rvert \rvert _{\infty}^p} = 1$).
    Additionally, the existence of $\lvert \lvert x \rvert \rvert _p$ implies that the argument does not go to infinity.
    Thus, the numerator of the last limit above tends to $L > 1$ and the denominator to positive infinity, which means that the limit tends to zero.
    But then:

    $$\lim_{p \rightarrow \infty} \frac{\lvert \lvert x \rvert \rvert _p}{\lvert \lvert x \rvert \rvert _{\infty}} = e^0 = 1,$$

    which concludes the proof since $\lvert \lvert x \rvert \rvert _{\infty}$ is constant with respect to the limit variable.
\end{solution}

\begin{exercise}{ - Unlisted; Arose from a discussion of exercise 26}
    Recall that for a continuous function $f \in C([0, 1])$ we define
    
    $$ \lvert \lvert f \rvert \rvert _p = (\int_{0}^{1} \lvert f(x) \rvert^p dx)^{1/p}, \lvert \lvert f \rvert \rvert _{\infty} = \max_{x \in [0, 1]} \lvert f(x) \rvert $$
    Show that:

    a) If $f \in C[0, 1]$, then for $1 \leq p \leq q \leq \infty, \lvert \lvert f \rvert \rvert_{1} \leq \lvert \lvert f \rvert \rvert_{p} \leq \lvert \lvert f \rvert \rvert_{q} \leq \lvert \lvert f \rvert \rvert_{\infty}.$

    b) If $f \in C[0, 1]$, then $\lim_{p \rightarrow \infty} \lvert \lvert f \rvert \rvert_{p} = \lvert \lvert f \rvert \rvert_{\infty}$.
\end{exercise}

\begin{solution}
    
    a) We begin by showing that for any real number $p \geq 1, \lvert \lvert f \rvert \rvert _{p} \leq \lvert \lvert f \rvert \rvert _{\infty}$.
    We do this as follows. First, we have that:
    
    $$\lvert \lvert f \rvert \rvert_p^{p} = \int_{0}^{1} \lvert f(x) \rvert^p dx$$

    Since $p \geq 1$, the $p$-th power is a non-decreasing function, and we know that by definition, $\lvert f^p(x) \rvert \leq \max_{0 \leq x \leq 1} \lvert f^p(x) \rvert$ for any $x \in [0, 1]$. 
    By integrating both sides we have that:

    $$\int_{0}^{1} \lvert f^p(x)  \rvert dx \leq \max_{0 \leq x \leq 1} \lvert f^p(x) \rvert \implies \lvert \lvert f \rvert \rvert _{p}^{p} \leq \lvert \lvert f \rvert \rvert _{\infty}^{p}$$

    By taking $p$-roots, we have the desired inequality.
    Now consider any two real numbers $p, q \geq 1$ such that $p < q$.
    This means that there exists $r > 1$ such that $q = rp$.
    Let $g(x) = f^P(x)$ and set $r'$ to be the number satisfying $1/r + 1/r' = 1$.
    We now apply Hölder's inequality for the functions $g$ and $h(x) = 1$:

    $$\int_{0}^{1} \lvert g(x) \cdot 1 \rvert dx \leq \lvert \lvert g \rvert \rvert_{r} \cdot \lvert \lvert 1 \rvert \rvert_{r'} \implies \int_{0}^{1} \lvert f^p(x) \rvert dx \leq (\int_{0}^{1} \lvert f^{rp}(x) \rvert)^{(1/r)} \implies (\int_{0}^{1} \lvert f^p(x) \rvert dx)^{r} \leq \int_{0}^{1} \lvert f^{rp}(x) \rvert dx,$$

    where for the last step we used the fact that $r > 1$.
    Continuing:

    $$(\int_{0}^{1} \lvert f^p(x) \rvert dx)^{r} \leq \int_{0}^{1} \lvert f^{rp}(x) \rvert dx \implies (\int_{0}^{1} \lvert f^p(x) \rvert dx)^{q/p} \leq \int_{0}^{1} \lvert f^{q}(x) \rvert dx$$
    $$ \implies (\int_{0}^{1} \lvert f^p(x) \rvert dx)^{1/p} \leq (\int_{0}^{1} \lvert f^{q}(x) \rvert dx)^{1/q} \implies \lvert \lvert f \rvert \rvert _{p} \leq \lvert \lvert f \rvert \rvert _{q}$$
    
    This completes the proof of the remaining inequalities, thus establishing that for $1 \leq p \leq q \leq \infty, \lvert \lvert f \rvert \rvert _{1} \leq \lvert \lvert f \rvert \rvert _{p} \leq \lvert \lvert f \rvert \rvert _{q} \leq \lvert \lvert f \rvert \rvert _{\infty}$.

    b) From part (a) we already know that as $p \rightarrow \infty, \lvert \lvert f \rvert \rvert _p$ always exists, and in fact is bounded above by $\lvert \lvert f \rvert \rvert _{\infty}$ and is non-decreasing (since for $p \leq q, \lvert \lvert f \rvert \rvert _p \leq \lvert \lvert f \rvert \rvert _q$).
    This means that $\lim_{p \rightarrow \infty} \lvert \lvert f \rvert \rvert _p$ exists, and will equal the supremum of the set $S = \{ \lvert \lvert f \rvert \rvert _{p}, p \geq 1\}$.
    We thus only need to show that this supremum is in fact $\lvert \lvert f \rvert \rvert _{\infty}$.
    Since we already know that this constitutes an upper bound for $S$, assume that the supremum of $S$ is $M < \lvert \lvert f \rvert \rvert_{\infty}$.
    Then, by the definition of the infinity norm, there exists $x_0 \in [0, 1]$ such that $\lvert f(x_0) \rvert = \lvert \lvert f \rvert \rvert _{\infty} > M$.
    For the sake of simplicity, we shall assume that $x_0 \in (0, 1)$: as will become clear, in the ``edge cases'' the only thing that changes is that some quantities lack a factor of 2.
    Set now $\epsilon = \lvert f(x_0) \rvert - M > 0$.
    Since $f$ is continuous, there must exist $\delta > 0$ such that:
    
    $$\lvert x - x_0 \rvert < \delta \implies \lvert f(x_0) \rvert - \lvert f(x) \rvert < \epsilon,$$

    where the absolute value in the second inequality can be omitted since $\lvert f(x_0) \rvert$ is the maximum value of $\lvert f \rvert$.
    Now, this can be rewritten as $\lvert f(x) \rvert > \lvert f(x_0) \rvert - \epsilon = \lvert f(x_0) \rvert - \lvert f(x_0) \rvert + M = M$, which means that in the interval $(x_0 - \delta, x_0 + \delta), \lvert f(x) \rvert > M$.
    Therefore, for all $x$ in this interval we have that $\lvert f(x) \rvert > M \implies \lvert f^p(x) \rvert > M^p$. 
    By integrating:

    $$\int_{x_0 - \delta}^{x_0 + \delta} \lvert f^p(x) \rvert dx > 2M^p \delta \implies \Biggl(\int_{x_0 - \delta}^{x_0 + \delta} \lvert f^p(x) \rvert dx\Biggr)^{1/p} > (2\delta)^{1/p}M$$

    We now have that $\lvert \lvert f \rvert \rvert _p$ is greater than or equal to the LHS here, since $(x_0 - \delta, x_0 + \delta) \subset (0, 1)$.
    Furthermore, as $p \rightarrow \infty$, the RHS tends to M, since $\delta$ is constant.
    By taking limits, we can thus obtain that:

    $$\lim_{p \rightarrow \infty} \lvert \lvert f \rvert \rvert _p \geq M$$

    Now we observe that we can repeat the entirety of the argument above for some $N$ with $M < N < \lvert \lvert f \rvert \rvert _{\infty}$.
    But then this means also that $\lim_{p \rightarrow \infty} \lvert \lvert f \rvert \rvert _p \geq N$, and then the $\epsilon$-based limit definition would allow us to find $p$ such that $\lvert \lvert f \rvert \rvert _p > M$, which contradicts the defining property of $M$ as the supremum of all $\lvert \lvert f \rvert \rvert _p$.

    Therefore, we have arrived at a contradiction, and thus it must be the case that $\lim_{p \rightarrow \infty} \lvert \lvert f \rvert \rvert _p = \lvert \lvert f \rvert \rvert _{\infty}$.

\end{solution}

\section{Limits in Metric Spaces}

\begin{exercise}{30}
    If $A \subset B$, show that $\text{diam}(A) \leq \text{diam}(B)$.
\end{exercise}

\begin{solution}
    
    Consider the definition of $\text{diam}(A): \text{diam}(A) = \{\sup\{d(a, b): a, b \in A\}$.
    Because $A \subset B$, we have that any two $a, b \in A$ also belong in $B$.
    Therefore, the set $S_1$ over which the diameter is computed for $B$ is a superset of the set $S_2$ over which the diameter is computed for $A$.
    But then exercise 2 of Chapter 1 guarantees that $\sup S_1 \leq \sup S_2$, which means precisely that $\text{diam}(A) \leq \text{diam}(B)$.
\end{solution}

\begin{exercise}{32}
    In a normed vector space $(V, \lvert \lvert \cdot \rvert \rvert)$ show that $B_r(x) = x + B_r(0) = \{x + y: \lvert \lvert y \rvert \rvert < r\}$ and that $B_r(0) = rB_1(0) = \{r x : \lvert \lvert x \rvert \rvert <1 \}$.
\end{exercise}

\begin{solution}
    
    Let us call $S = \{x + y: \lvert \lvert y \rvert \rvert < r\}$, in which case we asked to show that $B_r(x) = S$.
    First, let $z \in B_r(x)$.
    By definition, this means that $d(x, z) < r$.
    Recall that in a normed vector space we have that $\lvert \lvert z - x \rvert \rvert = d(x, y)$ (unless a different metric is explicitly specified).
    Observe then that we can write $z = x + (z - x)$, where $\lvert \lvert z - x \rvert \rvert = d(z, x) = d(x, z) < r$. 
    By setting $y = z - x$, we obtain that $z \in S$.
    Therefore, $B_r(x) \subset S$.

    In the other direction, suppose $z \in S$, which means that there exists $y, \lvert \lvert y \rvert \rvert < r$ such that $z = x + y$.
    Then we observe that $d(x, z) = \lvert \lvert z - x \rvert \rvert = x + y - x \rvert \rvert = \lvert \lvert y \rvert \rvert < r$.
    By definition, this means that $z \in B_r(x)$, and thus that $S \subset B_r(x)$, meaning that in fact $S = B_r(x)$.

    For the second part of the exercise, set $S = \{rx : \lvert \lvert x \rvert \rvert < 1 \}$.
    First, suppose $x \in B_r(0)$, which means $ \lvert \lvert x \rvert \rvert < r$.
    By using the scalar multiplication properties of vector spaces, we can then write $x = r \cdot \frac{x}{r}$.
    Set $y = \frac{x}{r}$, in which case $\lvert \lvert y \rvert \rvert = \lvert \lvert \frac{x}{r} \rvert \rvert = \frac{1}{\lvert r \rvert} \lvert \lvert x \rvert \rvert < 1$. 
    This means that $x$ can be written in the form $ry, \lvert \lvert y \rvert \rvert < 1$, thus that $x \in S$, thus that $B_r(0) \subset S$.
    Conversely, assume $x \in S$, which means $x = ry, \lvert \lvert y \rvert \rvert < 1$.
    But then $\lvert \lvert x \rvert \rvert = \lvert \lvert r y \rvert \rvert =  \lvert r \rvert \cdot \lvert \lvert y \rvert \rvert < r$, i.e. that $x \in B_r(0)$, and thus that $S \subset B_r(0)$, completing the proof that $B_r(0) = S$.
\end{solution}

\begin{exercise}{33}
    Limits are unique. [Hint: $d(x, y) \leq d(x, x_n) + d(x_n, y)$.]
\end{exercise}

\begin{solution}
    
    Suppose that a sequence $(x_n)$ in a metric space $M$ converges to two $a, b \in M$ such that $a \neq b$.
    By the definition of metrics, we know then that it must be the case that $d(a, b) > 0$.
    Set $\epsilon = d(a, b) > 0$, in which case by the definition of the limit in a metric space there must exist $N_1, N_2$ such that $d(x_n, a) < \epsilon/4, d(x_n, b) < \epsilon/4$ for $n \geq N_1, n \geq N_2$ respectively.
    If we then pick $n > \max\{N_1, N_2\}$ we have that both of these inequalities hold for $x_n$.
    Recall the triangle inequality for metrics:

    $$d(a, b) \leq d(a, x_n) + d(x_n, b) < \frac{\epsilon}{4} + \frac{\epsilon}{4} = \frac{\epsilon}{2} < \epsilon = d(a, b)$$

    This is a clear contradiction, which means that if a sequence in a metric space has a limit, the limit has to be unique.
\end{solution}

\begin{exercise}{34}
    If $x_n \rightarrow x$ in $(M, d)$, show that $d(x_n, y) \rightarrow d(x, y)$ for any $y \in M$.
    More generally, if $x_n \rightarrow x, y_n \rightarrow y$, show that $d(x_n, y_n) \rightarrow d(x, y)$.
\end{exercise}

\begin{solution}
    
    Pick $\epsilon > 0$.
    Because $x_n r\rightarrow x$, we know that there exists $N > 0$ such that $d(x_n, x) < \epsilon$ for all $n \geq N$.
    By the triangle inequality, for $n \geq N$ we have that:
    $$d(x_n, y) \leq d(x, y) + d(x_n, x) < \epsilon + d(x, y) \implies d(x_n, y) - d(x, y) < \epsilon$$
    By exercise 2 of Chapter 3, we also have that:
    $$\lvert d(x_n, x) - d(y, x) \rvert \leq d(x_n, y) \implies -d(x_n, y) \leq d(x_n, x) - d(x, y) < \epsilon - d(x, y) $$
    $$\implies d(x_n, y) - d(x, y) > -\epsilon$$

    Combining these two inequalities we obtain that $\lvert d(x_n, y) - d(x, y) \rvert < \epsilon$ for all $n > N$, which is precisely the definition of $d(x_n, y) \rightarrow d(x, y)$.

    For the second, more general statement, pick again $\epsilon > 0$.
    Then there exist $N_1, N_2$ such that $d(x_n, x) < \frac{\epsilon}{2}, d(y_n, y) < \frac{\epsilon}{2}$ whenever $n > N_1, n > N_2$ respectively.
    Set then $N = \max\{N_1, N_2\}$.
    By the triangle inequality for $n > N$ we have that:

    $$d(x_n, y_n) \leq d(x_n, x) + d(x, y_n) \leq d(x_n, x) + d(x, y) + d(y, y_n) < d(x, y) + \frac{\epsilon}{2} + \frac{\epsilon}{2}$$
    $$\implies d(x_n, y_n) - d(x, y) < \epsilon$$
    
    $$d(x, y) \leq d(x, x_n) + d(x_n, y) \leq d(x, x_n) + d(x_n, y_n) + d(y_n, y) < \frac{\epsilon}{2} + \frac{\epsilon}{2} + d(x_n, y_n)$$
    $$\implies d(x,y) - d(x_n, y_n) < \epsilon \implies - \epsilon < d(x_n, y_n) - d(x, y)$$

    Putting together these two inequalities results in $\lvert d(x_n, y_n) - d(x,y) \rvert < \epsilon$ for $n > N$, which is precisely the definition of $d(x_n, y_n) \rightarrow d(x, y)$.
\end{solution}

\begin{exercise}{35}
    If $x_n \rightarrow x$, then $x_{n_k} \rightarrow x$ for any subsequence $(x_{n_k})$ of $(x_n)$.
\end{exercise}

\begin{solution}
    
    Since $x_n \rightarrow x$, for any given $\epsilon > 0$ we can always find $N > 0 $ such that for all $n \geq N, d(x_n, x) < \epsilon$.
    Because $(x_{n_k})$ contains infinite terms of $(x_n)$ selected in an order which maintains the order of indices, it must be the case that for $n_k \geq N$, $d(x_{n_k}, x) < \epsilon$.
    We conclude that $x_{n_k} \rightarrow x$.

\end{solution}

\begin{exercise}{36}
    A convergent sequence is Cauchy, and a Cauchy sequence is bounded (that is, the set $\{x_n : n \geq 1\}$ is bounded).
\end{exercise}

\begin{solution}
    
    We begin by showing that a convergent sequence is Cauchy.
    Pick any $\epsilon > 0$.
    Because the sequence converges, say to $x$, there exists $N > 0$ such that whenever $n \geq N$ it is the case that $d(x_n, x) < \epsilon/2$.
    For any two such $n_1, n_2 \geq N$ we then have:
    
    $$d(x_{n_1}, x_{n_2}) \leq d(x_{n_1}, x) + d(x, x_{n_2}) < \epsilon,$$
    
    which is precisely the definition of $x$ being Cauchy.

    Now assume $(x_n)$ is Cauchy.
    Pick e.g. $\epsilon = 1$, and find $N > 0$ such that for $n_1, n_2 \geq N$ it holds that $d(x_{n_1}, x_{n_2}) < \epsilon$.
    Consider the set $\{d(x_n, x_1), 1 \leq n \leq N\}$.
    This has a finite number of non-negative elements, and as such a well-defined non-negative maximum $M$.
    For any $n > N$ we then have that:

    $$d(x_n, x_1) \leq d(x_n, x_N) + d(x_N, x_1) < M + \epsilon,$$

    and therefore all elements of the sequence satisfy $d(x_n, x_1) < M + \epsilon$, i.e. they are contained in the open ball $B_{M + \epsilon}(x_1)$, which is to say that the sequence is bounded.

\end{solution}

\begin{exercise}{37}
    A Cauchy sequence with a convergent subsequence converges.
\end{exercise}

\begin{solution}
    
    Let $(x_n)$ be a Cauchy sequence with a convergent subsequence $(x_{n_k}) \rightarrow x$.
    Pick any $\epsilon > 0$.
    Because $(x_{n_k}) \rightarrow x$, there exists $N_1 > 0$ such that for $n_k \geq N_1$ it holds that $d(x_{n_k}, x) < \epsilon/2$.
    Furthermore, because $(x_n)$ is Cauchy, there exists $N_2 > 0$ such that for $i, j \geq N_2$ it holds that $d(x_{i}, x_{j}) < \epsilon/2$.
    Set $N = \max\{N_1, N_2\}$ and pick any $n > N$.
    Then, by the triangle inequality:

    $$d(x_n, x) \leq d(x_n, x_{N_1}) + d(x_{N_1}, x) < \epsilon/2 + \epsilon/2 = \epsilon$$

    But then this means that $(x_n)$ converges to $x$, which completes the proof.
\end{solution}

\begin{exercise}{39}
    If every subsequence of $(x_n)$ has a \textit{further} subsequence that converges to $x$, then $(x_n)$ converges to x.
\end{exercise}

\begin{solution}

    By way of contradiction, assume $(x_n)$ does not converge to $x$.
    Then there exists $\epsilon > 0$ such that for all $N > 0$ there exists $n \geq N$ such that $d(x_n, x) \geq \epsilon$.
    Consider constructing the following subsequence of $(x_n)$: for any $i > 0$, the $i$-th element of the subsequence equals the first $x_n$ such that $d(x_n, x) \geq \epsilon, n \geq i$ and $x_n$ has not been selected before.
    By the hypothesis above, this is always well-defined.

    As a subsequence of $(x_n)$, by the hypothesis of the exercise this subsequence will have a further subsequence converging to $x$.
    Notice, however, that by construction all elements of this ``further subsequence'' are such that $d(x_{n_{k_l}}, x) \geq \epsilon$, which contradicts convergence to $x$.
    Therefore, $(x_n)$ itself must converge to $x$.
\end{solution}

\begin{exercise}{41}
    Given $x, y \in l_2$, recall that $\langle x, y \rangle = \sum_{i=1}^{\infty} x_i y_i$.
    Show that if $x^{(k)} \rightarrow x$ and $y^{(k)} \rightarrow y$ in $l_2$, then $\langle x^{(k)}, y^{(k)} \rangle \rightarrow \langle x, y \rangle$.
\end{exercise}

\begin{solution}
    
    
\end{solution}