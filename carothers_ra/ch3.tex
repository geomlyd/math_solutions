\chapter{Metrics and Norms}

\section{Metric Spaces}


\begin{exercise}{2}
    If $d$ is a metric on $M$, show that $\lvert d(x, z) - d(y, z) \rvert \leq d(x, y) $ for any $x, y, z \in M$.
\end{exercise}

\begin{solution}
    
    First, we apply the triangle inequality as follows:
    $$d(x, z) \leq d(x, y) + d(y ,z) \implies d(x, z) - d(y, z) \leq d(x, y)$$
    Another application of it yields:
    $$d(y, z) \leq d(y, x) + d(x, z) \implies -d(y, x) \leq d(x, z) - d(y, z) \implies -d(x, y) \leq d(x, z) - d(y,z),$$
    where we used the symmetry property of metric $d$.
    Putting the two inequalities together results in:
    $$\lvert d(x, z) - d(y, z) \rvert \leq d(x, y)$$
\end{solution}

\begin{exercise}{3}
    As it happens, some of our requirements for a metric are redundant.
    To see why this is so, let $M$ be a set and suppose $d: M \times M \rightarrow \mathbb{R}$ satisfies $d(x, y) = 0$ if and only if $x = y$, and $d(x, y) \leq d(x, z) + d(y, z)$ for all $x, y, z \in M$.
    Prove that $d$ is a metric; that is, show that $d(x, y) \geq 0$ and $d(x, y) = d(y, x)$ hold for all $x, y$.
\end{exercise}

\begin{solution}
    
    Pick any two $x, y \in M$.
    We know then that for any $z \in M$ it holds that $d(x, z) \leq d(x, y) + d(z, y)$.
    More specifically, this holds for $z = x$, in which case we obtain:
    $$d(x, x) \leq d(x, y) + d(x, y) \implies 0 \leq 2d(x, y),$$
    which means that for any two $x, y \in M, d(x, y) \geq 0$.

    For the symmetry propety, once again pick any two $x, y \in M$.
    Then $d(x, y) \leq d(x, z) + d(y, z)$ for any $z$.
    Pick then $z = x$, in which case we obtain $d(x, y) \leq d(x, x) + d(y, x) \implies d(x, y) \leq d(y, x)$.
    By exchanging the roles of $x, y$, we have that $d(y, x) \leq d(y, z) + d(x, z)$ for any $z$.
    Pick $z = y$ to observe that $d(y, x) \leq d(y, y) + d(x, y) \implies d(y, x) \leq d(x, y)$.
    But then $d(x, y) \leq d(y, x) \leq d(x, y)$, thus the only possibility is that $d(x, y) = d(y, x)$.
    Therefore $d$ is indeed a metric.
\end{solution}

\newpage

\begin{exercise}{6}
    If $d$ is any metric on $M$, show that $\rho(x, y) = \sqrt{d(x, y)}, \sigma(x, y) = d(x, y)/(1 + d(x, y))$ and $\tau(x, y) = \min\{d(x, y), 1\}$ are also metrics on $M$.
    [Hint: $\sigma(x, y) = F(d(x, y))$, where $F$ is as in exercise 5.]
\end{exercise}

\begin{solution}
    
    We will solve this as a simple application of 7.
    If $F_1: [0, \infty) \rightarrow [0, \infty), F_1(x) = \sqrt{x}$, then $\rho(x, y) = F_1(d(x, y))$.
    Then $F_1(x)/x = 1/\sqrt{x}$, which is clearly decreasing for $x> 0$, and therefore exercise 7 guarantees that $\rho$ is a metric.

    Similarly, if $F_2: [0, \infty) \rightarrow [0, \infty), F_2(x) = \frac{x}{1 + x}$, then $F_2'(x) = \frac{x + 1 - x}{(1 + x)^2} = \frac{1}{(x + 1)^2}$, which is clearly decreasing as for $x \geq 0$, and therefore $\sigma$ is also a metric.

    Lastly, if $F_3: [0, \infty) \rightarrow [0, \infty), F_3(x) = \min\{x, 1\}$, then $\tau(x, y) = F_3(d(x, y))$ and:
    $$F_3(x)/x = \begin{cases}
        1, & x \leq 1 \\
        \frac{1}{x}, & x > 1
    \end{cases},$$
    which is of course a decreasing function for $ x > 0$, and therefore $\tau$ is a metric.
\end{solution}
    
\begin{exercise}{7}
    Here is a generalization of exercises 5 and 6.
    Let $f: [0, \infty) \rightarrow [0, \infty)$ be increasing and satisfy $f(0) = 0$, and $f(x) > 0$ for all $x > 0$.
    If $f$ also satisfies $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$, then $f \circ d$ is a metric whenever $d$ is a metric.
    Show that each of the following conditions is sufficient to ensure that $f(x + y) \leq f(x) + f(y)$ for all $x, y \geq 0$:

    (a) $f$ has a second derivative satisfying $f'' \leq 0$

    (b) $f$ has a decreasing first derivative

    (c) $f(x)/x$ is decreasing for $x > 0$
    
    [Hint: First show that (a) $\implies$ (b) $\implies$ (c).]
\end{exercise}

\begin{solution}
    
    As indicated in the hint, we first show that (a) $\implies$ (b) $\implies$ (c).
    If, after that, we can show that (c) implies the triangle inequality for $f$, we know also that any of (a), (b) imply it as well (since they imply (c)).

    (a) $\implies$ (b): Since $f'' \leq 0$, $f$ more specifically has a first derivative on every point of $[0, \infty)$.
    From calculus 1, it's known also that this implies that $f'$ is decreasing (one would prove this via the Mean Value Theorem).

    (b) $\implies$ (c): (b) implies that $f$ is differentiable, thus $g(x) = f(x)/x, x > 0$ is also differentiable, with $g'(x) = \frac{f'(x)x - f(x)}{x^2}$.
    If we can show that the numerator here is non-positive for $x > 0$, we'll have shown that $g$ is decreasing.
    Pick any $x > 0$, and apply the Mean Value Theorem on $f$ in the interval $[0, x]$.
    This means that there exists $y \in (0, x)$ such that:
    $$f'(y) = \frac{f(x) - f(0)}{x - 0} = \frac{f(x)}{x},$$
    where we used the fact that $f(0) = 0$.
    Therefore, we have that:
    $$f'(x)x - f(x) = f'(x)x - xf'(y) = x(f'(x) - f'(y)) \leq 0,$$
    since $f'$ is decreasing and $y < x$.
    This completes the proof that (b) $\implies$ (c).

    We now need to show that the triangle inequality for $f$ follows from (c).
    Pick any two $x, y \geq 0$.
    If any of them, or both, are zero, the triangle inequality follows from the fact that $f(0) = 0$.
    If $x, y \neq 0$, then we know that $x \leq x + y, y \leq x + y$, and thus that $\frac{f(x)}{x} \geq \frac{f(x + y)}{x + y}, \frac{f(y)}{y} \geq \frac{f(x + y)}{x + y}$ (by the fact that $f(x)/x$ is decreasing). 
    We can rewrite these as:
    $$(x + y)f(x) \geq xf(x+ y), (x+ y)f(y) \geq yf(x + y),$$
    and now we can add them to obtain that:
    $$(x + y)(f(x) + f(y)) \geq f(x + y)(x + y) \implies f(x) + f(y) \geq f(x + y),$$
    which is of course what we wanted to show.
\end{solution}

\begin{exercise}{10}
    The \textit{Hilbert cube}, $H^{\infty}$, is the collection of all real sequences $x = (x_n)$ with $\lvert x_n \rvert \leq 1, n = 1, 2, \ldots$.

    (i) Show that $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - y_n \rvert$ defines a metric on $H^{\infty}$.

    (ii) Given $x, y \in H^{\infty}$ and $k \in \mathbb{N}$, let $M_k = \max\{ \lvert x_1 - y_1 \rvert, \ldots, \lvert x_k - y_k \rvert\}$.
    Show that $2^{-k}M_k \leq d(x, y) \leq M_k + 2^{-k + 1}$.
\end{exercise}

\begin{solution}
    
    (i) We will make use of exercise 3, which allows us to conclude that $d$ is a metric if the following three things hold:
    \begin{itemize}
        \item First, that $d(x, y)$ is a non-negative real numer for any $x, y$, which, due to the infinite sum, is non-trivial here. 
        Consider any two sequences $x, y$. We know that $\lvert x_n \rvert \leq 1, \lvert y_n \rvert \leq 1$, for all $n$, therefore the maximum value that $\lvert x_n - y_n \rvert$ is 2.
        This means that $2^{-n} \lvert x_n - y_n \rvert \leq 2^{-n+1}$.
        The partial sums that correspond to $d$ are therefore non-decreasing and upper bounded by a convergent geometric series, therefore the series that defines $d$ also converges.
        \item Second, that $d(x, y) = 0$ iff $x = y$.
        If two sequences $x, y$ are equal, then $x_n = y_n$ for all $n$, thus $d(x, y) = \sum_{n=1}^{\infty} 2^{-n} \lvert x_n - x_n \rvert = 0$.
        Conversely, if $d(x, y) = 0$, then we have a non-decreasing sequence of non-negative partial sums that converges to zero.
        The only way this can happen is if each term of the sequence is zero, i.e.\ if $\lvert x_n - y_n \rvert = 0$ for all $n$, that is, if $x_n = y_n$ for all $n$.
        But this means precisely that $x = y$.
        \item Third, that the triangle inequality holds.
        Pick any three sequences $x, y, z$, and $k \in \mathbb{N}^+$.
        Then:
        $$\sum_{n = 1}^{k} 2^{-n} \lvert x_n - y_n \rvert = \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n + z_n - y_n \rvert \leq \sum_{n=1}^{k} 2^{-n} \lvert x_n - z_n \rvert + \sum_{n=1}^{k} 2^{-n} \lvert z_n - y_n \rvert,$$
        by the triangle inequality.
        Since this holds for any $k$ and all series converge (by the first item proven), we know that the inequality holds for the infinite series as well, i.e.\ that:
        $$d(x, y) \leq d(x, z) + d(y, z)$$
    \end{itemize}
    Therefore $d$ is a metric on the Hilbert cube.

    (ii) Pick any two sequences $x, y$ in the Hilbert cube and $k \in \mathbb{N}$.
    We know then by definition that $M_k \geq \lvert x_n - y_n \rvert$ for $i \leq k$.
    In addition, for $n > k$, the $n$-th term of the series that forms the metric $d$ is $2^{-n}\lvert x_n - y_n \rvert \leq 2^{-n+1}$.
    Thus:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert \leq M_k \sum_{n=1}^{k}2^{-n} + \sum_{n=k+1}^{\infty} 2^{-n + 1}$$
    $$ = M_k(1 - 2^{-k}) + 2(2 - 1 - (1 - 2^{-k})) = M_k - 2^{-k}M_k + 2^{-k+1} \leq M_k + 2^{-k+1},$$
    where we used the fact that the geometric series with ratio $r=1/2$ sums to 2, and our second sum here was thus 2 minus the sum of the first $k$ terms minus 1 since the series starts at 1 instead of 0.
    In the last step we also use the fact that $M_k \geq 0$.
    For the other inequality, we have again that:
    $$d(x, y) = \sum_{n=1}^{k} 2^{-n} \lvert x_n - y_n \rvert + \sum_{n > k}^{\infty}2^{-n}\lvert x_n - y_n \rvert$$
    Notice that in order for $x, y$ to minimize this, it should be the case that $x_n - y_n = 0, n > k$, and that the maximum absolute difference $M_k$ is multiplied with the smallest possible quantity up to $k$, that is, with $2^{-k}$, while for all other $n < k, x_n - y_n = 0$.
    This would yield that $d(x, y) \geq M_k2^{-k} + 0 + 0 = M_k2^{-k}$.

\end{solution}

\begin{exercise}{12}
    Check that $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert$ defines a metric on $C[a, b]$, the collection of all continuous, real valued functions defined on the closed interval $[a, b]$.
\end{exercise}

\begin{solution}
    
    Note that here continuity is important to ensure that the difference of any two functions is continuous, and as such achieves a maximum value in any closed interval.
    Using exercise 3, we need to check the following:
    \begin{itemize}
        \item One, that $d(f, g) = 0$ iff $f = g$.
        Suppose first that $d(f, g) = 0$, that is, $\max_{a \leq b} \lvert f(t) - g(t) \rvert = 0$.
        If $f, g$ were not equal, there would exist $x \in [a, b]$ such that $f(x) \neq g(x)$.
        Clearly then $\max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert \geq \lvert f(x) - g(x) \rvert > 0$, which is a contradiction.
        Therefore $f = g$.
        In the other direction, if $f = g$, we equivalently have that $f - g$ is the zero function on $[a, b]$, and therefore $d(f, g) = \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = 0$.
        \item Two, that for any three $f, g, h \in C[a, b], d(f, h) \leq d(f, g) + d(g, h)$.
        We have first that $d(f, h) = \max_{a \leq t \leq b} \lvert f(t) - h(t) \rvert$.
        Suppose that this function achieves its maximum value on $t_M \in [a, b]$ (if there are more than one, pick any).
        Then $d(f, h) = \lvert f(t_M) - h_(t_M) \rvert = \lvert f(t_M) - g(t_M) + g(t_M) - h(t_M) \rvert \leq \lvert f(t_M) - g(t_M) \rvert + \lvert g(t_M) - h(t_M) \rvert$.
        By definition, it holds that $\lvert f(t_M) - g(t_M) \rvert \leq \max_{a \leq t \leq b} \lvert f(t) - g(t) \rvert = d(f, g)$, and similarly $\lvert g(t_M) - h(t_M) \rvert \leq d(g, h)$.
        We therefore obtain that $d(f, h) \leq d(f, g) + d(g, h)$.
    \end{itemize}
    Therefore $d$ is indeed a metric on $C[a, b]$.
\end{solution}

\begin{exercise}{14}
    We say that a subset $A$ of a metric space is \textbf{bounded} if there is some $x_0 \in M$ and some constant $C < \infty$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Show that a finite union of bounded sets is again bounded.
\end{exercise}

\begin{solution}
    
    Suppose we have the finite union 
    $$U = A_1 \cup A_2 \cup \ldots \cup A_n$$
    of $n$ bounded subsets of a metric space $M$.
    Let then $x_1, x_2, \ldots, x_n, C_1, \ldots, C_n$ be such that $d(a, x_i) \leq C$ for $a \in A_i$.
    Form the set $\{d(x_1, x_1), d(x_1, x_2), \ldots, d(x_n, x_1)\}$, which, crucially, has a finite number of elements, and therefore also has a maximum element, $M$.
    By the same reasoning, there exists $C$ such that $C = \max\{C_1, \ldots, C_n\}$.
    Now pick any $a \in U$, which must belong in at least one $A_i$.
    Therefore:
    $$d(a, x_1) \leq d(a, x_i) + d(x_i, x_1) \leq C_i + M \leq C + M$$
    If we thus set $x_U = x_1$ and $C_U = C + M$, we have shown precisely that $U$ is bounded.
\end{solution}

\begin{exercise}{15}
    We define the \textbf{diameter} of a nonempty subset $A$ of $M$ by $\text{diam}(A) = \sup\{d(a, b) : a, b \in A\}$.
    Show that $A$ is bounded if and only if $\text{diam}(A)$ is finite.
\end{exercise}

\begin{solution}
    
    $\implies$: Suppose first that $A$ is bounded.
    Then there exists $x_0 \in M, C \in \mathbb{R}$ such that $d(a, x_0) \leq C$ for all $a \in A$.
    Pick any two $a, b \in A$.
    We then have that $d(a, b) \leq d(a, x_0) + d(x_0, b) \leq 2C$.
    Therefore the set $\{d(a, b): a, b \in A$\} has an upper bound, namely, $2C$, and therefore also a finite least upper bound, which means precisely that $\text{diam}(A)$ is finite.

    $\impliedby$: Now suppose $\text{diam}(A) = C \in \mathbb{R}$.
    Fix an $a \in A$ (which exists since $A$ is nonempty).
    Pick any $b \in A$, in which case we have that:
    $$d(a, b) \in \{d(x, y): x, y \in A\} \implies d(a, b) \leq \text{diam}(A) = C$$
    If thus set $x_0 = a$ and $C$ the respective constant, we see that the definition of $A$ being bounded is fulfilled.
\end{solution}

\section{Normed Vector Spaces}

\begin{exercise}{16}
    Let $V$ be a vector space, and let $d$ be a metric on $V$ satisfying $d(x, y) = d(x - y, 0)$ and $d(ax, ay) = \lvert a \rvert d(x, y)$ for every $x, y \in V$ and every scalar $a$.
    Show that $\lvert \lvert x \rvert \rvert = d(x, 0)$ defines a norm on $V$ (that has $d$ as its ``usual'' metric).
    Give an example of a metric on the vector space $\mathbb{R}$ that fails to be associated with a norm in this way.
\end{exercise}

\begin{solution}
    Let us examine the properties that would make $\lvert \lvert \cdot \rvert \rvert$ one by one:
    \begin{itemize}
        \item Suppose $x \in V$.
        Then $\lvert \lvert x \rvert \rvert = d(x, 0) \geq 0$, since $d$ is a metric (and of course $\lvert \lvert x \rvert \rvert$ is well-defined as a finite real number for the same reason).
        \item Suppose $\lvert \lvert x \rvert \rvert = 0$.
        Then $d(x, 0) = 0$, which by the properties of metrics we know is true iff $x = 0$.
        Conversely, if $x = 0$, by the same argument $d(x, 0) = 0 \implies \lvert \lvert x \rvert \rvert = 0$.
        Thus $\lvert \lvert x \rvert \rvert = 0 \iff x = 0$.
        \item For any scalar $a$, we have that $\lvert \lvert a x \rvert \rvert = d(ax, 0) = d(ax, a0) = \lvert a \rvert d(x, 0) = \lvert a \rvert \lvert \lvert x \rvert \rvert$ by the exercise hypothesis.
        \item Pick any two $x, y \in V$.
        Then: 
        $$\lvert \lvert x +y \rvert \rvert = d(x + y, 0) \leq d(x + y, y) + d(y, 0) = d(x + y - y, 0) + d(y, 0) = d(x, 0) + d(y, 0) = \lvert \lvert x \rvert \rvert + \lvert \lvert y \rvert \rvert,$$
        where we used the triangle inequality property for metrics and the fact that $d(x, y) = d(x - y, 0)$.
        Therefore the triangle inequality holds for the proposed norm.
    \end{itemize}
    We have thus shown that $\lvert \lvert \cdot \rvert \rvert$ is indeed a norm, and its usual metric will of course be $d'(x, y) = \lvert \lvert x - y \rvert \rvert = d(x - y, 0) = d(x, y)$, i.e.,\ $d$.

    For the requested example, consider the metric $\sigma(x, y) = \lvert x - y \rvert/(1 + \lvert x - y \rvert)$ from exercise 6 of section 3.1.
    Notice that the ``proposed'' norm would then be $\lvert \lvert x \rvert \rvert = \sigma(x, 0) = \lvert x \rvert/(1 + \lvert x \rvert)$.
    Then for a scalar $a$:
    $$\lvert \lvert a x \rvert \rvert = \lvert a x \rvert/(1 + \lvert a x \rvert) = \lvert a \rvert \cdot \lvert x \rvert/(1 + \lvert a \rvert \cdot \lvert x \rvert),$$
    which we can see does not necessarily equal $\lvert a \rvert \cdot \lvert \lvert x \rvert \rvert$, and therefore the ``proposed'' norm is not really a norm.
    The cause for this is the fact that $\sigma(ax, ay) \neq \lvert a \rvert \sigma(x, y)$ in general, for similar reasons.
\end{solution}

\begin{exercise}{18}
    Show that $\lvert \lvert x \rvert \rvert_{\infty} \leq \lvert \lvert x \rvert \rvert_2 \leq \lvert \lvert x \rvert \rvert_1$ for any $x \in \mathbb{R}^n$.
    Also check that $\lvert \lvert x \rvert \rvert_1 \leq n \lvert \lvert x \rvert \rvert_{\infty}$ and $\lvert \lvert x \rvert \rvert_1 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2$.
\end{exercise}

\begin{solution}
    
    We have that $\lvert \lvert x \rvert \rvert_{\infty} = \max_{1 \leq i \leq n} \lvert x_i \rvert$, while $\lvert \lvert x \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} \lvert x_i \rvert^2}$.
    By definition, there must exist at least one $j \in \{1, 2, \ldots, n\}$ such that $\lvert x_j \rvert = \max_{1 \leq i \leq n} \lvert x_i \rvert$.
    In addition, the square is an increasing function for $x \geq 0$, which would mean that $(\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 = \max_{1 \leq i \leq n} \lvert x_i \rvert^2$.
    We therefore have that:
    $$\sum_{i=1}^{n} \lvert \lvert x_i \rvert \rvert^2 = (\max_{1 \leq i \leq n} \lvert x_i \rvert)^2 + X,$$
    for $X \geq 0$ (the sum of the absolute values of the remaining coordinates).
    Thus $(\lvert \lvert x \rvert \rvert_2)^2 \geq (\lvert \lvert x \rvert \rvert_{\infty})^2$, and by taking square roots we have the first desired result.
    
    We also have that:
    $$(\lvert \lvert x \rvert \rvert_1)^2 = \Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr)\Biggl(\sum_{i=1}^{n} \lvert x_i \rvert\Biggr) \geq \sum_{i=1}^{n} \lvert x_i \rvert^2 = (\lvert \lvert x_2 \rvert \rvert)^2,$$
    which we obtain by observing that the product results in a sum of non-negative terms over all pairs of indices $i, j \in \{1, 2, \ldots, n\}$, which of course includes all terms for which $i = j$.
    Taking square roots yields the second desired result.

    For the third result, observe that for each $i, \lvert x_i \rvert \leq \max_{1 \leq i \leq n} \lvert x_i \rvert = \lvert \lvert x \rvert \rvert_{\infty}$.
    This means of course that:
    $$\lvert \lvert x \rvert \rvert_1 = \sum_{i=1}^{n} \lvert x_i \rvert \leq n \lvert \lvert x \rvert \rvert_{\infty}$$
    
    Lastly, we consider the following ``trick'' for the last part.
    For a given $x = (x_1, x_2, \ldots, x_n)$, consider the vectors $y, z \in \mathbb{R}^{n^2}$:
    $$y = \begin{pmatrix}
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \lvert x_2 \rvert \\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}, z = \begin{pmatrix}
        \lvert x_1 \rvert  \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_1 \rvert \\
        \vdots \\
        \lvert x_n \rvert \\
        \lvert x_n \rvert\\
        \vdots \\
        \lvert x_n \rvert
    \end{pmatrix}$$
    We have that $\lvert \lvert z \rvert \rvert_2 = \lvert \lvert y \rvert \rvert_2 = \sqrt{\sum_{i=1}^{n} n\lvert x_i \rvert^2 } = \sqrt{n} \lvert \lvert x \rvert \rvert_2$,
    while $\langle y, z \rangle = \sum_{i=1}^{n} \sum_{j=1}^{n} \lvert x_i \rvert \cdot \lvert x_j \rvert = (\sum_{i=1}^{n} \lvert x_i \rvert)^2 = (\lvert \lvert x \rvert \rvert_1)^2$.
    Applying the Cauchy-Schwarz inequality on $y, z$, we then have that:
    $$\langle y, z \rangle \leq \lvert \lvert y \rvert \rvert \cdot \lvert \lvert z \rvert \rvert \implies (\lvert \lvert x \rvert \rvert_1)^2 \leq \sqrt{n} \lvert \lvert x \rvert \rvert_2 \sqrt{n} \lvert \lvert x \rvert \rvert_2,$$
    which, once more by taking square roots, gives us the last desired result.
\end{solution}