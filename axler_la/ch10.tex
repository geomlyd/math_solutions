\chapter{Trace and Determinant}

\section{Trace}

\begin{exercise}{1}
    Suppose $T \in L(V)$ and $v_1, \ldots, v_n$ is a basis of $V$. Prove that the matrix $M(T, (v_1, \ldots, v_n))$ is invertible if and only if $T$ is invertible.
\end{exercise}

\begin{solution}

    $\implies$: Suppose that $A = M(T, (v_1, \ldots, v_n))$ is an invertible matrix. Then there exists some matrix $A^{-1}$ such that $AA^{-1} = I$. We know that $L(V)$ is isomorphic to the vector space of square matrices of size $\text{dim} V \times \text{dim} V$ with respect to this basis. That means that there exists an operator $S \in L(V)$ such that $M(S, (v_1, \ldots, v_n) = A^{-1}$. Let $R_1 = TS$. Observe that 
    $$M(R, (v_1, \ldots, v_n)) = M(T, (v_1, \ldots, v_n))M(S, (v_1, \ldots, v_n)) = AA^{-1} = M(I, (v_1, \ldots, v_n))$$.
    Again, due to the two vector spaces being isomorphic, equality of matrices implies equality of operators, i.e. $R_1 = TS = I$. In the same way we can prove that $ST = I$, hence $S$ is the inverse of $T$, hence the inverse of $T$ does exist.

    $\impliedby$: If $T$ is invertible and $S = T^{-1}$, then let $A = M(T, (v_1, \ldots, v_n)), B = M(S, (v_1, \ldots, v_n))$. Then $M(I, (v_1, \ldots, v_n)) = M(TS, (v_1, \ldots, v_n)) = M(T, (v_1, \ldots, v_n))M(S, (v_1, \ldots, v_n)) \implies I = AB$. Similarly $BA = I$, which by definition of the inverse of a matrix means that $B = A^{-1}$, i.e. $A$ is invertible.
\end{solution}

\begin{exercise}{2}
    Suppose $A$ and $B$ are square matrices of the same size and $AB = I$. Prove that $BA = I$.
\end{exercise}

\begin{solution}

    Let $n$ be the ``dimension'' of $A, B$, i.e. $A, B$ are square matrices of size $n \times n$. Then if $V = \mathbf{F}^n$, if we think of $A, B$ as matrices with respect to the standard basis of $V$, we know that $L(V)$ and the vector space of those matrices are isomorphic. This means that there exist unique operators $T, S$ such that $M(T) = A, M(S) = B$ (with respect to the standard basis, omitted for brevity). It is true that $AB = I$. This means that $M(TS) = M(T)M(S) = AB = I = M(I)$. Again due to the isomorphism between $L(V)$ and the vector space of matrices, we conclude that $TS = I$. We claim that $S$ is invertible and that $S^-1 = T$, which would directly imply that $M(ST) = M(I) = I = M(S)M(T) = BA$. To prove this we will prove that $S$ is injective. Suppose it was not, in which case there exists a non-zero $v$ such that $S(v) = 0$. Then $TS(v) = T(0) = 0 = I(v) = v$ which is a contradiction. Therefore $S$ is injective and as a consequence invertible.

    Therefore, if $v_1, \ldots, v_n$ is a basis of $V$, $S(v_1), \ldots S(v_n)$ is also a basis of $V$. It must hold that $T(S(v_i)) = I(v_i) = v_i$. Consider then the values of $ST$ on the basis $S(v_i)$: $ST(S(v_i)) = S(v_i) = I(S(v_i))$, therefore $ST = I$, and because $TS = I$ by definition $T = S^-1$, thus proving the exercise by the observation made above.
\end{solution}

\begin{exercise}{3}
    Suppose $T \in L(V)$ has the same matrix with respect to every basis of $V$. Prove that $T$ is a scalar multiple of the identity operator.
\end{exercise}

\begin{solution}

    To begin, note that if $\text{dim} V = 1$, it is obvious that the only operators that can be defined are scalar multiples of the identity operator, since the basis has exactly one vector which can only be mapped to a scalar multiple of itself. Also, if $\text{dim} V = 0$ it is trivially true that the only operator that can be defined is $T = 0$. Therefore from now on we can assume that $\text{dim} V \geq 2$. Let therefore $v_1, v_2, \ldots, v_n$ be a basis of $V$, $n \geq 2$. With respect to any basis of $V$, the matrix of $T$ is the same, so let us call it $A$. In particular, for any two $i, j, i \neq j$ it holds that:
    $$T(v_i) = A_{ii}v_i + A_{ji}v_j + \sum_{k \neq i, j} A_{ki}v_k$$
    Consider now the basis that results from exchanging $v_i, v_j$ in the original basis and keeping everything else the same. The matrix of $T$ with respect to it will again equal $A$, thus for the same $i, j$ we see that we now have:
    $$T(v_i) = A_{jj}v_i + A_{ij}v_j + \sum_{k \neq i, j} A_{kj}v_k$$
    The reasoning is that the coefficients for $T(v_i)$ now come from the $j$-th column of the matrix instead of the $i$-th, and that its $j$-th row now corresponds to $v_i$ instead of $v_j$. By equating the two right-hand sides above and observing that these vectors are linearly independent we obtain that $A_{ii} = A_{jj}, A_{ij} = A_{ji}$ and $ A_{ki} = A_{kj}$ for all $k \neq i, j$. Now, since we can do this for any two $i, j$ we conclude that all of the diagonal elements of $A$ are equal to some number $a$ and that all off-diagonal elements of $A$ are equal to some number $b$. The first of these two observations is straightforward, while the second arises from the fact that this is a symmetric matrix whose columns are equal element-wise.

    It suffices therefore to show that $b = 0$. Indeed, consider the basis $-v_1, v_2, \ldots, v_n$. Then:
    $$T(-v_1) = a(-v_1) + b\sum_{i \neq 1}v_i \implies T(v_1) = av_1 - b \sum_{i \neq 1} v_i$$
    However, from our first equation we obtain that:
    $$T(v_1) = av_1 + b\sum_{i \neq 1}v_i$$
    Equating these two and using again the linear independence of $v_i$ we obtain $2b = 0$, thus $b=0$. Thus $A$ is a diagonal matrix with all of its diagonal elements being equal to some number $a$, therefore it is a scalar multiple of the identity.
\end{solution}

\begin{exercise}{5}
    Suppose $B$ is a square matrix with complex entries. Prove that there exists an invertible square matrix $A$ with complex entries such that $A^{-1}BA$ is an upper-triangular matrix.
\end{exercise}

\begin{solution}

    Suppose $u_1, \ldots, u_n$ is the standard basis of $\mathbf{F}^n$, where $n$ is the ``dimension'' of the matrix $B$. We know then that there exists a unique $T \in L(\mathbf{F}^n)$ such that $M(T, (u_1, \ldots, u_n)) = B$. Since our field is the complex numbers, there exists a basis $v_1, \ldots, v_n$ of $\mathbf{F}^n$ such that $M(T, (v_1, \ldots, v_n))$ is upper-triangular. We know then that these two matrices of $T$ are related in the following manner:
    
    $$M(T, (v_1, \ldots, v_n)) = A^{-1}M(T, (u_1, \ldots, u_n))A = A^{-1}BA$$

    , where $A = M(I, (v_1, \ldots, v_n), (u_1, \ldots, u_n))$. Notice then that for this choice of $A$, which we know is invertible, $A^{-1}BA$ is indeed upper-triangular.
\end{solution}

\begin{exercise}{10}
    Suppose $V$ is an inner product space and $T \in L(V)$. Prove that
    $$\text{trace} T^* = \overline{\text{trace} T}$$
\end{exercise}

\begin{solution}

    We can always find an orthonormal basis of $V$ by using the Gram-Schmidt procedure on any basis of it. We also know that if $A$ is the matrix of $T$ with respect to this basis, the matrix of $T^*$ with respect to it is the conjugate transpose of $A$. We also know that the trace of any operator is equal to the trace of any of its matrices. 
    
    Therefore, $\text{trace} T^* = \sum_i \overline{A_{ii}} = \overline{\sum_i A_{ii}} = \overline{\text{trace} T}$
\end{solution}

\begin{exercise}{14}
    Suupose $T \in L(V)$ and $c \in \mathbf{F}$. Prove that $\text{trace} (cT) = c \text{trace} (T)$.
\end{exercise}

\begin{solution}

    We know that the trace of any operator is equal to the trace of any of its matrices. Therefore, select any basis of $V$ and let $A$ be the matrix of $T$ with respect to this basis. Then, if we multiply $A$ with $c$, the result is the matrix that corresponds to the operator $cT$ with respect to this basis. Additionally, it is obvious that since every element of the matrix is multiplied by $c$, the sum of its diagonal elements is also multiplied by $c$. Therefore:
    $$\text{trace} (cT) = \text{trace} (cA) = c \text{trace} (A) = c \text{trace} T$$
\end{solution}

\begin{exercise}{15}
    Suppose $S, T \in L(V)$. Prove that $\text{trace}(ST) = \text{trace}(TS)$.
\end{exercise}

\begin{solution}

    Pick any basis $v_1, \ldots, v_n$ of $V$ and let $A = M(T, (v_1, \ldots, v_n)), B = M(S, (v_1, \ldots, v_n))$. We then have that:
    $$\text{trace} (ST) = \text{trace} (BA) = \text{trace} (AB) = \text{trace} (TS)$$

    , where we used the fact that for any operator its trace equals the trace of its matrix, the definition of the matrix of the composition of two operators and the proven fact that for two \textit{matrices}, $\text{trace} (BA) = \text{trace} (AB)$.
\end{solution}

\begin{exercise}{18}
    Suppose $V$ is an inner product space with orthonormal basis $e_1, \ldots, e_n$ and $T \in L(V)$. Prove that:
    $$\text{trace} (T^*T) = \lvert \lvert T e_1 \rvert \rvert^2 + \ldots + \lvert \lvert T e_n \rvert \rvert^2$$
    Conclude that the right side of the equation above is independent of which orthonormal basis $e_1, \ldots, e_n$ is chosen for $V$.
\end{exercise}

\begin{solution}

    Because $e_1, \ldots, e_n$ is an orthonormal basis, we know that:
    $$T^*T(e_i) = \sum_{j} \langle T^*Te_i, e_j \rangle e_j$$
    The diagonal element of the matrix of $T^*T$ with respect to this basis in the $i$-th column equals the coefficient for $e_i$ when writing $T^*T(e_i)$  as a linear combination of all $e_j$ . Given the above sum, this equals $\langle T^*T e_i, e_i \rangle = \langle T e_i, T e_i \rangle = \lvert \lvert T e_i \rvert \rvert^2$. The trace of this matrix (denoted as $A$) equals the sum of all of these elements, namely:
    $$\text{trace} (A) = \lvert \lvert Te_1 \rvert \rvert^2 + \ldots + \lvert \lvert Te_n \rvert \rvert^2$$
    Since the trace of an operator equals the trace of any of its matrices, we have that $\text{trace}(T^*T) = \lvert \lvert T e_1 \rvert \rvert^2 + \ldots + \lvert \lvert T e_n \rvert \rvert^2$. Lastly, because the trace is independent of the choice of basis, the right side of this equation must also be independent of which \textit{orthonormal} basis is chosen.
\end{solution}

\begin{exercise}{19}
    Suppose $V$ is an inner-product space. Prove that
    $$\langle S, T \rangle = \text{trace} (ST^*)$$
    defines an inner product on $L(V)$.
\end{exercise}

\begin{solution}

    We need to examine the properties that define an inner product one by one:
    \begin{itemize}
        \item \textbf{Positivity}: For any $T \in L(V)$, we have that $\langle T, T \rangle = \text{trace} (TT^*)$. From the previous exercise we observe that this equals $\lvert \lvert T e_1 \rvert \rvert^2 + \ldots + \lvert \lvert T e_n \rvert \rvert^2$, where $e_1, \ldots, e_n$ is any orthonormal basis of $V$. These norms correspond to the inner product on $V$ and are thus nonnegative. Therefore it is also true that $\langle T, T \rangle \geq 0$.
        \item \textbf{Definitiness}: Given a $T \in L(V)$, we need to examine when is it the case that $\langle T, T^* \rangle = 0$. By the observation above, the only way this can happen is if each $\lvert \vert T e_i \rvert \rvert$ is zero. By the properties of the inner product on $V$, this happens iff $T e_i = 0$. Therefore, $T$ must map all vectors of an orthonormal basis to zero, and therefore it must be the zero operator. We conclude that $\langle T, T \rangle = 0$ iff $T = 0$.
        \item  \textbf{Homogeneity in the first slot}: For $T \in L(V), a \in \mathbf{F}$ we have that $\langle a T, T \rangle = \text{trace}(a TT^*) = a \text{trace} (TT^*) = a \langle T, T \rangle$, where we used exercise $14$.
        \item \textbf{Additivity in the first slot}: For $S, T, R \in L(V)$, we have that:
        $$\langle S + T, R \rangle = \text{trace}((S+T)R^* \rangle) \text{trace}(SR^* + TR^*) = \text{trace}(SR^*) + \text{trace}(TR^*)$$
        $$= \langle S, R \rangle + \langle T, R \rangle$$
        , where we used the additivity of traces and the distributive property of matrix multiplication on addition.
        \item \textbf{Conjugate symmetry}: Given $S, T \in L(V)$ we have that:
        $$\langle T, S \rangle = \text{trace}(TS^*) = \text{trace}(S^*T) = \text{trace}(S^*(T^*)^*) = $$
        $$\text{trace} ((T^*S)^*) = \overline{\text{trace} (T^*S)} = \overline{\text{trace} (ST^*)} = \overline{\langle S, T \rangle}$$
        , where we used exercises 10, 15 and the adjoint of the product of two operators.
    \end{itemize}
    Therefore, the defined function is indeed an inner product on $L(V)$.
\end{solution}

\section{Determinant}

\begin{exercise}{1}
    Suppose $V$ is a real vector space. Suppose $T \in L(V)$ has no eigenvalues. Prove that $\text{det} T > 0$.
\end{exercise}

\begin{solution}

    The determinant of $T$ is equal to the product of all the eigenvalues of $T_\mathbf{C}$, each raised to the corresponding multiplicity. Since $T$ has no eigenvalues, $T_\mathbf{C}$ can have no real eigenvalues. Therefore, all of its eigenvalues are of the form $a+bi, b \neq 0$. We know, however, that if $T_\mathbf{C}$ has $a+bi$ as an eigenvalue, it also has $a-bi$, and in fact with the same multiplicity. 
    
    Therefore, the product of all of the eigenvalues of $T_\mathbf{C}$ can be written as a product $\Pi_jp_j$ of terms of the form $p_j = (a_j+b_ji)(a_j+b_ji) = a_j^2 + b_j^2, b_j \neq 0$. Each of those terms is positive, therefore the product itself is positive, and the determinant of $T$, which equals this product, is also positive.
\end{solution}

\begin{exercise}{2}
    Suppose $V$ is a real vector space with even dimension and $T \in L(V)$. Suppose $\text{det} T < 0$. Prove that $T$ has at least two distinct eigenvalues.
\end{exercise}

\begin{solution}

    Suppose that $T$ has less than two distinct eigenvalues. There are then two cases:
    \begin{itemize}
        \item If $T$ has no eigenvalues, then by the previous exercise $\text{det} T > 0$, which is a contradiction.
        \item  If $T$ has exactly one distinct eigenvalue $\lambda$ with multiplicity $d$, then $d$ is either odd or even. If $d$ is even, then the remainining, non-real eigenvalues of $T_\mathbf{C}$ have a sum of multiplicities equal to $\text{dim} V - d$ which is also even. Because they come in conjugate pairs of equal multiplicities, the determinant of $T$ must equal $\text{det} T = \lambda^d \Pi_j p_j$, where $p_j = (a_j + b_ji)(a_j + b_ji) = a_j^2+b_j^2, b_j \neq 0$. The determinant is thus clearly non-negative because $d$ is even and $p_j \geq 0$, therefore we have a contradiction.

        If $d$ is odd, then the  remainining, non-real eigenvalues of $T_\mathbf{C}$ have a sum of multiplicities equal to $\text{dim} V - d$ which is odd. However, this is a contradiction because the non-real eigenvalues of $T_\mathbf{C}$ come in conjugate pairs of equal multiplicities, and thus the sum of those multiplicities cannot be odd.
    \end{itemize}

Since the hypothesis that $T$ has less than two distinct eigenvalues always leads to a contradiction, $T$ must have at least two distinct eigenvalues.
\end{solution}

\begin{exercise}{4}
    Suppose $T \in L(V)$ and $c \in \mathbf{F}$. Prove that $\text{det}(cT) = c^{\text{dim} V}\text{det} T$.
\end{exercise}

\begin{solution}

    Suppose first that $V$ is a complex vector space. Suppose $\lambda_1, \lambda_2, \ldots, \lambda_n$ are the eigenvalues of $T$, each one repeated according to its multiplicity. Then $\text{det} T = \lambda_1 \cdot \lambda_2 \cdot \ldots \cdot \lambda_n$. Now let $S = c T$. If $c = 0$, then $S = 0$ and $\text{det} S = 0 = 0^{\text{dim} V}\text{det} T$. Therefore assume that $c \neq 0$. Then, if $\lambda$ is an eigenvalue of $T$, there exists a non-zero $v \in V$ such that $T(w) = \lambda w$. Clearly then, $S(v) = cT(v) = c\lambda w$, therefore, $c\lambda$ is an eigenvalue of $S$ with $v$ being a corresponding eigenvector. Conversely, if $S(v) = \mu v$ for a non-zero $v$ then $cT(v) = \mu v \implies T(v) = \frac{\mu}{c} v$, which means that $\frac{\mu}{c}$ is an eigenvalue of $T$ with $v$ being a corresponding eigenvector.

    These two observations lead to the conclusion that $S$ has as eigenvalues $c\lambda_1, \ldots, c\lambda_n$, where each distinct $c\lambda_j$ is repeated as many times as $\lambda_j$ was for $T$ (because the corresponding sets of eigenvectors are equal). Clearly then, $\text{det} S = \text{det} (CT) = (c\lambda_1) \cdot (c\lambda_2) \cdot \ldots \cdot (c \lambda_n) = c^{\text{dim} V}\text{det} T$, because the sum of all multiplicities equals $\text{dim} V$.

    For a real vector space $V$, it suffices to apply the above argument to $T_\mathbf{C}$.
\end{solution}

\begin{exercise}{6}
    Suppose $A$ is a block upper-triangular matrix 
    $$A = \begin{pmatrix}
        A_1 & \ldots & * \\ \ & \ddots & \vdots \\ 0 & \ & A_m
    \end{pmatrix}$$
where each $A_j$ along the diagonal is a square matrix. Prove that $$\text{det} A = (\text{det} A_1)\cdots(\text{det} A_m)$$
\end{exercise}

\begin{solution}

    Consider the algorithm we use for the computation of the determinant: we compute all possible permutations of the row indices, and then for each of them we take the first-column element of the first row in the permutation, the second-column element of the second column in the permutation etc. and multiply all of them together.

    Each of those quantities is multiplied with 1 or -1 and then all of them are added together. In our case here, let $s_1, s_2, \ldots s_m$ be the sizes of the square blocks on the diagonal. By the description of the algorithm, we can see that for a particular permutation term, the first $s_1$ columns will contribute to the product elements from the first $s_1$ rows of that permutation.

    Observe that if any of those first $s_1$ permuted row indices is greater than $s_1$, all of the elements in columns 1 to $s_1$ are zero. Therefore, the corresponding product will also be zero. This leads to the conclusion that non-zero terms of the sum must the first $s_1$ rows be a permutation of $(1, 2, \ldots, s_1)$.

    The same reasoning applied to any other block results in the conclusion that for permutations \\ $(m_{1}, m_{2}, \ldots, m_{n})$ whose product is not zero it must be the case that:
    $$(m_{1}, \ldots, m_{s_1}) \in \text{perm}(1, 2, \ldots s_1), (m_{s_1+1}, \ldots, m_{s_1+s_2}) \in \text{perm}(s_1+1, \ldots, s_1+s_2), \ldots, $$ $$(m_{\sum_{k=1}^{m-1}s_i+1}, \ldots, m_{n}) \in \text{perm}(\sum_{k=1}^{m-1}s_m, \ldots, n)$$

    In other words, the permutations can be ``decomposed'' to sub-permutations of row indices of each block on the diagonal. For permutations with this property, the sign of the permutation must equal the product of the signs of the sub-permutations. This can be proved by induction on the number $m$ of sub-permutations:
    \begin{itemize}
        \item For $m=2$, we have that all of the indices of the second sub-permutation are larger than all of the indices of the first sub-permutation. Therefore, concatenating them does not create additional wrongly-ordered pairs of indices. This means that the total number of wrong pairs is $k_1+k_2$, where $k_1, k_2$ the number of wrong pairs in the two sub-permutations. If both of them are odd or even, i.e.\ corresponding signs 1, the sum is even, i.e.\ corresponding sign 1. If exactly one is odd, i.e.\ one sign -1 and the other 1, the sum is odd, i.e.\ sign -1. Therefore the sign is always equal to the product of the two sub-permutation signs.
        \item If this is true for $m \geq 2$ number of sub-permutations, then for $m+1$ sub-permutations the resulting permutation can be thought of as a permutation resulting from the concatenation of two sub-permutations: the first is the concatenation of $m$ sub-permutations and the second is the last sub-permutation. Applying the induction hypothesis on the first $m$ sub-permutations and then the exact same argument as the induction basis on the concatenation of the last sub-permutation yields the desired result .
    \end{itemize}

    Having proved this, we have that:
    $$\text{det} A = \sum_{\substack{(m_1, \ldots, m_{s_1}) \\ \in \text{perm}(1, \ldots, s_1)}} \sum_{\substack{(m_{s_1+1}, \ldots, m_{s_1+s_2}) \\ \in \text{perm}(s_1+1, \ldots, s_2)}} \ldots \sum_{\substack{(m_{\sum_{k=1}^{m-1}s_k+1}, \ldots, m_n) \\ \in \text{perm}({\sum_{k=1}^{m-1}s_k+1, \ldots, n)}}} \text{sign}(m_1, \ldots, m_n) A_{m_1, 1} \cdot A_{m_2, 2} \cdots A_{m_n, n}$$
    $$= \sum_{\substack{(m_1, \ldots, m_{s_1}) \\ \in \text{perm}(1, \ldots, s_1)}} \text{sign}(m_1, \ldots, m_{s_1}) A_{m_1, 1} \cdots A_{m_{s_1}, s_1} \cdot$$
    $$\sum_{\substack{(m_{s_1+1}, \ldots, m_{s_1+s_2}) \\ \in \text{perm}(s_1+1, \ldots, s_2)}} \text{sign}(m_{s_1+1}, \ldots, m_{s_1+s_2}) A_{s_1+1, s_1+1} \cdots A_{m_{s_1+s_2}, s_1+s_2}$$
    $$\cdot \ldots \sum_{\substack{(m_{\sum_{k=1}^{m-1}s_k+1}, \ldots, m_n) \\ \in \text{perm}({\sum_{k=1}^{m-1}s_k+1, \ldots, n)}}} \text{sign}(m_{\sum_{k=1}^{m-1}s_k+1}, \ldots, m_n)A_{m_{\sum_{k=1}^{m-1}s_k+1}, {\sum_{k=1}^{m-1}s_k+1}} \cdots A_{m_n, n}$$
    $$= \text{det}(A_1) \cdots \text{det}(A_m)$$
\end{solution}

\newpage
\begin{exercise}{8}
    Suppose $V$ is an inner product space and $T \in L(V)$. Prove that
    $$\text{det} T^* = \overline{\text{det} T}$$
    Use this to prove that $\lvert \text{det} T \rvert = \text{det} \sqrt{T^*T}$, giving a different proof than was given in 10.47.
\end{exercise}

\begin{solution}

    Consider the characteristic polynomial of $T, p(z) = z^n + a_{n-1}z^{n-1} + \ldots + a_1z + a_0$. We know that the determinant of $T$ equals $a_0(-1)^{n}$, where $n = \text{dim} V$. Additionally, $p(T) = 0$. Take the adjoint of $p(T)$, which by the properties of the adjoint must be equal to:
    $$(p(T))^* = (T^*)^n + \overline{a_{n-1}}(T^*)^{n-1} + \ldots + \overline{a_1}T + \overline{a_0}I$$
    Additionally, since the adjoint of the zero map is the zero map, this must equal the zero map as well. This means that for the polynomial $q(z) = z^n + \overline{a_{n-1}}z^{n-1} + \ldots + \overline{a_1}z + \overline{a_0}, q(T^*) = 0$. Therefore, $q$ is the characteristic polynomial of $T^*$, since it is monic and has degree $\text{dim} V$. Therefore, the determinant of $T^*$ must equal $\text{det} T^* = (-1)^{\text{dim}V}\overline{a_0} = \overline{\text{det} T}$.

    Now, consider the following: 
    $$\text{det}(T^*T) = \text{det} T^* \text{det} T = \overline{\text{det} T} \text{det} T \implies \text{det}(T^*T) = \lvert \text{det} T \rvert^2$$. However, by definition $\sqrt{T^*T}\sqrt{T^*T} = T^*T$. Therefore:
    $$\text{det}(T^*T) = \text{det}(\sqrt{T^*T}\sqrt{T^*T}) \implies \text{det}(\sqrt{T^*T})\text{det}({\sqrt{T^*T}}) = \lvert \text{det} T \rvert^2$$

    By observing that $\sqrt{T^*T}$ is a positive operator, and thus all of its eigenvalues and its determinant are positive, and by taking square roots we obtain $\text{det}(\sqrt{T^*T}) = \lvert \text{det} T \rvert$.

\end{solution}